Journal List PLoS Comput Biol v Oct PMC library NLM provides access scientific literature Inclusion NLM database imply endorsement agreement contents NLM National Institutes Health Learn disclaimer PLoS Comput Biol Oct e Published online Oct doi journalpcbi PMCID PMC PMID Structured random receptive fields enable informative sensory encodings Biraj Pandey Formal analysis Investigation Software Validation Visualization Writing original draft Writing review editing Marius Pachitariu Data curation Writing review editing Bingni W Brunton Formal analysis Resources Supervision Visualization Writing review editing Kameron Decker Harris Conceptualization Formal analysis Investigation Supervision Visualization Writing original draft Writing review editing Biraj Pandey Department Applied Mathematics University Washington Seattle Washington United States America Find articles Biraj Pandey Marius Pachitariu Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Find articles Marius Pachitariu Bingni W Brunton Department Biology University Washington Seattle Washington United States America Find articles Bingni W Brunton Kameron Decker Harris Department Computer Science Western Washington University Bellingham Washington United States America Find articles Kameron Decker Harris Xuexin Wei Editor Author information Article notes Copyright License information Disclaimer Department Applied Mathematics University Washington Seattle Washington United States America Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Department Biology University Washington Seattle Washington United States America Department Computer Science Western Washington University Bellingham Washington United States America University Texas Austin UNITED STATES Corresponding author authors declared competing interests exist Email udeuwwsirrahnoremak Received Oct Accepted Aug Copyright Pandey et al open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Associated Data Supplementary Materials Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF pcbispdf GUID EEABECFA Attachment Submitted filename coverletterresubmissionpdf pcbispdf GUID FDDFCFDCBEDFB Data Availability Statement relevant data within manuscript Supporting information files Abstract Brains must represent outside world animals survive thrive early sensory systems neural populations diverse receptive fields structured detect important features inputs yet significant variability ignored classical models sensory neurons model neuronal receptive fields random variable samples parameterized distributions demonstrate model two sensory modalities using data insect mechanosensors mammalian primary visual cortex approach leads significant theoretical connection foundational concepts receptive fields random features leading theory understanding artificial neural networks modeled neurons perform randomized wavelet transform inputs removes high frequency noise boosts signal random feature neurons enable learning fewer training samples smaller networks artificial tasks structured random model receptive fields provides unifying mathematically tractable framework understand sensory encodings across spatial temporal domains Author summary Evolution ensured animal brains dedicated extracting useful information raw sensory stimuli discarding everything else Models sensory neurons key part theories brain represents world work model tuning properties sensory neurons way incorporates randomness builds bridge leading mathematical theory understanding artificial neural networks learn models capture important properties large populations real neurons presented varying stimuli Moreover give precise mathematical formula sensory neurons two distinct areas one involving gyroscopic organ insects visual processing center mammals transform inputs also find artificial models imbued properties real neurons learn efficiently shorter training time fewer examples mathematical theory explains findings work expands understanding sensory representation large networks benefits neuroscience machine learning communities Introduction long argued brain uses large population neurons represent world view sensory stimuli encoded responses population used downstream areas diverse tasks including learning decisionmaking movement control sensory areas different neurons responding differing stimuli also providing measure redundancy However still lack clear understanding response properties wellsuited different sensory modalities One way approach sensory encoding understanding neuron would respond arbitrary stimuli Experimentally typically present many stimuli animal measure responses sensory neurons attempt estimate kind model neurons respond arbitrary stimulus common assumption neuron computes linear filter stimulus drives spiking nonlinear spikegenerating mechanism Mathematically assumption summarized number measured spikes stimulus x equal w x weight vector w nonlinearity weights w define filtering properties neuron also known receptive field model known linearnonlinear LN model also common form artificial neuron artificial neural networks ANNs LN models used extensively describe firing diverse neurons various sensory modalities vertebrates invertebrates mammalian visual system LN models used characterize retinal ganglion cells lateral geniculate neurons simple cells primary visual cortex V also used characterize auditory sensory neurons avian midbrain somatosensory neurons cortex insects used understand response properties visual interneurons mechanosensory neurons involved proprioception auditory neurons courtship behavior Given stimulus presented neural response data one estimate receptive fields population neurons Simple visual receptive fields classically understood similar wavelets particular spatial frequency angular selectivity mechanosensory areas receptive fields selective temporal frequency short time window Commonly parametric modeling Gabor wavelets smoothing regularization etc used produce clean receptive fields Yet data alone show noisy receptive fields perhaps best modeled using random distribution show modeling receptive fields random samples produces realistic receptive fields reflect structure noisiness seen experimental data importantly perspective creates significant theoretical connections foundational ideas neuroscience artificial intelligence connection helps us understand receptive fields structures structure relates kinds stimuli relevant animal Modeling filtering properties population LN neurons samples random distribution leads study networks random weights machine learning ML networks known random feature networks RFNs study RFNs rapidly gained popularity recent years large part offers theoretically tractable way study learning properties ANNs weights tuned using data RFN contains many neurons approximate functions live wellunderstood function space function space called reproducing kernel Hilbert space RKHS depends network details particular weight ie receptive field distribution Learning framed approximating functions space limited data Several recent works highlight RFN theorys usefulness understanding learning neural systems Bordelon Canatar Pehlevan series papers shown neural codes allow learning examples spectral properties secondorder statistics aligns spectral properties task applied V found neural code aligned tasks depend low spatial frequency components Harris constructed RFN model sparse networks found associative centers like cerebellum insect mushroom body showed areas may behave like additive kernels architecture also considered Hashemi et al classes kernels beneficial learning high dimensions learn fewer examples remain resilient input noise adversarial perturbation Xie et al investigated relationship fraction active neurons model cerebellumcontrolled neuron thresholdsand generalization performance learning movement trajectories vast majority network studies random weights weights w drawn Gaussian distribution independent entries sampling equivalent fully unstructured receptive field looks like white noise Closely related work previous study ANNs showed directly learning structured receptive fields could improve image classification deep networks receptive fields parametrized sum Gaussian derivatives fourth order led better performance rival architectures low data regimes paper study effect structured yet random receptive fields lead informative sensory encodings Specifically consider receptive fields generated Gaussian process GP thought drawing weights w Gaussian distribution particular covariance matrix show networks random weights project input new basis filter particular components theory introduces realistic structure receptive fields random feature models crucial current understanding artificial networks Next show receptive field datasets two disparate sensory systems mechanosensory neurons insect wings V cortical neurons mice monkeys wellmodeled GPs covariance functions wavelet eigenbases Given success modeling data GP apply weight distributions RFNs used synthetic learning tasks find structured weights improve learning reducing number training examples size network needed learn task Thus structured random weights offer realistic generative model receptive fields multiple sensory areas understand performing random change basis change basis enables network represent important properties stimulus demonstrate useful learning Results construct generative model receptive fields sensory neurons use weights ANN refer network structured random feature network first review basics random feature networks details rationale behind generative model process generate hidden weights main theory result networks weights transform inputs new basis filter particular components thus bridging sensory neuroscience theory neural networks Next show neurons two receptive field datasetsinsect mechanosensory neurons mammalian V cortical neuronsare welldescribed generative model close resemblance secondorder statistics sampled receptive fields principal components data model Finally show performance structured random feature networks several synthetic learning tasks hidden weights generative model allows network learn fewer training examples smaller network sizes Theoretical analysis consider receptive fields generated GPs order connect foundational concept sensory neuroscience theory random features artificial neural networks GPs thought samples Gaussian distribution particular covariance matrix initialize hidden weights RFNs using GPs show using GP causes network project input new basis filter particular components basis determined covariance matrix Gaussian useful removing irrelevant noisy components input use results study space functions RFNs containing many neurons learn connecting construction theory kernel methods Random feature networks start introducing main learning algorithm neuronal model work RFN Consider twolayer feedforward ANN Traditionally weights initialized randomly learned backpropagation minimizing loss objective sharp contrast RFNs hidden layer weights sampled randomly distribution fixed hidden unit computes random feature input output layer weights trained Fig Note weights randomly drawn neurons response deterministic function input given weights Open separate window Fig Random feature networks structured weights study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distribution Mathematically hidden layer activations output given h W x h x R stimulus h h h h R hidden neuron responses R predicted output use rectified linear ReLU nonlinearity x max x applied entrywise Eq hidden layer weights W w w w R drawn randomly fixed readout weights trained RFNs RFN experiments train readout weights R offset R using support vector machine SVM classifier squared hinge loss penalty regularization strength tuned range fold crossvalidation RFNs include threshold hidden neurons although could help certain contexts vast majority studies RFNs neurons weights w R initialized iid spherical Gaussian distribution w N call networks built way classical unstructured RFNs Fig propose variation hidden weights initialized w N C C R positive semidefinite covariance matrix call networks structured RFNs Fig mean weights random specified covariance compare unstructured structured weights equal footing normalize covariance matrices Tr C Tr ensures mean square amplitude weights E w Receptive fields modeled linear weights Sensory neurons respond preferentially specific features inputs stimulus selectivity often summarized neurons receptive field describes features sensory space elicits responses stimulated Mathematically receptive fields modeled linear filter stimulus space Linear filters also integral component widely used LN model sensory processing According model firing rate neuron nonlinear function applied projection stimulus onto lowdimensional subspace linear filter linear filter model receptive fields explain responses individual neurons diverse stimuli used describe disparate sensory systems like visual auditory somatosensory systems diverse species including birds mammals insects stimuli uncorrelated filters estimated computing spike triggered average STA average stimulus elicited spike neuron stimuli correlated STA filter whitened inverse stimulus covariance matrix Often STAs denoised fitting parametric function STA Gabor wavelets simple cells V model receptive field neuron weight vector w nonlinear function Instead fitting parametric function construct covariance functions realization resulting Gaussian process resembles biological receptive field Fig Structured weights project filter input covariance eigenbasis generate network weights Gaussian processes GP whose covariance functions inspired receptive fields sensory neurons brain definition GP stochastic process finite observations follow Gaussian distribution find networks weights project inputs new basis filter irrelevant components see adds inductive bias classical RFNs tasks naturalistic inputs improves learning view weight vector w finitedimensional discretization continuous function w sample GP continuous function domain compact subset R assume discretized using grid equally spaced points w w Let input realvalued function x domain could represent finite timeseries image luminance retina complicated spatiotemporal sets like movie continuous setting dimensional inner product w x w x gets replaced L inner product w x w x Every GP fully specified mean covariance function C always assume mean zero study different covariance functions KosambiKarhunenLove theorem realization zeromean GP random series representation w z terms standard Gaussian random variables z N functions weights pairs eigenvalue eigenfunction pairs covariance operator C L L C f C f continuous analog covariance matrix C C positive definite opposed semidefinite eigenfunctions form complete basis L Using Eq inner product stimulus neurons weights w x z x z x z x x x Eq shows structured weights compute projection input x onto eigenfunction x reweight filter eigenvalue taking inner product random Gaussian weights z illuminating see continuous equations look like dimensional discrete setting Samples finitedimensional GP used hidden weights RFNs w N C First GP series representation Eq becomes w z matrices eigenvalues eigenvectors z N Gaussian random vector definition covariance matrix C E w w equal steps linear algebra Finally Eq analogous w x z x Since orthogonal matrix x equivalent change basis diagonal matrix shrinks expands certain directions perform filtering summarized following theorem Theorem Basis change formula Assume w N C C eigenvalue decomposition x R define x x w x z x z N Theorem says projecting input onto structured weight vector first filtering input GP eigenbasis random projection onto spherical random Gaussian form GP eigenbasis determined choice covariance function covariance function compatible input structure hidden weights filter irrelevant features noise stimuli amplifying descriptive features inductive bias facilitates inference stimuli downstream predictor spherical Gaussian distribution canonical choice unstructured RFNs simple way evaluate effective kernel structured RFNs k struct x x k unstruct x x see Appendix expression structured kernel provides concrete connection kernel theory learning using nonlinear neural networks readers interested kernel theories full example simulation results work given Appendix show exponential reduction number samples needed learn frequency detection using structured versus unstructured basis Fig Appendix Examples random yet structured receptive fields goal model weights artificial neurons way inspired biological neurons receptive fields Structured RFNs sample hidden weights GPs structured covariance construct covariance functions make generated weights resemble neuronal receptive fields start toy example stationary GP wellunderstood Fourier eigenbasis show receptive fields generated GP selective frequencies timeseries signals construct locally stationary covariance models insect mechanosensory V neuron receptive fields models shown good match experimental data Warmup Frequency selectivity stationary covariance illustrate results theoretical analysis start toy example temporal receptive fields selective particular frequencies example may familiar readers comfortable Fourier series basic signal processing Let input finite continuous timeseries x interval L use covariance function C k k cos k stationary process k k L k th natural frequency k weight coefficients covariance function Eq stationary means depends difference timepoints Applying compound angle formula get C k k cos k cos k sin k sin k Since sinusoidal functions cos k sin k form orthonormal basis L Eq eigendecomposition covariance eigenfunctions sines cosines eigenvalues k Eq know structured weights covariance form random series w k k z k cos k z k sin k z k z k N Thus receptive fields made sinusoids weighted k Gaussian variables z k z k Suppose want receptive fields retain specific frequency information signal filter rest Take k k k f lo k f hi call bandlimited spectrum passband f lo f hi bandwidth f hi f lo bandwidth increases receptive fields become less smooth since made wider range frequencies k nonzero decay certain rate rate controls smoothness resulting GP receptive fields act input signals x implicitly transform inputs Fourier basis filter frequencies based magnitude k bandlimited setting frequencies outside passband filtered makes receptive fields selective particular range frequencies ignore others hand classical random features weight frequencies equally even though natural settings high frequency signals corrupted noise Insect mechanosensors next consider particular biological sensor sensitive timehistory forces Campaniform sensilla CS domeshaped mechanoreceptors detect local stress strain insect exoskeleton embedded cuticle deformation cuticle bending torsion induces depolarizing currents CS opening mechanosensitive ion channels CS encode proprioceptive information useful body state estimation movement control diverse tasks like walking kicking jumping flying model receptive fields CS believed critical flight control namely ones found base halteres wings Fig Halteres wings flap rhythmically flight rotations insects body induce torsional forces felt active sensory structures CS detect small strain forces thereby encoding angular velocity insect body Experimental results show haltere wing CS selective broad range oscillatory frequencies STAs smooth oscillatory selective frequency decay time Fig B Open separate window Fig Random receptive field model insect mechanosensors Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shown model temporal receptive fields locally stationary GP bandlimited spectrum Examples receptive fields generated GP shown Fig C inputs CS modeled finite continuous timeseries x finite interval L neuron weights generated covariance function C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum k k L k th natural frequency warmup frequency selectivity weights accounted parameters f lo f hi bandwidth f hi f lo increases receptive fields built wider selection frequencies makes receptive fields less smooth Fig field localized near decay determined parameter increases receptive field selective larger time windows eigenbasis covariance function Eq similar Fourier eigenbasis modulated decaying exponential eigenbasis orthonormal basis span k e cos k k e sin k nonorthogonal set functions L hidden weights transform timeseries inputs eigenbasis discard components outside passband frequencies f lo f hi fit covariance model receptive field data CS neurons wings hawkmoth Manduca sexta data Briefly CS receptive fields estimated spiketriggered average STA experimental mechanical stimuli wings stimuli generated bandpassed white noise Hz characterize receptive fields population CS neurons compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field number samples case kHz ms samples normalization sets overall scale covariance matrix data covariance matrix shows tridiagonal structure Fig main diagonal positive diagonals negative diagonals decay away top left matrix Open separate window Fig Spectral properties mechanosensory RFs model similar compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behavior fit covariance model data optimize parameters f lo f hi finding f lo Hz f hi Hz ms best fit sensilla data minimizing Frobenius norm difference C data model see Appendix resulting model covariance matrix Fig B matches data covariance matrix Fig remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig B Appendix simulate effect finite number neurons generate weight vectors equal number neurons recorded recompute model covariance matrix Fig C call finite neuron model covariance matrix C finite shows bump bloblike structures evident C data C model result suggests bumpy structures attributed small number recorded neurons hypothesize effects would disappear larger dataset C data would closely resemble C model comparison also calculate Frobenius difference null models unstructured covariance model Fourier model unstructured model Frobenius norm difference Fourier model sensilla covariance model much lower difference compared null models fitting data accurately show covariance matrices sampled receptive fields null models Fig C E Appendix Comparing eigenvectors eigenvalues data model covariance matrices find spectral properties C model C finite similar C data eigenvalue curves models match data quite well Fig E curves directly comparable covariance normalized trace makes sum eigenvalues unity data model covariance matrices lowdimensional first data eigenvectors explain variance top explain top eigenvectors model finite sample match data quite well Fig Primary visual cortex turn visually driven neurons mammalian primary cortex Primary visual cortex V earliest cortical area processing visual information Fig neurons V detect small changes visual features like orientations spatial frequencies contrast size Open separate window Fig Random receptive field model Primary Visual Cortex V Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fields model receptive fields simple cells V clear excitatory inhibitory regions light shone excitatory regions increase cells response viceversa Fig B shape regions determines orientation selectivity widths determine frequency selectivity receptive fields centered location visual field decay away integrate visual stimuli within small region center Gabor functions widely used mathematical model receptive fields simple cells model receptive fields using another locally stationary GP show examples generated receptive fields Fig C Consider inputs cortical cells continuous twodimensional image x domain L L x R Since image realvalued x grayscale contrast single color channel pixel values neuron weights generated covariance function following form C exp f smooth receptive fields exp c c localized center c receptive field center defined c size receptive field determined parameter increases receptive field extends farther center c Fig Spatial frequency selectivity accounted bandwidth parameter f f decreases spatial frequency receptive field goes making weights less smooth Fig E eigendecomposition covariance function Eq leads orthonormal basis single scale Hermite wavelets c wavelet eigenfunctions Hermite polynomials modulated decaying Gaussian k e c H k c k c k H k k th physicists Hermite polynomial eigenfunctions nonzero centers c shifted versions Eq detailed derivation values constants c c c normalization Appendix use Eq model receptive field data V neurons recorded calcium imaging transgenic mice expressing GCaMPs mice headfixed running airfloating ball presented unique white noise images pixels using Psychtoolbox pixels white black equal probability Images upsampled resolution screens via bilinear interpolation stimulus corrected eyemovements online using custom code responses cells collected using twophoton mesoscope preprocessed using Suitep Receptive fields calculated white noise images deconvolved calcium responses cells using STA covariance analysis picked cells signaltonoise SNR threshold gave us cells SNR calculated smaller set images presented twice using method preprocessing step moved center mass every receptive field center visual field compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field case pixels pixels data covariance matrix resembles tridiagonal matrix However diagonals nonzero equally spaced segments Additionally values decay away center matrix show C data zoomed nonzero region around center matrix Fig corresponds pixel region around center full pixel matrix full covariance matrix shown Fig F Appendix Open separate window Fig Spectral properties V RFs model similar compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons red covariance model number offdiagonals center rate decay away center determined parameters f c respectively covariance pixels decays function distance c leads equallyspaced nonzero segments hand covariance also decays function distance pixels brings diagonal structure model frequency parameter f increases number offdiagonals increases Pixels generated weights become correlated weights become spatially smoother size parameter increases diagonals decay slower center c increasing correlations center pixel leading significant weights occupy visual field optimize parameters fit data finding f pixels minimizing Frobenius norm difference C data model need optimize center parameter c since preprocess data receptive fields centered c center grid resulting model covariance matrix Fig B data covariance matrix Fig match remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig G Appendix simulate effect finite number neurons generate weights equal number neurons data compute C finite shown Fig C finite matrix C finite looks even like C data shows negative covariances far center result finite sample size comparison also calculate normalized Frobenius difference null models unstructured covariance model translation invariant V model translation invariant model remove spatially localizing exponential Eq fit spatial frequency parameter f unstructured model Frobenius norm difference translation invariant model V covariance model much lower difference better fit data show covariance matrices sampled receptive fields null models Fig H J Appendix Similar spectral properties evident eigenvectors eigenvalues C model C finite C data analytical forms derived Eq Fig E covariances normalized unit trace Note analytical eigenfunctions shown finer grid model data analysis performed continuous space differences eigenfunctions eigenvalues analytical model results due discretization Examining eigenvectors Fig also see good match although rotations differences ordering eigenvectors explain variance receptive field data reference top eigenvectors explain variance data variance model eigenvalue curves models analytical forms match data Fig E reasonably well although well mechanosensors Appendix repeat analysis receptive fields measured different stimulus sets mouse different experimental dataset nonhuman primate V findings consistent results shown Fig K P Appendix Advantages structured random weights artificial learning tasks hypothesis neuronal inductive bias structured receptive fields allows networks learn fewer neurons training examples steps gradient descent classification tasks naturalistic inputs examine hypothesis compare performance structured receptive fields classical ones several classification tasks find artificial learning tasks structured random networks learn accurately smaller network sizes fewer training examples gradient steps Frequency detection CS naturally encode timehistory strain forces acting insect body sensors inspired temporal filtering properties shown accurately classify spatiotemporal data Inspired result test sensillainspired mechanosensory receptive fields timeseries classification task Fig top example presented network ms timeseries sampled kHz goal detect whether example contains sinusoidal signal positive examples sinusoidal signals f Hz corrupted noise SNR dB negative examples Gaussian white noise matched amplitude positive examples Note frequency detection task linearly separable random phases positive negative examples See Appendix additional details including definition SNR crossvalidation used find optimal parameters f lo Hz f hi Hz ms Open separate window Fig Random mechanosensory weights enable learning fewer neurons timeseries classification tasks show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible task number hidden neurons structured RFN significantly outperforms classical RFN show test performance using tuned parameters Fig Even noisy task achieves test error using hidden neurons Meanwhile classical network takes neurons achieve similar error Predictably performance suffers weights incompatible task show results f lo Hz f hi Hz Fig incompatible RFN performs better chance error much worse classical RFN takes neurons achieve test error test error decrease level even additional hidden neurons Frequency XOR task challenge mechanosensorinspired networks difficult task build frequency ExclusiveOR XOR problem Fig B top XOR binary function returns true inputs different otherwise returns false XOR classical example function linearly separable thus harder learn inputs ms timeseries sampled kHz inputs either contain pure frequency f Hz f Hz mixed frequency signals f f white noise pure mixed frequency cases add noise SNR See Appendix details goal task output true input contains either pure tone false input contains mixed frequencies white noise tune GP covariance parameters f lo f hi Eq using crossvalidation cross validation procedure algorithmic details identical frequency detection task Using cross validation find optimal parameters f lo Hz f hi Hz ms incompatible weights take f lo Hz f hi Hz structured RFN significantly outperform classical RFN number hidden neurons show network performance using parameters Fig B Classification error achieved hidden neurons sharp contrast classical RFN requires hidden neurons achieve error incompatible weights network needs neurons achieve test error improve larger network sizes four input subclasses consistently fails classify pure Hz sinusoidal signals outside passband Image classification next test Vinspired receptive fields two standard digit classification tasks MNIST KMNIST MNIST KMNIST datasets contain images handwritten digits MNIST Arabic numerals whereas KMNIST Japanese hiragana phonetic characters datasets come split training test examples classes training examples per class Every example grayscale image centered characters hidden weight center c chosen uniformly random pixels ensures networks weights uniformly cover image space fact means network represent sum locallysmooth functions see Appendix use network hidden neurons tune GP covariance parameters f Eq using fold cross validation MNIST training set parameter ranges pixels optimal parameters found grid search find optimal parameters pixels f pixels refit optimal model using entire training set parameters MNIST used KMNIST task without additional tuning Vinspired achieves much lower average classification error compared classical RFN number hidden neurons show learning performance using parameters MNIST task Fig achieve error MNIST task requires neurons versus neurons classical RFN structured RFN achieves error neurons Qualitatively similar results hold KMNIST task Fig B although overall errors larger reflecting harder task achieve error KMNIST requires neurons versus neurons classical RFN structured RFN achieves error neurons Open separate window Fig Random V weights enable learning fewer neurons fewer examples digit classification tasks show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properly network performance suffers GP covariance parameters match task happens size parameter smaller stroke width spatial scale f doesnt match stroke variations character Taking incompatible parameters f Fig B structured weights performs worse classical RFN tasks hidden neurons achieves relatively poor test errors MNIST Fig KMNIST Fig B Structured weights improve generalization limited data Alongside learning fewer hidden neurons V structured RFNs also learn accurately fewer examples test fewshot learning using image classification datasets training examples reduced training examples per class test set GP parameters remain Structured encodings allow learning fewer samples unstructured encodings show fewshot learning results Fig C networks performance saturate past hundred hidden neurons MNIST lowest error achieved V structured RFN versus classical RFN using incompatible weights Fig C structured network acheives error using structured features KMNIST task opposed classical RFN using incompatible weights Fig Networks train faster initialized structured weights study effect structured weights initialization strategy fullytrained neural networks weights network vary hypothesized structured initialization allows networks learn faster ie training loss test error would decrease faster unstructured weights shown performance RFNs improves biologically inspired weight sampling However RFNs Eq readout weights modified training hidden weights W frozen initial value compare biologicallymotivated initialization classical initialization variance inversely proportional number hidden neurons w unstruct N initialization widely known Kaiming normal scheme thought stabilize training dynamics controlling magnitude gradients classical approach ensures Tr fair comparison scale structured weight covariance matrix Tr C studies RFNs trace equal weight scale absorbed readout weights due homogeneity ReLU compare structured unstructured weights MNIST KMNIST tasks common benchmarks fullytrained networks architecture single hidden layer feedforward neural network Fig hidden neurons crossentropy loss training sets minimized using simple gradient descent GD epochs fair comparison learning rate optimized network separately define area training loss curve metric speed learning perform grid search range e e learning rate minimizes metric resulting parameters structured unstructured incompatible networks respectively parameters image classification MNIST KMNIST tasks Vinitialized network minimizes loss function faster classically initialized network MNIST task V network achieves loss value epochs compared network Fig see qualitatively similar results KMNIST task end training Vinspired networks loss classically initialized network reaches Fig B find Vinitialized network performs better classical initialization covariance parameters match task incompatible parameters Vinitialized network achieves loss value MNIST KMNIST Open separate window Fig V weight initialization fullytrained networks enables faster training digit classification tasks show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible task minimize training loss faster Vinitialized network also generalizes well achieves lower test error end training MNIST achieves test error compared error classically initialized network using incompatible weights Fig C KMNIST see error compared error classical initalization using incompatible weights Fig see similar results across diverse hidden layer widths learning rates Fig Q Appendix benefits evident wider networks smaller learning rates Furthermore structured weights show similar results trained epochs rate neurons shown optimizers like minibatch Stochastic Gradient Descent SGD ADAM batch size rate neurons shown Structured initialization facilitates learning across wide range networks However improvement universal significant benefit found initializing early convolutional layers deep network AlexNet applying ImageNet dataset shown Appendix Fig U Appendix large amounts training data fact small fraction network initialized structured weights could explain null result Also many scenarios incompatible structured weights get performance par compatible ones end training poor inductive bias overcome Improving representation structured random weights shown structured receptive field weights improve performance RFNs fullytrained networks number supervised learning tasks long receptive fields compatible task performance gains unstructured features possible incompatible networks performs better even worse using classical unstructured weights results understood theoretical framework Structured weights effectively cause input x undergo linear transformation new representation x following Theorem examples new representation bandlimited due design covariance function V weights eigenvalues nonzero spectrum decays exponentially acts lowpass filter moving bandlimited representation filter noisehighfrequency componentsand reduce dimensionalitycoordinates x outside passband zero general noise dimensionality make learning harder easiest understand effects frequency detection task simplicity assume using stationary features warmup frequency detection task signal power contained f Hz frequency everything else due noise weights compatible task means w sum sines cosines frequencies k passband includes f narrower make bandwidth still retaining signal higher SNR x becomes since noise filtered see Appendix Discussion paper describe random generative model receptive fields sensory neurons Specifically model receptive field random filter sampled Gaussian process GP covariance structure matched statistics experimental neural data show two kinds sensory neuronsinsect mechanosensory simple cells mammalian Vhave receptive fields welldescribed GPs particular generated receptive fields secondorder statistics principal components match receptive field data Theoretically show individual neurons perform randomized transformation filtering inputs connection provides framework sensory neurons compute input transformations like Fourier wavelet transforms biologically plausible way numerical results using structured random receptive fields show offer better learning performance unstructured receptive fields several benchmarks structured networks achieve higher test performance fewer neurons fewer training examples unless frequency content receptive fields incompatible task networks fully trained initializing structured weights leads better network performance measured training loss generalization fewer iterations gradient descent Structured random features may understood theoretically transforming inputs informative basis retains important information stimulus filtering away irrelevant signals Modeling sensory neurons modalities random feature formulation natural extension traditional linearnonlinear LN neuron model approach may applied brain regions LN models successful instance sensory areas primarily feedforward connectivity like somatosensory auditory regions neurons auditory somatosensory systems selective spatial temporal structures stimuli spatial structure emerges networks trained artificial tactile tasks receptive fields could modeled GPs spatiotemporal covariance functions could useful artificial tasks spatiotemporal stimuli movies multivariate timeseries Neurons localized random temporal responses found compatible manifold coding decisionmaking task GPs complementary approach traditional sparse coding efficient coding hypotheses connections theories interesting future research Receptive fields development generative model offers new directions explore biological basis computational principles behind receptive fields Development lays basic architecture conserved animal animal yet details every neural connection specified leading amount inevitable randomness least initially receptive fields random constrained covariance natural ask biology implements Unsupervised Hebbian dynamics local inhibition allow networks learn principal components input interesting future direction similar learning rules may give rise overcomplete nonorthogonal structure similar studied may prove biologically plausible weights result taskdriven optimization assumes receptive field properties actually lie within synaptic weights spatial receptive fields assumption plausible temporal properties receptive fields likely result neurons intrinsic dynamics LN framework model Heterogeneous physiological eg resonator dynamics mechanical position shape mechanosensor relative body structure properties combine give diverse temporal receptive field structures Development thus leverages different mechanisms build structure receptive field properties sensory neurons Connections compressive sensing Random projections seen extensive use field compressive sensing highdimensional signal found measurements long sparse representation Random compression matrices known optimal properties however many cases structured randomness realistic Recent work shown structured random projections local wiring constraints one dimension compatible dictionary learning supporting previous empirical results work shows structured random receptive fields equivalent employing wavelet dictionary dense Gaussian projection Machine learning inductive bias important open question neuroscience machine learning certain networks characterized features architecture weights nonlinearities better others certain problems One perspective network good problem biased towards approximating functions close target known inductive bias depends alignment features encoded neurons task hand approach shows structured receptive fields equivalent linear transformation input build biases Furthermore describe nonlinear properties network using kernel varies depending receptive field structure target function small norm RKHS inductive bias easier learn small norm RKHS means target function varies smoothly inputs Smooth functions easier learn compared fast varying ones way receptive field structure influences ease learning target function conjecture receptive fields neuralinspired distributions affect RKHS geometry target functions norm small RKHS compared RKHS random whitenoise receptive fields leave future work verify conjecture detail Networks endowed principles neural computation like batch normalization pooling inputs residual connections found contain inductive biases certain learning problems Learning datadependent kernels another way add inductive bias also saw initializing fullytrained networks generative models improved speed convergence generalization compared unstructured initialization result consistent known results initialization effect generalization initialization literature mostly focused avoiding explodingvanishing gradients conjecture inductive bias structured connectivity places network closer good solution loss landscape random Vinspired receptive fields model seen similar happens convolutional neural network CNN similarities differences compared brains recent study showed CNNs fixed Vlike convolutional layer robust adversarial perturbations inputs similar vein work using randomly sampled Gabor receptive fields first layer deep network also shown improve performance wavelet scattering transform multilayer network wavelet coefficients passed nonlinearities model similar deep CNNs framework differs randomized model yields wavelets single scale similar studies robustness learning deep networks weights possible Adding layers model sampling weights variety spatial frequencies field sizes would yield random networks behave similar scattering transform offering another connection brain CNNs Directly learning filters Hermite wavelet basis led good perfomance ANNs little data idea extended multiple scales structured random features seen RFN version ideas supporting evidence principles used biology Limitations future directions several limitations random feature approach model neuron responses scalar firing rates instead discrete spikes ignore complex neuronal dynamics neuromodulatory context many details Like LN models random feature model assumes zero plasticity hidden layer neurons However associative learning drive changes receptive fields individual neurons sensory areas like V auditory cortex RFN purely feedforward account feedback connections Recent work suggests feedforward architecture lacks sufficient computational power serve detailed inputoutput model network cortical neurons might need additional layers convolutional filters difficult interpret parameters found fitting receptive field data connect experimental conditions Also GP model weights captures covariance second moments neglects higherorder statistics remains shown theory yield concrete predictions tested vivo experimental conditions random feature receptive field model randomized extension LN neuron model LN model fits parameterized function receptive field contrast random feature framework fits distribution entire population receptive fields generates realistic receptive fields distribution natural question compare goal capture individual differences neuronal receptive fields one resort LN model neurons receptive field fit data random feature model flexible provides direct connection random feature theory mathematically tractable generative connection kernel learning opens door using techniques mainstay machine learning theory literature instance estimate generalization error sample complexity context learning biologically realistic networks see several future directions structured random features connecting computational neuroscience machine learning already stated auditory somatosensory tactile regions good candidates study well developmental principles could give rise random yet structured receptive field properties account plasticity hidden layer one could also analyze neural tangent kernel NTK associated structured features kernels often used analyze ANNs trained gradient descent number hidden neurons large step size small incorporate lateral feedback connections weights could sampled GPs recurrent covariance functions theory may also help explain CNNs fixed Vlike convolutional layer robust adversarial input perturbations filtering high frequency corruptions seems likely structured random features also robust would interesting study intermediate layer weights fullytrained networks approximate samples GP studying covariance structure Finally one could try develop covariance functions optimize RFNs sophisticated learning tasks see near high performancelower error faster training etcon difficult tasks possible Methods methods described throughout Results section details additional results Appendix Supporting information Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF Click additional data file pdf Acknowledgments thank Dario Ringach providing macaque V data Brandon Pratt hawkmoth mechanosensor data grateful Ali Weber Steven Peterson Owen Levin Alice C Schwarze useful discussions thank Sarah Lindo Michalis Michaelos Carsen Stringer help mouse surgeries calcium imaging data processing respectively Funding Statement BP supported UW Applied Math Frederic Wan Endowed Fellowship Terry Keegan Memorial ARCS Endowment Fellowship Natural Science Foundation Graduate Research Fellowship Program Grant DGE MP supported Janelia Research Campus Howard Hughes Medical Institute BWB supported grants FA FA Air Force Office Scientific Research KDH supported Washington Research Foundation postdoctoral fellowship Western Washington University funders role study design data collection analysis decision publish preparation manuscript httpsamathwashingtonedusupportus httpswwwarcsfoundationorgnationalhomepage httpswwwnsfgrfporg httpswwwjaneliaorglabpachitariulab httpswwwafrlafmilAFOSR httpswwwwrfseattleorggrantswrfpostdoctoralfellowships httpscswwueduharri Data Availability relevant data within manuscript Supporting information files References Yuste R neuron doctrine neural networks Nature Reviews Neuroscience doi nrn PubMed CrossRef Google Scholar Fusi Miller EK Rigotti neurons mix high dimensionality higher cognition Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Saxena Cunningham JP Towards neural population doctrine Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Stringer C Pachitariu Steinmetz N Carandini Harris KD Highdimensional geometry population responses visual cortex Nature doi PMC free article PubMed CrossRef Google Scholar Sherrington C Integrative Action Nervous System Cambridge University Press Google Scholar Chichilnisky EJ simple white noise analysis neuronal light responses Network Computation Neural Systems PubMed Google Scholar Sakai HM Naka K Signal transmission catfish retina V Sensitivity circuit Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Clay Reid R Alonso JM Specificity monosynaptic connections thalamus visual cortex Nature doi PubMed CrossRef Google Scholar Jones JP Palmer LA evaluation twodimensional Gabor filter model simple receptive fields cat striate cortex Journal Neurophysiology PubMed Google Scholar Knudsen EI Konishi Centersurround organization auditory receptive fields owl Science doi science PubMed CrossRef Google Scholar H Central mechanisms tactile shape perception Current opinion neurobiology PubMed Google Scholar Rusanen J Frolov R Weckstrm Kinoshita Arikawa K Nonlinear amplification graded voltage signals firstorder visual interneurons butterfly Papilio xuthus Journal Experimental Biology PubMed Google Scholar Fox JL Fairhall AL Daniel TL Encoding properties haltere neurons enable motion feature detection biological gyroscope Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Pratt B Deora Mohren Daniel Neural evidence supports dual sensorymotor role insect wings Proceedings Royal Society B Biological Sciences doi rspb PMC free article PubMed CrossRef Google Scholar Clemens J Ronacher B Feature Extraction Integration Underlying Perceptual Decision Making Courtship Behavior Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Park Pillow JW Receptive field inference localized priors PLoS computational biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Bonin V Histed MH Yurgenson Reid RC Local Diversity FineScale Organization Receptive Fields Mouse Visual Cortex Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Rosenblatt F perceptron probabilistic model information storage organization brain Psychological Review doi h PubMed CrossRef Google Scholar Caron SJC Ruta V Abbott LF Axel R Random convergence olfactory inputs Drosophila mushroom body Nature doi nature PMC free article PubMed CrossRef Google Scholar LitwinKumar Harris KD Axel R Sompolinsky H Abbott LF Optimal Degrees Synaptic Connectivity Neuron e doi jneuron PMC free article PubMed CrossRef Google Scholar Broomhead DS Lowe Radial basis functions multivariable functional interpolation adaptive networks Royal Signals Radar Establishment Malvern United Kingdom Google Scholar Igelnik B Pao YH Stochastic choice basis functions adaptive function approximation functionallink net IEEE transactions neural networks doi PubMed CrossRef Google Scholar Rahimi Recht B Random Features LargeScale Kernel Machines Platt JC Koller Singer Roweis ST editors Advances Neural Information Processing Systems Curran Associates Inc p Google Scholar Liu F Huang X Chen Suykens JAK Random Features Kernel Approximation Survey Algorithms Theory Beyond arXiv cs stat PubMed Arora Du SS Hu W Li Z Wang R FineGrained Analysis Optimization Generalization Overparameterized TwoLayer Neural Networks arXiv cs stat Arora Du SS Hu W Li Z Salakhutdinov R Wang R Exact Computation Infinitely Wide Neural Net arXiv cs stat Chen L Xu Deep Neural Tangent Kernel Laplace Kernel RKHS arXiv cs math stat Neal RM Priors Infinite Networks New York NY Springer New York p Available CrossRef Google Scholar Williams CKI Computation Infinite Neural Networks Neural Computation doi CrossRef Google Scholar Rahimi Recht B Uniform approximation functions random bases th Annual Allerton Conference Communication Control Computing IEEE p Available httpieeexploreieeeorgdocument Bordelon B Canatar Pehlevan C Spectrum Dependent Learning Curves Kernel Regression Wide Neural Networks arXiv cs stat Bordelon B Pehlevan C Population Codes Enable Learning Examples Shaping Inductive Bias bioRxiv Google Scholar Canatar Bordelon B Pehlevan C Spectral Bias TaskModel Alignment Explain Generalization Kernel Regression Infinitely Wide Neural Networks arXiv condmat stat PMC free article PubMed Harris KD Additive function approximation brain arXiv cs qbio stat Hashemi Schaeffer H Shi R Topcu U Tran G Ward R Generalization Bounds Sparse Random Feature Expansions arXiv cs math stat Xie Muscinelli Decker Harris K LitwinKumar Taskdependent optimal representations cerebellar learning bioRxiv Google Scholar Jacobsen JH van Gemert J Lou Z Smeulders AWM Structured Receptive Fields CNNs arXiv cs Paninski L Convergence properties spiketriggered analysis techniques Network Computation Neural Systems p PubMed Google Scholar Rasmussen CE Williams CKI Gaussian Processes Machine Learning Adaptive Computation Machine Learning MIT Press Google Scholar Kosambi DD Statistics function space Journal Indian Mathematical Society New Series Google Scholar Wahba G Spline Models Observational Data SIAM Dickerson BH Fox JL Sponberg Functional diversity generic encoding insect campaniform sensilla Current Opinion Physiology doi jcophys CrossRef Google Scholar Yarger Fox JL Dipteran Halteres Perspectives Function Integration Unique Sensory Organ Integrative Comparative Biology doi icbicw PubMed CrossRef Google Scholar Fox JL Daniel TL neural basis gyroscopic force measurement halteres Holorusia Journal Comparative Physiology doi sz PubMed CrossRef Google Scholar Genton MG Classes Kernels Machine Learning Statistics Perspective Journal Machine Learning Research Dec Google Scholar Hubel DH Wiesel TN Receptive fields single neurones cats striate cortex Journal Physiology doi jphysiolsp PMC free article PubMed CrossRef Google Scholar Marr Hildreth E Brenner Theory edge detection Proceedings Royal Society London Series B Biological Sciences PubMed Google Scholar Martens JB Hermite transformtheory IEEE Transactions Acoustics Speech Signal Processing doi CrossRef Google Scholar Kleiner Brainard Pelli Whats new Psychtoolbox PerceptionECVP Abstract Supplement European Conference Visual Perception ECVP August Arezzo Italy Sofroniew NJ Flickinger King J Svoboda K large field view twophoton mesoscope subcellular resolution vivo imaging Elife e doi eLife PMC free article PubMed CrossRef Google Scholar Pachitariu Stringer C Dipoppa Schrder Rossi LF Dalgleish H et al Suitep beyond neurons standard twophoton microscopy BioRxiv Google Scholar Mohren TL Daniel TL Brunton SL Brunton BW Neuralinspired sensors enable sparse efficient classification spatiotemporal data Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar LeCun Cortes C Burges C MNIST handwritten digit database ATT Labs Online Available httpyannlecuncomexdbmnist Google Scholar Clanuwat BoberIrizar Kitamoto Lamb Yamamoto K Ha Deep Learning Classical Japanese Literature arXiv cs stat K Zhang X Ren Sun J Delving Deep Rectifiers Surpassing HumanLevel Performance ImageNet Classification IEEE International Conference Computer Vision ICCV p Krizhevsky One weird trick parallelizing convolutional neural networks arXiv cs Russakovsky Deng J Su H Krause J Satheesh et al ImageNet Large Scale Visual Recognition Challenge International Journal Computer Vision doi sy CrossRef Google Scholar Pruszynski JA Johansson RS Edgeorientation processing firstorder tactile neurons Nature Neuroscience doi nn PubMed CrossRef Google Scholar Zhao CW Daley MJ Pruszynski JA Neural network models tactile system develop firstorder units spatially complex receptive fields PLOS ONE e doi journalpone PMC free article PubMed CrossRef Google Scholar Xu Ruan C Korpeoglu E Kumar Achan K Temporal Kernel Approach Deep Learning Continuoustime Information arXiv cs Koay SA Charles Thiberge SY Brody CD Tank DW Sequential efficient neuralpopulation coding complex task information bioRxiv PubMed Google Scholar Olshausen BA Field DJ Sparse Coding Overcomplete Basis Set Strategy Employed V Vision Research doi PubMed CrossRef Google Scholar Barlow HB et al Possible principles underlying transformation sensory messages Sensory communication Google Scholar Chalk Marre Tkaik G Toward Unified Theory Efficient Predictive Sparse Coding Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Swanson LW Brain architecture Understanding basic plan vol xv New York NY US Oxford University Press Google Scholar Strausfeld NJ Arthropod Brains Evolution Functional Elegance Historical Significance Harvard University Press Available httpswwwjstororgstablejctvdpvh Zador critique pure learning artificial neural networks learn animal brains Nature Communications doi PMC free article PubMed CrossRef Google Scholar Oja E Principal components minor components linear neural networks Neural Networks doi CrossRef Google Scholar Pehlevan C Sengupta Chklovskii DB Similarity Matching Objectives Lead HebbianAntiHebbian Networks Neural Computation doi necoa PubMed CrossRef Google Scholar Ringach DL Haphazard Wiring Simple Receptive Fields Orientation Columns Visual Cortex Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Ostojic Brunel N Spiking Neuron Models LinearNonlinear Models PLOS Computational Biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Weber AI Pillow JW Capturing Dynamical Repertoire Single Neurons Generalized Linear Models Neural Computation doi necoa PubMed CrossRef Google Scholar Fairhall receptive field dead Long live receptive field Current Opinion Neurobiology ixxii doi jconb PMC free article PubMed CrossRef Google Scholar Barth FG Mechanics preprocess information fine tuning mechanoreceptors Journal Comparative Physiology doi sz PMC free article PubMed CrossRef Google Scholar Eldar YC Kutyniok G editors Compressed Sensing Theory Applications Cambridge University Press Google Scholar Foucart Rauhut H Mathematical Introduction Compressive Sensing Birkhuser Basel Ganguli Sompolinsky H Compressed Sensing Sparsity Dimensionality Neuronal Information Processing Data Analysis Annual Review Neuroscience doi annurevneuro PubMed CrossRef Google Scholar Fallah K Willats AA Liu N Rozell CJ Learning sparse codes compressed representations biologically plausible local wiring constraints bioRxiv Google Scholar Barranca VJ Kovai G Zhou Cai Improved Compressive Sensing Natural Scenes Using Localized Random Sampling Scientific Reports doi srep PMC free article PubMed CrossRef Google Scholar ShaweTaylor J Cristianini N Kernel Methods Pattern Analysis Cambridge University Press Google Scholar ShalevShwartz BenDavid Understanding Machine Learning Theory Algorithms Cambridge University Press Google Scholar Yamins DLK Hong H Cadieu CF Solomon EA Seibert DiCarlo JJ Performanceoptimized hierarchical models predict neural responses higher visual cortex Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar K Zhang X Ren Sun J Deep Residual Learning Image Recognition arXiv cs Sinha Duchi JC Learning Kernels Random Features Lee Sugiyama Luxburg U Guyon Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileedcbbafbacbePaperpdf Google Scholar Arpit Campos V Bengio Initialize Network Robust Initialization WeightNorm ResNets Wallach H Larochelle H Beygelzimer dAlchBuc F Fox E Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileefaccdaPaperpdf Google Scholar Glorot X Bengio Understanding difficulty training deep feedforward neural networks Proceedings Thirteenth International Conference Artificial Intelligence Statistics JMLR Workshop Conference Proceedings p Available httpproceedingsmlrpressvglorotahtml Olah C Mordvintsev Schubert L Feature Visualization Distill Lindsay GW Convolutional Neural Networks Model Visual System Past Present Future Journal Cognitive Neuroscience p PubMed Google Scholar Dapello J Marques Schrimpf Geiger F Cox DiCarlo JJ Simulating Primary Visual Cortex Front CNNs Improves Robustness Image Perturbations Advances Neural Information Processing Systems Google Scholar Illing B Gerstner W Brea J Biologically plausible deep learningBut far go shallow networks Neural Networks doi jneunet PubMed CrossRef Google Scholar Mallat Group Invariant Scattering Communications Pure Applied Mathematics doi cpa CrossRef Google Scholar Bruna J Mallat Invariant Scattering Convolution Networks IEEE Transactions Pattern Analysis Machine Intelligence doi TPAMI PubMed CrossRef Google Scholar Andn J Mallat Deep Scattering Spectrum IEEE Transactions Signal Processing doi TSP CrossRef Google Scholar Pintea SL Tomen N Goes SF Loog van Gemert JC Resolution learning deep convolutional networks using scalespace theory arXiv cs PubMed Goltstein PM Meijer GT Pennartz CM Conditioning sharpens spatial representation rewarded stimuli mouse primary visual cortex eLife e doi eLife PMC free article PubMed CrossRef Google Scholar Fritz J Shamma Elhilali Klein Rapid taskrelated plasticity spectrotemporal receptive fields primary auditory cortex Nature Neuroscience doi nn PubMed CrossRef Google Scholar Beniaguev Segev London Single cortical neurons deep artificial neural networks Neuron e doi jneuron PubMed CrossRef Google Scholar Jacot Gabriel F Hongler C Neural Tangent Kernel Convergence Generalization Neural Networks Advances Neural Information Processing Systems vol Curran Associates Inc Available httpspapersnipsccpaperhashabefaebbaecbdfaAbstracthtml Google Scholar Mattos CLC Dai Z Damianou Forth J Barreto GA Lawrence ND Recurrent Gaussian Processes arXiv cs stat Ringach DL Spatial structure symmetry simplecell receptive fields macaque primary visual cortex Journal Neurophysiology Jul doi jn PubMed CrossRef Google Scholar PLoS Comput Biol Oct e Decision Letter Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Mar Dear Mr Pandey Thank much submitting manuscript Structured random receptive fields enable informative sensory encodings consideration PLOS Computational Biology papers reviewed journal manuscript reviewed members editorial board several independent reviewers light reviews email would like invite resubmission significantlyrevised version takes account reviewers comments make decision publication seen revised manuscript response reviewers comments revised manuscript also likely sent reviewers evaluation ready resubmit please upload following letter containing detailed list responses review comments description changes made manuscript Please note forming response article accepted may opportunity make peer review history publicly available record include editor decision letters reviews responses reviewer comments eligible contact opt Two versions revised manuscript one either highlights tracked changes denoting text changed clean version uploaded manuscript file Important additional instructions given reviewer comments Please prepare submit revised manuscript within days anticipate delay please let us know expected resubmission date replying email Please note revised manuscripts received day due date may require evaluation peer review similar newly submitted manuscripts Thank submission hope editorial process constructive far welcome feedback time Please dont hesitate contact us questions comments Sincerely Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer authors make two main contributions First modeling receptive fields independent draws Gaussian process Second arguing structured receptive fields beneficial efficient learning downstream authors prove theorem provides interpretation random receptive fields performing filtering eigenbasis Gaussian process inductive bias argued reason downstream predictors performance may enhanced receptive fields compatible learning problem contributions potentially important neither two ideas sufficiently motivated developed Therefore paper feels light content suggestions improvement need modeling receptive fields random draws motivated found strange introduction paper reviews wide range literature including even technical references reproducing kernel Hilbert spaces whose relevance questionable main motivation work short sentence referring data ref went paper see motivates authors approach advantage modeling receptive fields random draws authors provide evidence models capture second order statistics experimental receptive fields clear advantage modeling approach compared say fitting LN model every neuron authors state performance metrics make comparisons relevant techniques Authors mention RKHSs times even theoretical discussion Appendix However clear connection important relevant Reading paragraph equation sorry line numbers provided discussion section feels like authors want claim learning efficiency results related learning RKHSs would nice see point fully developed Instead MNIST authors could use dataset naturalistic complex stimuli theoretical study makes receptive field covariance compatible incompatible downstream learning pursued may related RKHS picture authors hinting authors great list possible future directions One directions could pursued paper Black line Figure visible Reviewer submitted article authors present novel framework modeling receptive fields sensory cortices introducing concept structured receptive field drawn biased probability distribution model sensory neurons interpretable fashion still incorporating randomness crucial biological systems article broadly split two parts First structured receptive fields used fit experimental data biological receptive fields suitability machine learning applications discussed results interesting novel described points respects analysis explication results could improved Major points results matching biological receptive fields based qualitativenot quantitativecomparisons authors compare covariance matrices experimental data fitted model based similar look plot One way results might strengthened including null hypothesis like purely random receptive fields receptive fields based Gabor filters V authors nice job motivating theory theory random feature networks machine learning literature would also liked see authors discuss detail ideas context previous computational neuroscience approaches describing receptive fields including efficient coding sparse coding related approaches establishing structured weights lead faster learning Section convincing way show view would define metric speed learning eg area learning curve ii optimize learning rate separately network respect metric iii compare metric two networks separately optimized learning rates way simulations currently done leaves open possibility slower learning unstructured case might due suboptimal choice learning rate Minor points section authors explain L regularization parameter set possible presented results would change parameter set differently first paragraph Section claim made without specifying particular task adding inductive bias improves learning authors careful statements like elsewhere paper since obviously aware given illustrate point simulations inductive bias expected improve learning well matched particular computational task actually worsen learning case Section authors state GP covariance function reflects statistical regularities within sensory inputs network authors provide citation statement cases considered secondtolast paragraph section bandwidth fhi flo equation section stochastic coefficients cosomegak sinomegak independent end section authors precisely explain statement smoothness also controlled overall magnitude nonzero eigenvalues describing computation Cdata Section explanation meant centered receptive fields could helpful Section unit pixels seems incorrect since squared pixel generally meant pixel explanation mechanism leads prominent dark patches covariance matrices shown Figure would appreciated result Figures E extent Figure authors mean remarkably well might addressed response major point Figure E authors comment faster decay spectrum model Could due noise effect related calcium imaging text describing Figure stated Cdata shown zoomed nothing mentioned caption mention scale given one look supplemental figure get still imprecise idea whats going Given dark patches still clearly visible full covariance plotted supplemental figure would recommend either showing full version instead Figure describing zoomedin version clearly plots machine learning applications authors add green curves fact show higher test errors blue ones tautological since parameters latter optimized precisely minimize test error may value curves point illustrate poorly chosen inductive bias worse inductive bias doesnt seem borne pairs green vs orange curves figures doesnt seem point made text last paragraph appendix authors claim functions small Hnorm easier learn larger norm statement made precise perhaps specific regression algorithm Reviewer paper authors considered receptive fields random samples parametrized distributions gaussian process particular following modalities insect mechanosensors mammalian visual cortex neurons demonstrated random feature neurons RFN remove high frequency noise boost signals Finally also show RFNs enable efficient learning number neurons training time perspective topic general interest visual neuroscience growing subfield within computational neuroscience intersecting artificial neural networks comments questions motivations implications work attached theoretical analysis equivalence structured weight vector transformation filtering Gaussian Processing eigenbasis random projection onto spherical random gaussian interesting Aside connection kernel theory learning broader implications finding either sensory neuroscience deep learning fields would helpful get discussion around topic revised version motivation using Gaussian Process type generative model mathematical andor neuro perspectives possible sampling random filters Gaussian Process mathematically tractable also biologically plausible compared say learning filters taskdriven optimization would helpful get discussion around revised version surprising filter samples GPs covariance real data match various properties real neural populations might helpful control models fail capture biological fidelity example Fig revised version Minor comment name structured random receptive field properly defined paper bit vague initially thought random implies stochastic Dapello et al Overall think paper interesting recommend acceptance revisions authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes Reviewer None PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer Reviewer Figure Files revising submission please upload figure files Preflight Analysis Conversion Engine PACE digital diagnostic tool httpspacevapexcovantagecom PACE helps ensure figures meet PLOS requirements use PACE must first register user login navigate UPLOAD tab find detailed instructions use tool encounter issues questions using PACE please email us grosolpserugif Data Requirements Please note condition publication PLOS data policy requires make available data used draw conclusions outlined manuscript Data must deposited appropriate repository included within body manuscript uploaded supporting information includes numerical values used generate graphs histograms etc example PLOS Biology see httpwwwplosbiologyorgarticleinfoAdoiFFjournalpbios Reproducibility enhance reproducibility results recommend deposit laboratory protocols protocolsio protocol assigned identifier DOI cited independently future Additionally PLOS ONE offers option publish peerreviewed clinical study protocols Read information sharing protocols httpsplosorgprotocolsutmmediumeditorialemailutmsourceauthorlettersutmcampaignprotocols PLoS Comput Biol Oct e Author response Decision Letter Oct e Published online Oct doi journalpcbir Author response Decision Letter Copyright License information Disclaimer Copyright notice Jun Attachment Submitted filename coverletterresubmissionpdf Click additional data file pdf PLoS Comput Biol Oct e Decision Letter Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Aug Dear Dr Harris pleased inform manuscript Structured random receptive fields enable informative sensory encodings provisionally accepted publication PLOS Computational Biology manuscript formally accepted need complete formatting changes receive follow email member team touch set requests Please note manuscript scheduled publication made required changes swift response appreciated IMPORTANT editorial review process complete PLOS permit corrections spelling formatting significant scientific errors point onwards Requests major changes affect scientific understanding work cause delays publication date manuscript institutions press office journal office choose press release paper automatically opted early publication ask notify us institution planning press release article press must coordinated PLOS Thank supporting Open Access publishing looking forward publishing work PLOS Computational Biology Best regards Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer thank authors response appreciate effort put revision Although argued many asked beyond scope paper disagree agree necessary address concerns Clarification new additions paper including Appendix adequate dont comments suggestions recommend acceptance reading paper rebuttal think negative recommending rejection first review apologize think fair assessment would major revision Reviewer authors done excellent job responding earlier comments pleased recommend manuscript accepted publication especially appreciated new appendix section greatly strengthens mathematical foundations work Two minor comments follow things noticed reading manuscript might help improve paper final publication bar plot part Fig illustrating results three models described line perhaps also including sketch receptive fields two null models look like would helpful way visually summarize result similar visualization could helpful Fig Perhaps missed Figures exactly percentages reported panel calculated Comparing orange curves green curves doesnt appear numbers correspond curves appear next straightforward way could discern authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer PLoS Comput Biol Oct e Acceptance letter Oct e Published online Oct doi journalpcbir Acceptance letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Oct PCOMPBIOLDR Structured random receptive fields enable informative sensory encodings Dear Dr Harris pleased inform manuscript formally accepted publication PLOS Computational Biology manuscript production department notified publication date due course corresponding author soon receiving typeset proof review ensure errors introduced production Please review PDF proof manuscript carefully last chance correct errors Please note major changes affect scientific understanding work likely cause delays publication date manuscript Soon final files uploaded unless opted early version manuscript published online date early version articles publication date final article published URL versions paper accessible readers Thank supporting PLOS Computational Biology openaccess publishing looking forward publishing work kind regards Anita Estes PLOS Computational Biology Carlyle House Carlyle Road Cambridge CB DN United Kingdom grosolploibpmocsolp Phone ploscompbiolorg PLOSCompBiol Articles PLOS Computational Biology provided courtesy PLOS Journal List PLoS Comput Biol v Oct PMC Journal List PLoS Comput Biol v Oct PMC Journal List Journal List PLoS Comput Biol PLoS Comput Biol v Oct v Oct PMC library NLM provides access scientific literature Inclusion NLM database imply endorsement agreement contents NLM National Institutes Health Learn disclaimer Learn disclaimer PLoS Comput Biol Oct e Published online Oct doi journalpcbi PMCID PMC PMID Structured random receptive fields enable informative sensory encodings Biraj Pandey Formal analysis Investigation Software Validation Visualization Writing original draft Writing review editing Marius Pachitariu Data curation Writing review editing Bingni W Brunton Formal analysis Resources Supervision Visualization Writing review editing Kameron Decker Harris Conceptualization Formal analysis Investigation Supervision Visualization Writing original draft Writing review editing Biraj Pandey Department Applied Mathematics University Washington Seattle Washington United States America Find articles Biraj Pandey Marius Pachitariu Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Find articles Marius Pachitariu Bingni W Brunton Department Biology University Washington Seattle Washington United States America Find articles Bingni W Brunton Kameron Decker Harris Department Computer Science Western Washington University Bellingham Washington United States America Find articles Kameron Decker Harris Xuexin Wei Editor Author information Article notes Copyright License information Disclaimer Department Applied Mathematics University Washington Seattle Washington United States America Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Department Biology University Washington Seattle Washington United States America Department Computer Science Western Washington University Bellingham Washington United States America University Texas Austin UNITED STATES Corresponding author authors declared competing interests exist Email udeuwwsirrahnoremak Received Oct Accepted Aug Copyright Pandey et al open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Associated Data Supplementary Materials Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF pcbispdf GUID EEABECFA Attachment Submitted filename coverletterresubmissionpdf pcbispdf GUID FDDFCFDCBEDFB Data Availability Statement relevant data within manuscript Supporting information files Abstract Brains must represent outside world animals survive thrive early sensory systems neural populations diverse receptive fields structured detect important features inputs yet significant variability ignored classical models sensory neurons model neuronal receptive fields random variable samples parameterized distributions demonstrate model two sensory modalities using data insect mechanosensors mammalian primary visual cortex approach leads significant theoretical connection foundational concepts receptive fields random features leading theory understanding artificial neural networks modeled neurons perform randomized wavelet transform inputs removes high frequency noise boosts signal random feature neurons enable learning fewer training samples smaller networks artificial tasks structured random model receptive fields provides unifying mathematically tractable framework understand sensory encodings across spatial temporal domains Author summary Evolution ensured animal brains dedicated extracting useful information raw sensory stimuli discarding everything else Models sensory neurons key part theories brain represents world work model tuning properties sensory neurons way incorporates randomness builds bridge leading mathematical theory understanding artificial neural networks learn models capture important properties large populations real neurons presented varying stimuli Moreover give precise mathematical formula sensory neurons two distinct areas one involving gyroscopic organ insects visual processing center mammals transform inputs also find artificial models imbued properties real neurons learn efficiently shorter training time fewer examples mathematical theory explains findings work expands understanding sensory representation large networks benefits neuroscience machine learning communities Introduction long argued brain uses large population neurons represent world view sensory stimuli encoded responses population used downstream areas diverse tasks including learning decisionmaking movement control sensory areas different neurons responding differing stimuli also providing measure redundancy However still lack clear understanding response properties wellsuited different sensory modalities One way approach sensory encoding understanding neuron would respond arbitrary stimuli Experimentally typically present many stimuli animal measure responses sensory neurons attempt estimate kind model neurons respond arbitrary stimulus common assumption neuron computes linear filter stimulus drives spiking nonlinear spikegenerating mechanism Mathematically assumption summarized number measured spikes stimulus x equal w x weight vector w nonlinearity weights w define filtering properties neuron also known receptive field model known linearnonlinear LN model also common form artificial neuron artificial neural networks ANNs LN models used extensively describe firing diverse neurons various sensory modalities vertebrates invertebrates mammalian visual system LN models used characterize retinal ganglion cells lateral geniculate neurons simple cells primary visual cortex V also used characterize auditory sensory neurons avian midbrain somatosensory neurons cortex insects used understand response properties visual interneurons mechanosensory neurons involved proprioception auditory neurons courtship behavior Given stimulus presented neural response data one estimate receptive fields population neurons Simple visual receptive fields classically understood similar wavelets particular spatial frequency angular selectivity mechanosensory areas receptive fields selective temporal frequency short time window Commonly parametric modeling Gabor wavelets smoothing regularization etc used produce clean receptive fields Yet data alone show noisy receptive fields perhaps best modeled using random distribution show modeling receptive fields random samples produces realistic receptive fields reflect structure noisiness seen experimental data importantly perspective creates significant theoretical connections foundational ideas neuroscience artificial intelligence connection helps us understand receptive fields structures structure relates kinds stimuli relevant animal Modeling filtering properties population LN neurons samples random distribution leads study networks random weights machine learning ML networks known random feature networks RFNs study RFNs rapidly gained popularity recent years large part offers theoretically tractable way study learning properties ANNs weights tuned using data RFN contains many neurons approximate functions live wellunderstood function space function space called reproducing kernel Hilbert space RKHS depends network details particular weight ie receptive field distribution Learning framed approximating functions space limited data Several recent works highlight RFN theorys usefulness understanding learning neural systems Bordelon Canatar Pehlevan series papers shown neural codes allow learning examples spectral properties secondorder statistics aligns spectral properties task applied V found neural code aligned tasks depend low spatial frequency components Harris constructed RFN model sparse networks found associative centers like cerebellum insect mushroom body showed areas may behave like additive kernels architecture also considered Hashemi et al classes kernels beneficial learning high dimensions learn fewer examples remain resilient input noise adversarial perturbation Xie et al investigated relationship fraction active neurons model cerebellumcontrolled neuron thresholdsand generalization performance learning movement trajectories vast majority network studies random weights weights w drawn Gaussian distribution independent entries sampling equivalent fully unstructured receptive field looks like white noise Closely related work previous study ANNs showed directly learning structured receptive fields could improve image classification deep networks receptive fields parametrized sum Gaussian derivatives fourth order led better performance rival architectures low data regimes paper study effect structured yet random receptive fields lead informative sensory encodings Specifically consider receptive fields generated Gaussian process GP thought drawing weights w Gaussian distribution particular covariance matrix show networks random weights project input new basis filter particular components theory introduces realistic structure receptive fields random feature models crucial current understanding artificial networks Next show receptive field datasets two disparate sensory systems mechanosensory neurons insect wings V cortical neurons mice monkeys wellmodeled GPs covariance functions wavelet eigenbases Given success modeling data GP apply weight distributions RFNs used synthetic learning tasks find structured weights improve learning reducing number training examples size network needed learn task Thus structured random weights offer realistic generative model receptive fields multiple sensory areas understand performing random change basis change basis enables network represent important properties stimulus demonstrate useful learning Results construct generative model receptive fields sensory neurons use weights ANN refer network structured random feature network first review basics random feature networks details rationale behind generative model process generate hidden weights main theory result networks weights transform inputs new basis filter particular components thus bridging sensory neuroscience theory neural networks Next show neurons two receptive field datasetsinsect mechanosensory neurons mammalian V cortical neuronsare welldescribed generative model close resemblance secondorder statistics sampled receptive fields principal components data model Finally show performance structured random feature networks several synthetic learning tasks hidden weights generative model allows network learn fewer training examples smaller network sizes Theoretical analysis consider receptive fields generated GPs order connect foundational concept sensory neuroscience theory random features artificial neural networks GPs thought samples Gaussian distribution particular covariance matrix initialize hidden weights RFNs using GPs show using GP causes network project input new basis filter particular components basis determined covariance matrix Gaussian useful removing irrelevant noisy components input use results study space functions RFNs containing many neurons learn connecting construction theory kernel methods Random feature networks start introducing main learning algorithm neuronal model work RFN Consider twolayer feedforward ANN Traditionally weights initialized randomly learned backpropagation minimizing loss objective sharp contrast RFNs hidden layer weights sampled randomly distribution fixed hidden unit computes random feature input output layer weights trained Fig Note weights randomly drawn neurons response deterministic function input given weights Open separate window Fig Random feature networks structured weights study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distribution Mathematically hidden layer activations output given h W x h x R stimulus h h h h R hidden neuron responses R predicted output use rectified linear ReLU nonlinearity x max x applied entrywise Eq hidden layer weights W w w w R drawn randomly fixed readout weights trained RFNs RFN experiments train readout weights R offset R using support vector machine SVM classifier squared hinge loss penalty regularization strength tuned range fold crossvalidation RFNs include threshold hidden neurons although could help certain contexts vast majority studies RFNs neurons weights w R initialized iid spherical Gaussian distribution w N call networks built way classical unstructured RFNs Fig propose variation hidden weights initialized w N C C R positive semidefinite covariance matrix call networks structured RFNs Fig mean weights random specified covariance compare unstructured structured weights equal footing normalize covariance matrices Tr C Tr ensures mean square amplitude weights E w Receptive fields modeled linear weights Sensory neurons respond preferentially specific features inputs stimulus selectivity often summarized neurons receptive field describes features sensory space elicits responses stimulated Mathematically receptive fields modeled linear filter stimulus space Linear filters also integral component widely used LN model sensory processing According model firing rate neuron nonlinear function applied projection stimulus onto lowdimensional subspace linear filter linear filter model receptive fields explain responses individual neurons diverse stimuli used describe disparate sensory systems like visual auditory somatosensory systems diverse species including birds mammals insects stimuli uncorrelated filters estimated computing spike triggered average STA average stimulus elicited spike neuron stimuli correlated STA filter whitened inverse stimulus covariance matrix Often STAs denoised fitting parametric function STA Gabor wavelets simple cells V model receptive field neuron weight vector w nonlinear function Instead fitting parametric function construct covariance functions realization resulting Gaussian process resembles biological receptive field Fig Structured weights project filter input covariance eigenbasis generate network weights Gaussian processes GP whose covariance functions inspired receptive fields sensory neurons brain definition GP stochastic process finite observations follow Gaussian distribution find networks weights project inputs new basis filter irrelevant components see adds inductive bias classical RFNs tasks naturalistic inputs improves learning view weight vector w finitedimensional discretization continuous function w sample GP continuous function domain compact subset R assume discretized using grid equally spaced points w w Let input realvalued function x domain could represent finite timeseries image luminance retina complicated spatiotemporal sets like movie continuous setting dimensional inner product w x w x gets replaced L inner product w x w x Every GP fully specified mean covariance function C always assume mean zero study different covariance functions KosambiKarhunenLove theorem realization zeromean GP random series representation w z terms standard Gaussian random variables z N functions weights pairs eigenvalue eigenfunction pairs covariance operator C L L C f C f continuous analog covariance matrix C C positive definite opposed semidefinite eigenfunctions form complete basis L Using Eq inner product stimulus neurons weights w x z x z x z x x x Eq shows structured weights compute projection input x onto eigenfunction x reweight filter eigenvalue taking inner product random Gaussian weights z illuminating see continuous equations look like dimensional discrete setting Samples finitedimensional GP used hidden weights RFNs w N C First GP series representation Eq becomes w z matrices eigenvalues eigenvectors z N Gaussian random vector definition covariance matrix C E w w equal steps linear algebra Finally Eq analogous w x z x Since orthogonal matrix x equivalent change basis diagonal matrix shrinks expands certain directions perform filtering summarized following theorem Theorem Basis change formula Assume w N C C eigenvalue decomposition x R define x x w x z x z N Theorem says projecting input onto structured weight vector first filtering input GP eigenbasis random projection onto spherical random Gaussian form GP eigenbasis determined choice covariance function covariance function compatible input structure hidden weights filter irrelevant features noise stimuli amplifying descriptive features inductive bias facilitates inference stimuli downstream predictor spherical Gaussian distribution canonical choice unstructured RFNs simple way evaluate effective kernel structured RFNs k struct x x k unstruct x x see Appendix expression structured kernel provides concrete connection kernel theory learning using nonlinear neural networks readers interested kernel theories full example simulation results work given Appendix show exponential reduction number samples needed learn frequency detection using structured versus unstructured basis Fig Appendix Examples random yet structured receptive fields goal model weights artificial neurons way inspired biological neurons receptive fields Structured RFNs sample hidden weights GPs structured covariance construct covariance functions make generated weights resemble neuronal receptive fields start toy example stationary GP wellunderstood Fourier eigenbasis show receptive fields generated GP selective frequencies timeseries signals construct locally stationary covariance models insect mechanosensory V neuron receptive fields models shown good match experimental data Warmup Frequency selectivity stationary covariance illustrate results theoretical analysis start toy example temporal receptive fields selective particular frequencies example may familiar readers comfortable Fourier series basic signal processing Let input finite continuous timeseries x interval L use covariance function C k k cos k stationary process k k L k th natural frequency k weight coefficients covariance function Eq stationary means depends difference timepoints Applying compound angle formula get C k k cos k cos k sin k sin k Since sinusoidal functions cos k sin k form orthonormal basis L Eq eigendecomposition covariance eigenfunctions sines cosines eigenvalues k Eq know structured weights covariance form random series w k k z k cos k z k sin k z k z k N Thus receptive fields made sinusoids weighted k Gaussian variables z k z k Suppose want receptive fields retain specific frequency information signal filter rest Take k k k f lo k f hi call bandlimited spectrum passband f lo f hi bandwidth f hi f lo bandwidth increases receptive fields become less smooth since made wider range frequencies k nonzero decay certain rate rate controls smoothness resulting GP receptive fields act input signals x implicitly transform inputs Fourier basis filter frequencies based magnitude k bandlimited setting frequencies outside passband filtered makes receptive fields selective particular range frequencies ignore others hand classical random features weight frequencies equally even though natural settings high frequency signals corrupted noise Insect mechanosensors next consider particular biological sensor sensitive timehistory forces Campaniform sensilla CS domeshaped mechanoreceptors detect local stress strain insect exoskeleton embedded cuticle deformation cuticle bending torsion induces depolarizing currents CS opening mechanosensitive ion channels CS encode proprioceptive information useful body state estimation movement control diverse tasks like walking kicking jumping flying model receptive fields CS believed critical flight control namely ones found base halteres wings Fig Halteres wings flap rhythmically flight rotations insects body induce torsional forces felt active sensory structures CS detect small strain forces thereby encoding angular velocity insect body Experimental results show haltere wing CS selective broad range oscillatory frequencies STAs smooth oscillatory selective frequency decay time Fig B Open separate window Fig Random receptive field model insect mechanosensors Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shown model temporal receptive fields locally stationary GP bandlimited spectrum Examples receptive fields generated GP shown Fig C inputs CS modeled finite continuous timeseries x finite interval L neuron weights generated covariance function C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum k k L k th natural frequency warmup frequency selectivity weights accounted parameters f lo f hi bandwidth f hi f lo increases receptive fields built wider selection frequencies makes receptive fields less smooth Fig field localized near decay determined parameter increases receptive field selective larger time windows eigenbasis covariance function Eq similar Fourier eigenbasis modulated decaying exponential eigenbasis orthonormal basis span k e cos k k e sin k nonorthogonal set functions L hidden weights transform timeseries inputs eigenbasis discard components outside passband frequencies f lo f hi fit covariance model receptive field data CS neurons wings hawkmoth Manduca sexta data Briefly CS receptive fields estimated spiketriggered average STA experimental mechanical stimuli wings stimuli generated bandpassed white noise Hz characterize receptive fields population CS neurons compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field number samples case kHz ms samples normalization sets overall scale covariance matrix data covariance matrix shows tridiagonal structure Fig main diagonal positive diagonals negative diagonals decay away top left matrix Open separate window Fig Spectral properties mechanosensory RFs model similar compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behavior fit covariance model data optimize parameters f lo f hi finding f lo Hz f hi Hz ms best fit sensilla data minimizing Frobenius norm difference C data model see Appendix resulting model covariance matrix Fig B matches data covariance matrix Fig remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig B Appendix simulate effect finite number neurons generate weight vectors equal number neurons recorded recompute model covariance matrix Fig C call finite neuron model covariance matrix C finite shows bump bloblike structures evident C data C model result suggests bumpy structures attributed small number recorded neurons hypothesize effects would disappear larger dataset C data would closely resemble C model comparison also calculate Frobenius difference null models unstructured covariance model Fourier model unstructured model Frobenius norm difference Fourier model sensilla covariance model much lower difference compared null models fitting data accurately show covariance matrices sampled receptive fields null models Fig C E Appendix Comparing eigenvectors eigenvalues data model covariance matrices find spectral properties C model C finite similar C data eigenvalue curves models match data quite well Fig E curves directly comparable covariance normalized trace makes sum eigenvalues unity data model covariance matrices lowdimensional first data eigenvectors explain variance top explain top eigenvectors model finite sample match data quite well Fig Primary visual cortex turn visually driven neurons mammalian primary cortex Primary visual cortex V earliest cortical area processing visual information Fig neurons V detect small changes visual features like orientations spatial frequencies contrast size Open separate window Fig Random receptive field model Primary Visual Cortex V Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fields model receptive fields simple cells V clear excitatory inhibitory regions light shone excitatory regions increase cells response viceversa Fig B shape regions determines orientation selectivity widths determine frequency selectivity receptive fields centered location visual field decay away integrate visual stimuli within small region center Gabor functions widely used mathematical model receptive fields simple cells model receptive fields using another locally stationary GP show examples generated receptive fields Fig C Consider inputs cortical cells continuous twodimensional image x domain L L x R Since image realvalued x grayscale contrast single color channel pixel values neuron weights generated covariance function following form C exp f smooth receptive fields exp c c localized center c receptive field center defined c size receptive field determined parameter increases receptive field extends farther center c Fig Spatial frequency selectivity accounted bandwidth parameter f f decreases spatial frequency receptive field goes making weights less smooth Fig E eigendecomposition covariance function Eq leads orthonormal basis single scale Hermite wavelets c wavelet eigenfunctions Hermite polynomials modulated decaying Gaussian k e c H k c k c k H k k th physicists Hermite polynomial eigenfunctions nonzero centers c shifted versions Eq detailed derivation values constants c c c normalization Appendix use Eq model receptive field data V neurons recorded calcium imaging transgenic mice expressing GCaMPs mice headfixed running airfloating ball presented unique white noise images pixels using Psychtoolbox pixels white black equal probability Images upsampled resolution screens via bilinear interpolation stimulus corrected eyemovements online using custom code responses cells collected using twophoton mesoscope preprocessed using Suitep Receptive fields calculated white noise images deconvolved calcium responses cells using STA covariance analysis picked cells signaltonoise SNR threshold gave us cells SNR calculated smaller set images presented twice using method preprocessing step moved center mass every receptive field center visual field compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field case pixels pixels data covariance matrix resembles tridiagonal matrix However diagonals nonzero equally spaced segments Additionally values decay away center matrix show C data zoomed nonzero region around center matrix Fig corresponds pixel region around center full pixel matrix full covariance matrix shown Fig F Appendix Open separate window Fig Spectral properties V RFs model similar compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons red covariance model number offdiagonals center rate decay away center determined parameters f c respectively covariance pixels decays function distance c leads equallyspaced nonzero segments hand covariance also decays function distance pixels brings diagonal structure model frequency parameter f increases number offdiagonals increases Pixels generated weights become correlated weights become spatially smoother size parameter increases diagonals decay slower center c increasing correlations center pixel leading significant weights occupy visual field optimize parameters fit data finding f pixels minimizing Frobenius norm difference C data model need optimize center parameter c since preprocess data receptive fields centered c center grid resulting model covariance matrix Fig B data covariance matrix Fig match remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig G Appendix simulate effect finite number neurons generate weights equal number neurons data compute C finite shown Fig C finite matrix C finite looks even like C data shows negative covariances far center result finite sample size comparison also calculate normalized Frobenius difference null models unstructured covariance model translation invariant V model translation invariant model remove spatially localizing exponential Eq fit spatial frequency parameter f unstructured model Frobenius norm difference translation invariant model V covariance model much lower difference better fit data show covariance matrices sampled receptive fields null models Fig H J Appendix Similar spectral properties evident eigenvectors eigenvalues C model C finite C data analytical forms derived Eq Fig E covariances normalized unit trace Note analytical eigenfunctions shown finer grid model data analysis performed continuous space differences eigenfunctions eigenvalues analytical model results due discretization Examining eigenvectors Fig also see good match although rotations differences ordering eigenvectors explain variance receptive field data reference top eigenvectors explain variance data variance model eigenvalue curves models analytical forms match data Fig E reasonably well although well mechanosensors Appendix repeat analysis receptive fields measured different stimulus sets mouse different experimental dataset nonhuman primate V findings consistent results shown Fig K P Appendix Advantages structured random weights artificial learning tasks hypothesis neuronal inductive bias structured receptive fields allows networks learn fewer neurons training examples steps gradient descent classification tasks naturalistic inputs examine hypothesis compare performance structured receptive fields classical ones several classification tasks find artificial learning tasks structured random networks learn accurately smaller network sizes fewer training examples gradient steps Frequency detection CS naturally encode timehistory strain forces acting insect body sensors inspired temporal filtering properties shown accurately classify spatiotemporal data Inspired result test sensillainspired mechanosensory receptive fields timeseries classification task Fig top example presented network ms timeseries sampled kHz goal detect whether example contains sinusoidal signal positive examples sinusoidal signals f Hz corrupted noise SNR dB negative examples Gaussian white noise matched amplitude positive examples Note frequency detection task linearly separable random phases positive negative examples See Appendix additional details including definition SNR crossvalidation used find optimal parameters f lo Hz f hi Hz ms Open separate window Fig Random mechanosensory weights enable learning fewer neurons timeseries classification tasks show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible task number hidden neurons structured RFN significantly outperforms classical RFN show test performance using tuned parameters Fig Even noisy task achieves test error using hidden neurons Meanwhile classical network takes neurons achieve similar error Predictably performance suffers weights incompatible task show results f lo Hz f hi Hz Fig incompatible RFN performs better chance error much worse classical RFN takes neurons achieve test error test error decrease level even additional hidden neurons Frequency XOR task challenge mechanosensorinspired networks difficult task build frequency ExclusiveOR XOR problem Fig B top XOR binary function returns true inputs different otherwise returns false XOR classical example function linearly separable thus harder learn inputs ms timeseries sampled kHz inputs either contain pure frequency f Hz f Hz mixed frequency signals f f white noise pure mixed frequency cases add noise SNR See Appendix details goal task output true input contains either pure tone false input contains mixed frequencies white noise tune GP covariance parameters f lo f hi Eq using crossvalidation cross validation procedure algorithmic details identical frequency detection task Using cross validation find optimal parameters f lo Hz f hi Hz ms incompatible weights take f lo Hz f hi Hz structured RFN significantly outperform classical RFN number hidden neurons show network performance using parameters Fig B Classification error achieved hidden neurons sharp contrast classical RFN requires hidden neurons achieve error incompatible weights network needs neurons achieve test error improve larger network sizes four input subclasses consistently fails classify pure Hz sinusoidal signals outside passband Image classification next test Vinspired receptive fields two standard digit classification tasks MNIST KMNIST MNIST KMNIST datasets contain images handwritten digits MNIST Arabic numerals whereas KMNIST Japanese hiragana phonetic characters datasets come split training test examples classes training examples per class Every example grayscale image centered characters hidden weight center c chosen uniformly random pixels ensures networks weights uniformly cover image space fact means network represent sum locallysmooth functions see Appendix use network hidden neurons tune GP covariance parameters f Eq using fold cross validation MNIST training set parameter ranges pixels optimal parameters found grid search find optimal parameters pixels f pixels refit optimal model using entire training set parameters MNIST used KMNIST task without additional tuning Vinspired achieves much lower average classification error compared classical RFN number hidden neurons show learning performance using parameters MNIST task Fig achieve error MNIST task requires neurons versus neurons classical RFN structured RFN achieves error neurons Qualitatively similar results hold KMNIST task Fig B although overall errors larger reflecting harder task achieve error KMNIST requires neurons versus neurons classical RFN structured RFN achieves error neurons Open separate window Fig Random V weights enable learning fewer neurons fewer examples digit classification tasks show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properly network performance suffers GP covariance parameters match task happens size parameter smaller stroke width spatial scale f doesnt match stroke variations character Taking incompatible parameters f Fig B structured weights performs worse classical RFN tasks hidden neurons achieves relatively poor test errors MNIST Fig KMNIST Fig B Structured weights improve generalization limited data Alongside learning fewer hidden neurons V structured RFNs also learn accurately fewer examples test fewshot learning using image classification datasets training examples reduced training examples per class test set GP parameters remain Structured encodings allow learning fewer samples unstructured encodings show fewshot learning results Fig C networks performance saturate past hundred hidden neurons MNIST lowest error achieved V structured RFN versus classical RFN using incompatible weights Fig C structured network acheives error using structured features KMNIST task opposed classical RFN using incompatible weights Fig Networks train faster initialized structured weights study effect structured weights initialization strategy fullytrained neural networks weights network vary hypothesized structured initialization allows networks learn faster ie training loss test error would decrease faster unstructured weights shown performance RFNs improves biologically inspired weight sampling However RFNs Eq readout weights modified training hidden weights W frozen initial value compare biologicallymotivated initialization classical initialization variance inversely proportional number hidden neurons w unstruct N initialization widely known Kaiming normal scheme thought stabilize training dynamics controlling magnitude gradients classical approach ensures Tr fair comparison scale structured weight covariance matrix Tr C studies RFNs trace equal weight scale absorbed readout weights due homogeneity ReLU compare structured unstructured weights MNIST KMNIST tasks common benchmarks fullytrained networks architecture single hidden layer feedforward neural network Fig hidden neurons crossentropy loss training sets minimized using simple gradient descent GD epochs fair comparison learning rate optimized network separately define area training loss curve metric speed learning perform grid search range e e learning rate minimizes metric resulting parameters structured unstructured incompatible networks respectively parameters image classification MNIST KMNIST tasks Vinitialized network minimizes loss function faster classically initialized network MNIST task V network achieves loss value epochs compared network Fig see qualitatively similar results KMNIST task end training Vinspired networks loss classically initialized network reaches Fig B find Vinitialized network performs better classical initialization covariance parameters match task incompatible parameters Vinitialized network achieves loss value MNIST KMNIST Open separate window Fig V weight initialization fullytrained networks enables faster training digit classification tasks show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible task minimize training loss faster Vinitialized network also generalizes well achieves lower test error end training MNIST achieves test error compared error classically initialized network using incompatible weights Fig C KMNIST see error compared error classical initalization using incompatible weights Fig see similar results across diverse hidden layer widths learning rates Fig Q Appendix benefits evident wider networks smaller learning rates Furthermore structured weights show similar results trained epochs rate neurons shown optimizers like minibatch Stochastic Gradient Descent SGD ADAM batch size rate neurons shown Structured initialization facilitates learning across wide range networks However improvement universal significant benefit found initializing early convolutional layers deep network AlexNet applying ImageNet dataset shown Appendix Fig U Appendix large amounts training data fact small fraction network initialized structured weights could explain null result Also many scenarios incompatible structured weights get performance par compatible ones end training poor inductive bias overcome Improving representation structured random weights shown structured receptive field weights improve performance RFNs fullytrained networks number supervised learning tasks long receptive fields compatible task performance gains unstructured features possible incompatible networks performs better even worse using classical unstructured weights results understood theoretical framework Structured weights effectively cause input x undergo linear transformation new representation x following Theorem examples new representation bandlimited due design covariance function V weights eigenvalues nonzero spectrum decays exponentially acts lowpass filter moving bandlimited representation filter noisehighfrequency componentsand reduce dimensionalitycoordinates x outside passband zero general noise dimensionality make learning harder easiest understand effects frequency detection task simplicity assume using stationary features warmup frequency detection task signal power contained f Hz frequency everything else due noise weights compatible task means w sum sines cosines frequencies k passband includes f narrower make bandwidth still retaining signal higher SNR x becomes since noise filtered see Appendix Discussion paper describe random generative model receptive fields sensory neurons Specifically model receptive field random filter sampled Gaussian process GP covariance structure matched statistics experimental neural data show two kinds sensory neuronsinsect mechanosensory simple cells mammalian Vhave receptive fields welldescribed GPs particular generated receptive fields secondorder statistics principal components match receptive field data Theoretically show individual neurons perform randomized transformation filtering inputs connection provides framework sensory neurons compute input transformations like Fourier wavelet transforms biologically plausible way numerical results using structured random receptive fields show offer better learning performance unstructured receptive fields several benchmarks structured networks achieve higher test performance fewer neurons fewer training examples unless frequency content receptive fields incompatible task networks fully trained initializing structured weights leads better network performance measured training loss generalization fewer iterations gradient descent Structured random features may understood theoretically transforming inputs informative basis retains important information stimulus filtering away irrelevant signals Modeling sensory neurons modalities random feature formulation natural extension traditional linearnonlinear LN neuron model approach may applied brain regions LN models successful instance sensory areas primarily feedforward connectivity like somatosensory auditory regions neurons auditory somatosensory systems selective spatial temporal structures stimuli spatial structure emerges networks trained artificial tactile tasks receptive fields could modeled GPs spatiotemporal covariance functions could useful artificial tasks spatiotemporal stimuli movies multivariate timeseries Neurons localized random temporal responses found compatible manifold coding decisionmaking task GPs complementary approach traditional sparse coding efficient coding hypotheses connections theories interesting future research Receptive fields development generative model offers new directions explore biological basis computational principles behind receptive fields Development lays basic architecture conserved animal animal yet details every neural connection specified leading amount inevitable randomness least initially receptive fields random constrained covariance natural ask biology implements Unsupervised Hebbian dynamics local inhibition allow networks learn principal components input interesting future direction similar learning rules may give rise overcomplete nonorthogonal structure similar studied may prove biologically plausible weights result taskdriven optimization assumes receptive field properties actually lie within synaptic weights spatial receptive fields assumption plausible temporal properties receptive fields likely result neurons intrinsic dynamics LN framework model Heterogeneous physiological eg resonator dynamics mechanical position shape mechanosensor relative body structure properties combine give diverse temporal receptive field structures Development thus leverages different mechanisms build structure receptive field properties sensory neurons Connections compressive sensing Random projections seen extensive use field compressive sensing highdimensional signal found measurements long sparse representation Random compression matrices known optimal properties however many cases structured randomness realistic Recent work shown structured random projections local wiring constraints one dimension compatible dictionary learning supporting previous empirical results work shows structured random receptive fields equivalent employing wavelet dictionary dense Gaussian projection Machine learning inductive bias important open question neuroscience machine learning certain networks characterized features architecture weights nonlinearities better others certain problems One perspective network good problem biased towards approximating functions close target known inductive bias depends alignment features encoded neurons task hand approach shows structured receptive fields equivalent linear transformation input build biases Furthermore describe nonlinear properties network using kernel varies depending receptive field structure target function small norm RKHS inductive bias easier learn small norm RKHS means target function varies smoothly inputs Smooth functions easier learn compared fast varying ones way receptive field structure influences ease learning target function conjecture receptive fields neuralinspired distributions affect RKHS geometry target functions norm small RKHS compared RKHS random whitenoise receptive fields leave future work verify conjecture detail Networks endowed principles neural computation like batch normalization pooling inputs residual connections found contain inductive biases certain learning problems Learning datadependent kernels another way add inductive bias also saw initializing fullytrained networks generative models improved speed convergence generalization compared unstructured initialization result consistent known results initialization effect generalization initialization literature mostly focused avoiding explodingvanishing gradients conjecture inductive bias structured connectivity places network closer good solution loss landscape random Vinspired receptive fields model seen similar happens convolutional neural network CNN similarities differences compared brains recent study showed CNNs fixed Vlike convolutional layer robust adversarial perturbations inputs similar vein work using randomly sampled Gabor receptive fields first layer deep network also shown improve performance wavelet scattering transform multilayer network wavelet coefficients passed nonlinearities model similar deep CNNs framework differs randomized model yields wavelets single scale similar studies robustness learning deep networks weights possible Adding layers model sampling weights variety spatial frequencies field sizes would yield random networks behave similar scattering transform offering another connection brain CNNs Directly learning filters Hermite wavelet basis led good perfomance ANNs little data idea extended multiple scales structured random features seen RFN version ideas supporting evidence principles used biology Limitations future directions several limitations random feature approach model neuron responses scalar firing rates instead discrete spikes ignore complex neuronal dynamics neuromodulatory context many details Like LN models random feature model assumes zero plasticity hidden layer neurons However associative learning drive changes receptive fields individual neurons sensory areas like V auditory cortex RFN purely feedforward account feedback connections Recent work suggests feedforward architecture lacks sufficient computational power serve detailed inputoutput model network cortical neurons might need additional layers convolutional filters difficult interpret parameters found fitting receptive field data connect experimental conditions Also GP model weights captures covariance second moments neglects higherorder statistics remains shown theory yield concrete predictions tested vivo experimental conditions random feature receptive field model randomized extension LN neuron model LN model fits parameterized function receptive field contrast random feature framework fits distribution entire population receptive fields generates realistic receptive fields distribution natural question compare goal capture individual differences neuronal receptive fields one resort LN model neurons receptive field fit data random feature model flexible provides direct connection random feature theory mathematically tractable generative connection kernel learning opens door using techniques mainstay machine learning theory literature instance estimate generalization error sample complexity context learning biologically realistic networks see several future directions structured random features connecting computational neuroscience machine learning already stated auditory somatosensory tactile regions good candidates study well developmental principles could give rise random yet structured receptive field properties account plasticity hidden layer one could also analyze neural tangent kernel NTK associated structured features kernels often used analyze ANNs trained gradient descent number hidden neurons large step size small incorporate lateral feedback connections weights could sampled GPs recurrent covariance functions theory may also help explain CNNs fixed Vlike convolutional layer robust adversarial input perturbations filtering high frequency corruptions seems likely structured random features also robust would interesting study intermediate layer weights fullytrained networks approximate samples GP studying covariance structure Finally one could try develop covariance functions optimize RFNs sophisticated learning tasks see near high performancelower error faster training etcon difficult tasks possible Methods methods described throughout Results section details additional results Appendix Supporting information Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF Click additional data file pdf Acknowledgments thank Dario Ringach providing macaque V data Brandon Pratt hawkmoth mechanosensor data grateful Ali Weber Steven Peterson Owen Levin Alice C Schwarze useful discussions thank Sarah Lindo Michalis Michaelos Carsen Stringer help mouse surgeries calcium imaging data processing respectively Funding Statement BP supported UW Applied Math Frederic Wan Endowed Fellowship Terry Keegan Memorial ARCS Endowment Fellowship Natural Science Foundation Graduate Research Fellowship Program Grant DGE MP supported Janelia Research Campus Howard Hughes Medical Institute BWB supported grants FA FA Air Force Office Scientific Research KDH supported Washington Research Foundation postdoctoral fellowship Western Washington University funders role study design data collection analysis decision publish preparation manuscript httpsamathwashingtonedusupportus httpswwwarcsfoundationorgnationalhomepage httpswwwnsfgrfporg httpswwwjaneliaorglabpachitariulab httpswwwafrlafmilAFOSR httpswwwwrfseattleorggrantswrfpostdoctoralfellowships httpscswwueduharri Data Availability relevant data within manuscript Supporting information files References Yuste R neuron doctrine neural networks Nature Reviews Neuroscience doi nrn PubMed CrossRef Google Scholar Fusi Miller EK Rigotti neurons mix high dimensionality higher cognition Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Saxena Cunningham JP Towards neural population doctrine Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Stringer C Pachitariu Steinmetz N Carandini Harris KD Highdimensional geometry population responses visual cortex Nature doi PMC free article PubMed CrossRef Google Scholar Sherrington C Integrative Action Nervous System Cambridge University Press Google Scholar Chichilnisky EJ simple white noise analysis neuronal light responses Network Computation Neural Systems PubMed Google Scholar Sakai HM Naka K Signal transmission catfish retina V Sensitivity circuit Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Clay Reid R Alonso JM Specificity monosynaptic connections thalamus visual cortex Nature doi PubMed CrossRef Google Scholar Jones JP Palmer LA evaluation twodimensional Gabor filter model simple receptive fields cat striate cortex Journal Neurophysiology PubMed Google Scholar Knudsen EI Konishi Centersurround organization auditory receptive fields owl Science doi science PubMed CrossRef Google Scholar H Central mechanisms tactile shape perception Current opinion neurobiology PubMed Google Scholar Rusanen J Frolov R Weckstrm Kinoshita Arikawa K Nonlinear amplification graded voltage signals firstorder visual interneurons butterfly Papilio xuthus Journal Experimental Biology PubMed Google Scholar Fox JL Fairhall AL Daniel TL Encoding properties haltere neurons enable motion feature detection biological gyroscope Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Pratt B Deora Mohren Daniel Neural evidence supports dual sensorymotor role insect wings Proceedings Royal Society B Biological Sciences doi rspb PMC free article PubMed CrossRef Google Scholar Clemens J Ronacher B Feature Extraction Integration Underlying Perceptual Decision Making Courtship Behavior Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Park Pillow JW Receptive field inference localized priors PLoS computational biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Bonin V Histed MH Yurgenson Reid RC Local Diversity FineScale Organization Receptive Fields Mouse Visual Cortex Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Rosenblatt F perceptron probabilistic model information storage organization brain Psychological Review doi h PubMed CrossRef Google Scholar Caron SJC Ruta V Abbott LF Axel R Random convergence olfactory inputs Drosophila mushroom body Nature doi nature PMC free article PubMed CrossRef Google Scholar LitwinKumar Harris KD Axel R Sompolinsky H Abbott LF Optimal Degrees Synaptic Connectivity Neuron e doi jneuron PMC free article PubMed CrossRef Google Scholar Broomhead DS Lowe Radial basis functions multivariable functional interpolation adaptive networks Royal Signals Radar Establishment Malvern United Kingdom Google Scholar Igelnik B Pao YH Stochastic choice basis functions adaptive function approximation functionallink net IEEE transactions neural networks doi PubMed CrossRef Google Scholar Rahimi Recht B Random Features LargeScale Kernel Machines Platt JC Koller Singer Roweis ST editors Advances Neural Information Processing Systems Curran Associates Inc p Google Scholar Liu F Huang X Chen Suykens JAK Random Features Kernel Approximation Survey Algorithms Theory Beyond arXiv cs stat PubMed Arora Du SS Hu W Li Z Wang R FineGrained Analysis Optimization Generalization Overparameterized TwoLayer Neural Networks arXiv cs stat Arora Du SS Hu W Li Z Salakhutdinov R Wang R Exact Computation Infinitely Wide Neural Net arXiv cs stat Chen L Xu Deep Neural Tangent Kernel Laplace Kernel RKHS arXiv cs math stat Neal RM Priors Infinite Networks New York NY Springer New York p Available CrossRef Google Scholar Williams CKI Computation Infinite Neural Networks Neural Computation doi CrossRef Google Scholar Rahimi Recht B Uniform approximation functions random bases th Annual Allerton Conference Communication Control Computing IEEE p Available httpieeexploreieeeorgdocument Bordelon B Canatar Pehlevan C Spectrum Dependent Learning Curves Kernel Regression Wide Neural Networks arXiv cs stat Bordelon B Pehlevan C Population Codes Enable Learning Examples Shaping Inductive Bias bioRxiv Google Scholar Canatar Bordelon B Pehlevan C Spectral Bias TaskModel Alignment Explain Generalization Kernel Regression Infinitely Wide Neural Networks arXiv condmat stat PMC free article PubMed Harris KD Additive function approximation brain arXiv cs qbio stat Hashemi Schaeffer H Shi R Topcu U Tran G Ward R Generalization Bounds Sparse Random Feature Expansions arXiv cs math stat Xie Muscinelli Decker Harris K LitwinKumar Taskdependent optimal representations cerebellar learning bioRxiv Google Scholar Jacobsen JH van Gemert J Lou Z Smeulders AWM Structured Receptive Fields CNNs arXiv cs Paninski L Convergence properties spiketriggered analysis techniques Network Computation Neural Systems p PubMed Google Scholar Rasmussen CE Williams CKI Gaussian Processes Machine Learning Adaptive Computation Machine Learning MIT Press Google Scholar Kosambi DD Statistics function space Journal Indian Mathematical Society New Series Google Scholar Wahba G Spline Models Observational Data SIAM Dickerson BH Fox JL Sponberg Functional diversity generic encoding insect campaniform sensilla Current Opinion Physiology doi jcophys CrossRef Google Scholar Yarger Fox JL Dipteran Halteres Perspectives Function Integration Unique Sensory Organ Integrative Comparative Biology doi icbicw PubMed CrossRef Google Scholar Fox JL Daniel TL neural basis gyroscopic force measurement halteres Holorusia Journal Comparative Physiology doi sz PubMed CrossRef Google Scholar Genton MG Classes Kernels Machine Learning Statistics Perspective Journal Machine Learning Research Dec Google Scholar Hubel DH Wiesel TN Receptive fields single neurones cats striate cortex Journal Physiology doi jphysiolsp PMC free article PubMed CrossRef Google Scholar Marr Hildreth E Brenner Theory edge detection Proceedings Royal Society London Series B Biological Sciences PubMed Google Scholar Martens JB Hermite transformtheory IEEE Transactions Acoustics Speech Signal Processing doi CrossRef Google Scholar Kleiner Brainard Pelli Whats new Psychtoolbox PerceptionECVP Abstract Supplement European Conference Visual Perception ECVP August Arezzo Italy Sofroniew NJ Flickinger King J Svoboda K large field view twophoton mesoscope subcellular resolution vivo imaging Elife e doi eLife PMC free article PubMed CrossRef Google Scholar Pachitariu Stringer C Dipoppa Schrder Rossi LF Dalgleish H et al Suitep beyond neurons standard twophoton microscopy BioRxiv Google Scholar Mohren TL Daniel TL Brunton SL Brunton BW Neuralinspired sensors enable sparse efficient classification spatiotemporal data Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar LeCun Cortes C Burges C MNIST handwritten digit database ATT Labs Online Available httpyannlecuncomexdbmnist Google Scholar Clanuwat BoberIrizar Kitamoto Lamb Yamamoto K Ha Deep Learning Classical Japanese Literature arXiv cs stat K Zhang X Ren Sun J Delving Deep Rectifiers Surpassing HumanLevel Performance ImageNet Classification IEEE International Conference Computer Vision ICCV p Krizhevsky One weird trick parallelizing convolutional neural networks arXiv cs Russakovsky Deng J Su H Krause J Satheesh et al ImageNet Large Scale Visual Recognition Challenge International Journal Computer Vision doi sy CrossRef Google Scholar Pruszynski JA Johansson RS Edgeorientation processing firstorder tactile neurons Nature Neuroscience doi nn PubMed CrossRef Google Scholar Zhao CW Daley MJ Pruszynski JA Neural network models tactile system develop firstorder units spatially complex receptive fields PLOS ONE e doi journalpone PMC free article PubMed CrossRef Google Scholar Xu Ruan C Korpeoglu E Kumar Achan K Temporal Kernel Approach Deep Learning Continuoustime Information arXiv cs Koay SA Charles Thiberge SY Brody CD Tank DW Sequential efficient neuralpopulation coding complex task information bioRxiv PubMed Google Scholar Olshausen BA Field DJ Sparse Coding Overcomplete Basis Set Strategy Employed V Vision Research doi PubMed CrossRef Google Scholar Barlow HB et al Possible principles underlying transformation sensory messages Sensory communication Google Scholar Chalk Marre Tkaik G Toward Unified Theory Efficient Predictive Sparse Coding Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Swanson LW Brain architecture Understanding basic plan vol xv New York NY US Oxford University Press Google Scholar Strausfeld NJ Arthropod Brains Evolution Functional Elegance Historical Significance Harvard University Press Available httpswwwjstororgstablejctvdpvh Zador critique pure learning artificial neural networks learn animal brains Nature Communications doi PMC free article PubMed CrossRef Google Scholar Oja E Principal components minor components linear neural networks Neural Networks doi CrossRef Google Scholar Pehlevan C Sengupta Chklovskii DB Similarity Matching Objectives Lead HebbianAntiHebbian Networks Neural Computation doi necoa PubMed CrossRef Google Scholar Ringach DL Haphazard Wiring Simple Receptive Fields Orientation Columns Visual Cortex Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Ostojic Brunel N Spiking Neuron Models LinearNonlinear Models PLOS Computational Biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Weber AI Pillow JW Capturing Dynamical Repertoire Single Neurons Generalized Linear Models Neural Computation doi necoa PubMed CrossRef Google Scholar Fairhall receptive field dead Long live receptive field Current Opinion Neurobiology ixxii doi jconb PMC free article PubMed CrossRef Google Scholar Barth FG Mechanics preprocess information fine tuning mechanoreceptors Journal Comparative Physiology doi sz PMC free article PubMed CrossRef Google Scholar Eldar YC Kutyniok G editors Compressed Sensing Theory Applications Cambridge University Press Google Scholar Foucart Rauhut H Mathematical Introduction Compressive Sensing Birkhuser Basel Ganguli Sompolinsky H Compressed Sensing Sparsity Dimensionality Neuronal Information Processing Data Analysis Annual Review Neuroscience doi annurevneuro PubMed CrossRef Google Scholar Fallah K Willats AA Liu N Rozell CJ Learning sparse codes compressed representations biologically plausible local wiring constraints bioRxiv Google Scholar Barranca VJ Kovai G Zhou Cai Improved Compressive Sensing Natural Scenes Using Localized Random Sampling Scientific Reports doi srep PMC free article PubMed CrossRef Google Scholar ShaweTaylor J Cristianini N Kernel Methods Pattern Analysis Cambridge University Press Google Scholar ShalevShwartz BenDavid Understanding Machine Learning Theory Algorithms Cambridge University Press Google Scholar Yamins DLK Hong H Cadieu CF Solomon EA Seibert DiCarlo JJ Performanceoptimized hierarchical models predict neural responses higher visual cortex Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar K Zhang X Ren Sun J Deep Residual Learning Image Recognition arXiv cs Sinha Duchi JC Learning Kernels Random Features Lee Sugiyama Luxburg U Guyon Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileedcbbafbacbePaperpdf Google Scholar Arpit Campos V Bengio Initialize Network Robust Initialization WeightNorm ResNets Wallach H Larochelle H Beygelzimer dAlchBuc F Fox E Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileefaccdaPaperpdf Google Scholar Glorot X Bengio Understanding difficulty training deep feedforward neural networks Proceedings Thirteenth International Conference Artificial Intelligence Statistics JMLR Workshop Conference Proceedings p Available httpproceedingsmlrpressvglorotahtml Olah C Mordvintsev Schubert L Feature Visualization Distill Lindsay GW Convolutional Neural Networks Model Visual System Past Present Future Journal Cognitive Neuroscience p PubMed Google Scholar Dapello J Marques Schrimpf Geiger F Cox DiCarlo JJ Simulating Primary Visual Cortex Front CNNs Improves Robustness Image Perturbations Advances Neural Information Processing Systems Google Scholar Illing B Gerstner W Brea J Biologically plausible deep learningBut far go shallow networks Neural Networks doi jneunet PubMed CrossRef Google Scholar Mallat Group Invariant Scattering Communications Pure Applied Mathematics doi cpa CrossRef Google Scholar Bruna J Mallat Invariant Scattering Convolution Networks IEEE Transactions Pattern Analysis Machine Intelligence doi TPAMI PubMed CrossRef Google Scholar Andn J Mallat Deep Scattering Spectrum IEEE Transactions Signal Processing doi TSP CrossRef Google Scholar Pintea SL Tomen N Goes SF Loog van Gemert JC Resolution learning deep convolutional networks using scalespace theory arXiv cs PubMed Goltstein PM Meijer GT Pennartz CM Conditioning sharpens spatial representation rewarded stimuli mouse primary visual cortex eLife e doi eLife PMC free article PubMed CrossRef Google Scholar Fritz J Shamma Elhilali Klein Rapid taskrelated plasticity spectrotemporal receptive fields primary auditory cortex Nature Neuroscience doi nn PubMed CrossRef Google Scholar Beniaguev Segev London Single cortical neurons deep artificial neural networks Neuron e doi jneuron PubMed CrossRef Google Scholar Jacot Gabriel F Hongler C Neural Tangent Kernel Convergence Generalization Neural Networks Advances Neural Information Processing Systems vol Curran Associates Inc Available httpspapersnipsccpaperhashabefaebbaecbdfaAbstracthtml Google Scholar Mattos CLC Dai Z Damianou Forth J Barreto GA Lawrence ND Recurrent Gaussian Processes arXiv cs stat Ringach DL Spatial structure symmetry simplecell receptive fields macaque primary visual cortex Journal Neurophysiology Jul doi jn PubMed CrossRef Google Scholar PLoS Comput Biol Oct e Decision Letter Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Mar Dear Mr Pandey Thank much submitting manuscript Structured random receptive fields enable informative sensory encodings consideration PLOS Computational Biology papers reviewed journal manuscript reviewed members editorial board several independent reviewers light reviews email would like invite resubmission significantlyrevised version takes account reviewers comments make decision publication seen revised manuscript response reviewers comments revised manuscript also likely sent reviewers evaluation ready resubmit please upload following letter containing detailed list responses review comments description changes made manuscript Please note forming response article accepted may opportunity make peer review history publicly available record include editor decision letters reviews responses reviewer comments eligible contact opt Two versions revised manuscript one either highlights tracked changes denoting text changed clean version uploaded manuscript file Important additional instructions given reviewer comments Please prepare submit revised manuscript within days anticipate delay please let us know expected resubmission date replying email Please note revised manuscripts received day due date may require evaluation peer review similar newly submitted manuscripts Thank submission hope editorial process constructive far welcome feedback time Please dont hesitate contact us questions comments Sincerely Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer authors make two main contributions First modeling receptive fields independent draws Gaussian process Second arguing structured receptive fields beneficial efficient learning downstream authors prove theorem provides interpretation random receptive fields performing filtering eigenbasis Gaussian process inductive bias argued reason downstream predictors performance may enhanced receptive fields compatible learning problem contributions potentially important neither two ideas sufficiently motivated developed Therefore paper feels light content suggestions improvement need modeling receptive fields random draws motivated found strange introduction paper reviews wide range literature including even technical references reproducing kernel Hilbert spaces whose relevance questionable main motivation work short sentence referring data ref went paper see motivates authors approach advantage modeling receptive fields random draws authors provide evidence models capture second order statistics experimental receptive fields clear advantage modeling approach compared say fitting LN model every neuron authors state performance metrics make comparisons relevant techniques Authors mention RKHSs times even theoretical discussion Appendix However clear connection important relevant Reading paragraph equation sorry line numbers provided discussion section feels like authors want claim learning efficiency results related learning RKHSs would nice see point fully developed Instead MNIST authors could use dataset naturalistic complex stimuli theoretical study makes receptive field covariance compatible incompatible downstream learning pursued may related RKHS picture authors hinting authors great list possible future directions One directions could pursued paper Black line Figure visible Reviewer submitted article authors present novel framework modeling receptive fields sensory cortices introducing concept structured receptive field drawn biased probability distribution model sensory neurons interpretable fashion still incorporating randomness crucial biological systems article broadly split two parts First structured receptive fields used fit experimental data biological receptive fields suitability machine learning applications discussed results interesting novel described points respects analysis explication results could improved Major points results matching biological receptive fields based qualitativenot quantitativecomparisons authors compare covariance matrices experimental data fitted model based similar look plot One way results might strengthened including null hypothesis like purely random receptive fields receptive fields based Gabor filters V authors nice job motivating theory theory random feature networks machine learning literature would also liked see authors discuss detail ideas context previous computational neuroscience approaches describing receptive fields including efficient coding sparse coding related approaches establishing structured weights lead faster learning Section convincing way show view would define metric speed learning eg area learning curve ii optimize learning rate separately network respect metric iii compare metric two networks separately optimized learning rates way simulations currently done leaves open possibility slower learning unstructured case might due suboptimal choice learning rate Minor points section authors explain L regularization parameter set possible presented results would change parameter set differently first paragraph Section claim made without specifying particular task adding inductive bias improves learning authors careful statements like elsewhere paper since obviously aware given illustrate point simulations inductive bias expected improve learning well matched particular computational task actually worsen learning case Section authors state GP covariance function reflects statistical regularities within sensory inputs network authors provide citation statement cases considered secondtolast paragraph section bandwidth fhi flo equation section stochastic coefficients cosomegak sinomegak independent end section authors precisely explain statement smoothness also controlled overall magnitude nonzero eigenvalues describing computation Cdata Section explanation meant centered receptive fields could helpful Section unit pixels seems incorrect since squared pixel generally meant pixel explanation mechanism leads prominent dark patches covariance matrices shown Figure would appreciated result Figures E extent Figure authors mean remarkably well might addressed response major point Figure E authors comment faster decay spectrum model Could due noise effect related calcium imaging text describing Figure stated Cdata shown zoomed nothing mentioned caption mention scale given one look supplemental figure get still imprecise idea whats going Given dark patches still clearly visible full covariance plotted supplemental figure would recommend either showing full version instead Figure describing zoomedin version clearly plots machine learning applications authors add green curves fact show higher test errors blue ones tautological since parameters latter optimized precisely minimize test error may value curves point illustrate poorly chosen inductive bias worse inductive bias doesnt seem borne pairs green vs orange curves figures doesnt seem point made text last paragraph appendix authors claim functions small Hnorm easier learn larger norm statement made precise perhaps specific regression algorithm Reviewer paper authors considered receptive fields random samples parametrized distributions gaussian process particular following modalities insect mechanosensors mammalian visual cortex neurons demonstrated random feature neurons RFN remove high frequency noise boost signals Finally also show RFNs enable efficient learning number neurons training time perspective topic general interest visual neuroscience growing subfield within computational neuroscience intersecting artificial neural networks comments questions motivations implications work attached theoretical analysis equivalence structured weight vector transformation filtering Gaussian Processing eigenbasis random projection onto spherical random gaussian interesting Aside connection kernel theory learning broader implications finding either sensory neuroscience deep learning fields would helpful get discussion around topic revised version motivation using Gaussian Process type generative model mathematical andor neuro perspectives possible sampling random filters Gaussian Process mathematically tractable also biologically plausible compared say learning filters taskdriven optimization would helpful get discussion around revised version surprising filter samples GPs covariance real data match various properties real neural populations might helpful control models fail capture biological fidelity example Fig revised version Minor comment name structured random receptive field properly defined paper bit vague initially thought random implies stochastic Dapello et al Overall think paper interesting recommend acceptance revisions authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes Reviewer None PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer Reviewer Figure Files revising submission please upload figure files Preflight Analysis Conversion Engine PACE digital diagnostic tool httpspacevapexcovantagecom PACE helps ensure figures meet PLOS requirements use PACE must first register user login navigate UPLOAD tab find detailed instructions use tool encounter issues questions using PACE please email us grosolpserugif Data Requirements Please note condition publication PLOS data policy requires make available data used draw conclusions outlined manuscript Data must deposited appropriate repository included within body manuscript uploaded supporting information includes numerical values used generate graphs histograms etc example PLOS Biology see httpwwwplosbiologyorgarticleinfoAdoiFFjournalpbios Reproducibility enhance reproducibility results recommend deposit laboratory protocols protocolsio protocol assigned identifier DOI cited independently future Additionally PLOS ONE offers option publish peerreviewed clinical study protocols Read information sharing protocols httpsplosorgprotocolsutmmediumeditorialemailutmsourceauthorlettersutmcampaignprotocols PLoS Comput Biol Oct e Author response Decision Letter Oct e Published online Oct doi journalpcbir Author response Decision Letter Copyright License information Disclaimer Copyright notice Jun Attachment Submitted filename coverletterresubmissionpdf Click additional data file pdf PLoS Comput Biol Oct e Decision Letter Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Aug Dear Dr Harris pleased inform manuscript Structured random receptive fields enable informative sensory encodings provisionally accepted publication PLOS Computational Biology manuscript formally accepted need complete formatting changes receive follow email member team touch set requests Please note manuscript scheduled publication made required changes swift response appreciated IMPORTANT editorial review process complete PLOS permit corrections spelling formatting significant scientific errors point onwards Requests major changes affect scientific understanding work cause delays publication date manuscript institutions press office journal office choose press release paper automatically opted early publication ask notify us institution planning press release article press must coordinated PLOS Thank supporting Open Access publishing looking forward publishing work PLOS Computational Biology Best regards Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer thank authors response appreciate effort put revision Although argued many asked beyond scope paper disagree agree necessary address concerns Clarification new additions paper including Appendix adequate dont comments suggestions recommend acceptance reading paper rebuttal think negative recommending rejection first review apologize think fair assessment would major revision Reviewer authors done excellent job responding earlier comments pleased recommend manuscript accepted publication especially appreciated new appendix section greatly strengthens mathematical foundations work Two minor comments follow things noticed reading manuscript might help improve paper final publication bar plot part Fig illustrating results three models described line perhaps also including sketch receptive fields two null models look like would helpful way visually summarize result similar visualization could helpful Fig Perhaps missed Figures exactly percentages reported panel calculated Comparing orange curves green curves doesnt appear numbers correspond curves appear next straightforward way could discern authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer PLoS Comput Biol Oct e Acceptance letter Oct e Published online Oct doi journalpcbir Acceptance letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Oct PCOMPBIOLDR Structured random receptive fields enable informative sensory encodings Dear Dr Harris pleased inform manuscript formally accepted publication PLOS Computational Biology manuscript production department notified publication date due course corresponding author soon receiving typeset proof review ensure errors introduced production Please review PDF proof manuscript carefully last chance correct errors Please note major changes affect scientific understanding work likely cause delays publication date manuscript Soon final files uploaded unless opted early version manuscript published online date early version articles publication date final article published URL versions paper accessible readers Thank supporting PLOS Computational Biology openaccess publishing looking forward publishing work kind regards Anita Estes PLOS Computational Biology Carlyle House Carlyle Road Cambridge CB DN United Kingdom grosolploibpmocsolp Phone ploscompbiolorg PLOSCompBiol Articles PLOS Computational Biology provided courtesy PLOS PLoS Comput Biol Oct e Published online Oct doi journalpcbi PMCID PMC PMID Structured random receptive fields enable informative sensory encodings Biraj Pandey Formal analysis Investigation Software Validation Visualization Writing original draft Writing review editing Marius Pachitariu Data curation Writing review editing Bingni W Brunton Formal analysis Resources Supervision Visualization Writing review editing Kameron Decker Harris Conceptualization Formal analysis Investigation Supervision Visualization Writing original draft Writing review editing Biraj Pandey Department Applied Mathematics University Washington Seattle Washington United States America Find articles Biraj Pandey Marius Pachitariu Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Find articles Marius Pachitariu Bingni W Brunton Department Biology University Washington Seattle Washington United States America Find articles Bingni W Brunton Kameron Decker Harris Department Computer Science Western Washington University Bellingham Washington United States America Find articles Kameron Decker Harris Xuexin Wei Editor Author information Article notes Copyright License information Disclaimer Department Applied Mathematics University Washington Seattle Washington United States America Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Department Biology University Washington Seattle Washington United States America Department Computer Science Western Washington University Bellingham Washington United States America University Texas Austin UNITED STATES Corresponding author authors declared competing interests exist Email udeuwwsirrahnoremak Received Oct Accepted Aug Copyright Pandey et al open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Associated Data Supplementary Materials Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF pcbispdf GUID EEABECFA Attachment Submitted filename coverletterresubmissionpdf pcbispdf GUID FDDFCFDCBEDFB Data Availability Statement relevant data within manuscript Supporting information files Abstract Brains must represent outside world animals survive thrive early sensory systems neural populations diverse receptive fields structured detect important features inputs yet significant variability ignored classical models sensory neurons model neuronal receptive fields random variable samples parameterized distributions demonstrate model two sensory modalities using data insect mechanosensors mammalian primary visual cortex approach leads significant theoretical connection foundational concepts receptive fields random features leading theory understanding artificial neural networks modeled neurons perform randomized wavelet transform inputs removes high frequency noise boosts signal random feature neurons enable learning fewer training samples smaller networks artificial tasks structured random model receptive fields provides unifying mathematically tractable framework understand sensory encodings across spatial temporal domains Author summary Evolution ensured animal brains dedicated extracting useful information raw sensory stimuli discarding everything else Models sensory neurons key part theories brain represents world work model tuning properties sensory neurons way incorporates randomness builds bridge leading mathematical theory understanding artificial neural networks learn models capture important properties large populations real neurons presented varying stimuli Moreover give precise mathematical formula sensory neurons two distinct areas one involving gyroscopic organ insects visual processing center mammals transform inputs also find artificial models imbued properties real neurons learn efficiently shorter training time fewer examples mathematical theory explains findings work expands understanding sensory representation large networks benefits neuroscience machine learning communities Introduction long argued brain uses large population neurons represent world view sensory stimuli encoded responses population used downstream areas diverse tasks including learning decisionmaking movement control sensory areas different neurons responding differing stimuli also providing measure redundancy However still lack clear understanding response properties wellsuited different sensory modalities One way approach sensory encoding understanding neuron would respond arbitrary stimuli Experimentally typically present many stimuli animal measure responses sensory neurons attempt estimate kind model neurons respond arbitrary stimulus common assumption neuron computes linear filter stimulus drives spiking nonlinear spikegenerating mechanism Mathematically assumption summarized number measured spikes stimulus x equal w x weight vector w nonlinearity weights w define filtering properties neuron also known receptive field model known linearnonlinear LN model also common form artificial neuron artificial neural networks ANNs LN models used extensively describe firing diverse neurons various sensory modalities vertebrates invertebrates mammalian visual system LN models used characterize retinal ganglion cells lateral geniculate neurons simple cells primary visual cortex V also used characterize auditory sensory neurons avian midbrain somatosensory neurons cortex insects used understand response properties visual interneurons mechanosensory neurons involved proprioception auditory neurons courtship behavior Given stimulus presented neural response data one estimate receptive fields population neurons Simple visual receptive fields classically understood similar wavelets particular spatial frequency angular selectivity mechanosensory areas receptive fields selective temporal frequency short time window Commonly parametric modeling Gabor wavelets smoothing regularization etc used produce clean receptive fields Yet data alone show noisy receptive fields perhaps best modeled using random distribution show modeling receptive fields random samples produces realistic receptive fields reflect structure noisiness seen experimental data importantly perspective creates significant theoretical connections foundational ideas neuroscience artificial intelligence connection helps us understand receptive fields structures structure relates kinds stimuli relevant animal Modeling filtering properties population LN neurons samples random distribution leads study networks random weights machine learning ML networks known random feature networks RFNs study RFNs rapidly gained popularity recent years large part offers theoretically tractable way study learning properties ANNs weights tuned using data RFN contains many neurons approximate functions live wellunderstood function space function space called reproducing kernel Hilbert space RKHS depends network details particular weight ie receptive field distribution Learning framed approximating functions space limited data Several recent works highlight RFN theorys usefulness understanding learning neural systems Bordelon Canatar Pehlevan series papers shown neural codes allow learning examples spectral properties secondorder statistics aligns spectral properties task applied V found neural code aligned tasks depend low spatial frequency components Harris constructed RFN model sparse networks found associative centers like cerebellum insect mushroom body showed areas may behave like additive kernels architecture also considered Hashemi et al classes kernels beneficial learning high dimensions learn fewer examples remain resilient input noise adversarial perturbation Xie et al investigated relationship fraction active neurons model cerebellumcontrolled neuron thresholdsand generalization performance learning movement trajectories vast majority network studies random weights weights w drawn Gaussian distribution independent entries sampling equivalent fully unstructured receptive field looks like white noise Closely related work previous study ANNs showed directly learning structured receptive fields could improve image classification deep networks receptive fields parametrized sum Gaussian derivatives fourth order led better performance rival architectures low data regimes paper study effect structured yet random receptive fields lead informative sensory encodings Specifically consider receptive fields generated Gaussian process GP thought drawing weights w Gaussian distribution particular covariance matrix show networks random weights project input new basis filter particular components theory introduces realistic structure receptive fields random feature models crucial current understanding artificial networks Next show receptive field datasets two disparate sensory systems mechanosensory neurons insect wings V cortical neurons mice monkeys wellmodeled GPs covariance functions wavelet eigenbases Given success modeling data GP apply weight distributions RFNs used synthetic learning tasks find structured weights improve learning reducing number training examples size network needed learn task Thus structured random weights offer realistic generative model receptive fields multiple sensory areas understand performing random change basis change basis enables network represent important properties stimulus demonstrate useful learning Results construct generative model receptive fields sensory neurons use weights ANN refer network structured random feature network first review basics random feature networks details rationale behind generative model process generate hidden weights main theory result networks weights transform inputs new basis filter particular components thus bridging sensory neuroscience theory neural networks Next show neurons two receptive field datasetsinsect mechanosensory neurons mammalian V cortical neuronsare welldescribed generative model close resemblance secondorder statistics sampled receptive fields principal components data model Finally show performance structured random feature networks several synthetic learning tasks hidden weights generative model allows network learn fewer training examples smaller network sizes Theoretical analysis consider receptive fields generated GPs order connect foundational concept sensory neuroscience theory random features artificial neural networks GPs thought samples Gaussian distribution particular covariance matrix initialize hidden weights RFNs using GPs show using GP causes network project input new basis filter particular components basis determined covariance matrix Gaussian useful removing irrelevant noisy components input use results study space functions RFNs containing many neurons learn connecting construction theory kernel methods Random feature networks start introducing main learning algorithm neuronal model work RFN Consider twolayer feedforward ANN Traditionally weights initialized randomly learned backpropagation minimizing loss objective sharp contrast RFNs hidden layer weights sampled randomly distribution fixed hidden unit computes random feature input output layer weights trained Fig Note weights randomly drawn neurons response deterministic function input given weights Open separate window Fig Random feature networks structured weights study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distribution Mathematically hidden layer activations output given h W x h x R stimulus h h h h R hidden neuron responses R predicted output use rectified linear ReLU nonlinearity x max x applied entrywise Eq hidden layer weights W w w w R drawn randomly fixed readout weights trained RFNs RFN experiments train readout weights R offset R using support vector machine SVM classifier squared hinge loss penalty regularization strength tuned range fold crossvalidation RFNs include threshold hidden neurons although could help certain contexts vast majority studies RFNs neurons weights w R initialized iid spherical Gaussian distribution w N call networks built way classical unstructured RFNs Fig propose variation hidden weights initialized w N C C R positive semidefinite covariance matrix call networks structured RFNs Fig mean weights random specified covariance compare unstructured structured weights equal footing normalize covariance matrices Tr C Tr ensures mean square amplitude weights E w Receptive fields modeled linear weights Sensory neurons respond preferentially specific features inputs stimulus selectivity often summarized neurons receptive field describes features sensory space elicits responses stimulated Mathematically receptive fields modeled linear filter stimulus space Linear filters also integral component widely used LN model sensory processing According model firing rate neuron nonlinear function applied projection stimulus onto lowdimensional subspace linear filter linear filter model receptive fields explain responses individual neurons diverse stimuli used describe disparate sensory systems like visual auditory somatosensory systems diverse species including birds mammals insects stimuli uncorrelated filters estimated computing spike triggered average STA average stimulus elicited spike neuron stimuli correlated STA filter whitened inverse stimulus covariance matrix Often STAs denoised fitting parametric function STA Gabor wavelets simple cells V model receptive field neuron weight vector w nonlinear function Instead fitting parametric function construct covariance functions realization resulting Gaussian process resembles biological receptive field Fig Structured weights project filter input covariance eigenbasis generate network weights Gaussian processes GP whose covariance functions inspired receptive fields sensory neurons brain definition GP stochastic process finite observations follow Gaussian distribution find networks weights project inputs new basis filter irrelevant components see adds inductive bias classical RFNs tasks naturalistic inputs improves learning view weight vector w finitedimensional discretization continuous function w sample GP continuous function domain compact subset R assume discretized using grid equally spaced points w w Let input realvalued function x domain could represent finite timeseries image luminance retina complicated spatiotemporal sets like movie continuous setting dimensional inner product w x w x gets replaced L inner product w x w x Every GP fully specified mean covariance function C always assume mean zero study different covariance functions KosambiKarhunenLove theorem realization zeromean GP random series representation w z terms standard Gaussian random variables z N functions weights pairs eigenvalue eigenfunction pairs covariance operator C L L C f C f continuous analog covariance matrix C C positive definite opposed semidefinite eigenfunctions form complete basis L Using Eq inner product stimulus neurons weights w x z x z x z x x x Eq shows structured weights compute projection input x onto eigenfunction x reweight filter eigenvalue taking inner product random Gaussian weights z illuminating see continuous equations look like dimensional discrete setting Samples finitedimensional GP used hidden weights RFNs w N C First GP series representation Eq becomes w z matrices eigenvalues eigenvectors z N Gaussian random vector definition covariance matrix C E w w equal steps linear algebra Finally Eq analogous w x z x Since orthogonal matrix x equivalent change basis diagonal matrix shrinks expands certain directions perform filtering summarized following theorem Theorem Basis change formula Assume w N C C eigenvalue decomposition x R define x x w x z x z N Theorem says projecting input onto structured weight vector first filtering input GP eigenbasis random projection onto spherical random Gaussian form GP eigenbasis determined choice covariance function covariance function compatible input structure hidden weights filter irrelevant features noise stimuli amplifying descriptive features inductive bias facilitates inference stimuli downstream predictor spherical Gaussian distribution canonical choice unstructured RFNs simple way evaluate effective kernel structured RFNs k struct x x k unstruct x x see Appendix expression structured kernel provides concrete connection kernel theory learning using nonlinear neural networks readers interested kernel theories full example simulation results work given Appendix show exponential reduction number samples needed learn frequency detection using structured versus unstructured basis Fig Appendix Examples random yet structured receptive fields goal model weights artificial neurons way inspired biological neurons receptive fields Structured RFNs sample hidden weights GPs structured covariance construct covariance functions make generated weights resemble neuronal receptive fields start toy example stationary GP wellunderstood Fourier eigenbasis show receptive fields generated GP selective frequencies timeseries signals construct locally stationary covariance models insect mechanosensory V neuron receptive fields models shown good match experimental data Warmup Frequency selectivity stationary covariance illustrate results theoretical analysis start toy example temporal receptive fields selective particular frequencies example may familiar readers comfortable Fourier series basic signal processing Let input finite continuous timeseries x interval L use covariance function C k k cos k stationary process k k L k th natural frequency k weight coefficients covariance function Eq stationary means depends difference timepoints Applying compound angle formula get C k k cos k cos k sin k sin k Since sinusoidal functions cos k sin k form orthonormal basis L Eq eigendecomposition covariance eigenfunctions sines cosines eigenvalues k Eq know structured weights covariance form random series w k k z k cos k z k sin k z k z k N Thus receptive fields made sinusoids weighted k Gaussian variables z k z k Suppose want receptive fields retain specific frequency information signal filter rest Take k k k f lo k f hi call bandlimited spectrum passband f lo f hi bandwidth f hi f lo bandwidth increases receptive fields become less smooth since made wider range frequencies k nonzero decay certain rate rate controls smoothness resulting GP receptive fields act input signals x implicitly transform inputs Fourier basis filter frequencies based magnitude k bandlimited setting frequencies outside passband filtered makes receptive fields selective particular range frequencies ignore others hand classical random features weight frequencies equally even though natural settings high frequency signals corrupted noise Insect mechanosensors next consider particular biological sensor sensitive timehistory forces Campaniform sensilla CS domeshaped mechanoreceptors detect local stress strain insect exoskeleton embedded cuticle deformation cuticle bending torsion induces depolarizing currents CS opening mechanosensitive ion channels CS encode proprioceptive information useful body state estimation movement control diverse tasks like walking kicking jumping flying model receptive fields CS believed critical flight control namely ones found base halteres wings Fig Halteres wings flap rhythmically flight rotations insects body induce torsional forces felt active sensory structures CS detect small strain forces thereby encoding angular velocity insect body Experimental results show haltere wing CS selective broad range oscillatory frequencies STAs smooth oscillatory selective frequency decay time Fig B Open separate window Fig Random receptive field model insect mechanosensors Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shown model temporal receptive fields locally stationary GP bandlimited spectrum Examples receptive fields generated GP shown Fig C inputs CS modeled finite continuous timeseries x finite interval L neuron weights generated covariance function C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum k k L k th natural frequency warmup frequency selectivity weights accounted parameters f lo f hi bandwidth f hi f lo increases receptive fields built wider selection frequencies makes receptive fields less smooth Fig field localized near decay determined parameter increases receptive field selective larger time windows eigenbasis covariance function Eq similar Fourier eigenbasis modulated decaying exponential eigenbasis orthonormal basis span k e cos k k e sin k nonorthogonal set functions L hidden weights transform timeseries inputs eigenbasis discard components outside passband frequencies f lo f hi fit covariance model receptive field data CS neurons wings hawkmoth Manduca sexta data Briefly CS receptive fields estimated spiketriggered average STA experimental mechanical stimuli wings stimuli generated bandpassed white noise Hz characterize receptive fields population CS neurons compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field number samples case kHz ms samples normalization sets overall scale covariance matrix data covariance matrix shows tridiagonal structure Fig main diagonal positive diagonals negative diagonals decay away top left matrix Open separate window Fig Spectral properties mechanosensory RFs model similar compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behavior fit covariance model data optimize parameters f lo f hi finding f lo Hz f hi Hz ms best fit sensilla data minimizing Frobenius norm difference C data model see Appendix resulting model covariance matrix Fig B matches data covariance matrix Fig remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig B Appendix simulate effect finite number neurons generate weight vectors equal number neurons recorded recompute model covariance matrix Fig C call finite neuron model covariance matrix C finite shows bump bloblike structures evident C data C model result suggests bumpy structures attributed small number recorded neurons hypothesize effects would disappear larger dataset C data would closely resemble C model comparison also calculate Frobenius difference null models unstructured covariance model Fourier model unstructured model Frobenius norm difference Fourier model sensilla covariance model much lower difference compared null models fitting data accurately show covariance matrices sampled receptive fields null models Fig C E Appendix Comparing eigenvectors eigenvalues data model covariance matrices find spectral properties C model C finite similar C data eigenvalue curves models match data quite well Fig E curves directly comparable covariance normalized trace makes sum eigenvalues unity data model covariance matrices lowdimensional first data eigenvectors explain variance top explain top eigenvectors model finite sample match data quite well Fig Primary visual cortex turn visually driven neurons mammalian primary cortex Primary visual cortex V earliest cortical area processing visual information Fig neurons V detect small changes visual features like orientations spatial frequencies contrast size Open separate window Fig Random receptive field model Primary Visual Cortex V Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fields model receptive fields simple cells V clear excitatory inhibitory regions light shone excitatory regions increase cells response viceversa Fig B shape regions determines orientation selectivity widths determine frequency selectivity receptive fields centered location visual field decay away integrate visual stimuli within small region center Gabor functions widely used mathematical model receptive fields simple cells model receptive fields using another locally stationary GP show examples generated receptive fields Fig C Consider inputs cortical cells continuous twodimensional image x domain L L x R Since image realvalued x grayscale contrast single color channel pixel values neuron weights generated covariance function following form C exp f smooth receptive fields exp c c localized center c receptive field center defined c size receptive field determined parameter increases receptive field extends farther center c Fig Spatial frequency selectivity accounted bandwidth parameter f f decreases spatial frequency receptive field goes making weights less smooth Fig E eigendecomposition covariance function Eq leads orthonormal basis single scale Hermite wavelets c wavelet eigenfunctions Hermite polynomials modulated decaying Gaussian k e c H k c k c k H k k th physicists Hermite polynomial eigenfunctions nonzero centers c shifted versions Eq detailed derivation values constants c c c normalization Appendix use Eq model receptive field data V neurons recorded calcium imaging transgenic mice expressing GCaMPs mice headfixed running airfloating ball presented unique white noise images pixels using Psychtoolbox pixels white black equal probability Images upsampled resolution screens via bilinear interpolation stimulus corrected eyemovements online using custom code responses cells collected using twophoton mesoscope preprocessed using Suitep Receptive fields calculated white noise images deconvolved calcium responses cells using STA covariance analysis picked cells signaltonoise SNR threshold gave us cells SNR calculated smaller set images presented twice using method preprocessing step moved center mass every receptive field center visual field compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field case pixels pixels data covariance matrix resembles tridiagonal matrix However diagonals nonzero equally spaced segments Additionally values decay away center matrix show C data zoomed nonzero region around center matrix Fig corresponds pixel region around center full pixel matrix full covariance matrix shown Fig F Appendix Open separate window Fig Spectral properties V RFs model similar compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons red covariance model number offdiagonals center rate decay away center determined parameters f c respectively covariance pixels decays function distance c leads equallyspaced nonzero segments hand covariance also decays function distance pixels brings diagonal structure model frequency parameter f increases number offdiagonals increases Pixels generated weights become correlated weights become spatially smoother size parameter increases diagonals decay slower center c increasing correlations center pixel leading significant weights occupy visual field optimize parameters fit data finding f pixels minimizing Frobenius norm difference C data model need optimize center parameter c since preprocess data receptive fields centered c center grid resulting model covariance matrix Fig B data covariance matrix Fig match remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig G Appendix simulate effect finite number neurons generate weights equal number neurons data compute C finite shown Fig C finite matrix C finite looks even like C data shows negative covariances far center result finite sample size comparison also calculate normalized Frobenius difference null models unstructured covariance model translation invariant V model translation invariant model remove spatially localizing exponential Eq fit spatial frequency parameter f unstructured model Frobenius norm difference translation invariant model V covariance model much lower difference better fit data show covariance matrices sampled receptive fields null models Fig H J Appendix Similar spectral properties evident eigenvectors eigenvalues C model C finite C data analytical forms derived Eq Fig E covariances normalized unit trace Note analytical eigenfunctions shown finer grid model data analysis performed continuous space differences eigenfunctions eigenvalues analytical model results due discretization Examining eigenvectors Fig also see good match although rotations differences ordering eigenvectors explain variance receptive field data reference top eigenvectors explain variance data variance model eigenvalue curves models analytical forms match data Fig E reasonably well although well mechanosensors Appendix repeat analysis receptive fields measured different stimulus sets mouse different experimental dataset nonhuman primate V findings consistent results shown Fig K P Appendix Advantages structured random weights artificial learning tasks hypothesis neuronal inductive bias structured receptive fields allows networks learn fewer neurons training examples steps gradient descent classification tasks naturalistic inputs examine hypothesis compare performance structured receptive fields classical ones several classification tasks find artificial learning tasks structured random networks learn accurately smaller network sizes fewer training examples gradient steps Frequency detection CS naturally encode timehistory strain forces acting insect body sensors inspired temporal filtering properties shown accurately classify spatiotemporal data Inspired result test sensillainspired mechanosensory receptive fields timeseries classification task Fig top example presented network ms timeseries sampled kHz goal detect whether example contains sinusoidal signal positive examples sinusoidal signals f Hz corrupted noise SNR dB negative examples Gaussian white noise matched amplitude positive examples Note frequency detection task linearly separable random phases positive negative examples See Appendix additional details including definition SNR crossvalidation used find optimal parameters f lo Hz f hi Hz ms Open separate window Fig Random mechanosensory weights enable learning fewer neurons timeseries classification tasks show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible task number hidden neurons structured RFN significantly outperforms classical RFN show test performance using tuned parameters Fig Even noisy task achieves test error using hidden neurons Meanwhile classical network takes neurons achieve similar error Predictably performance suffers weights incompatible task show results f lo Hz f hi Hz Fig incompatible RFN performs better chance error much worse classical RFN takes neurons achieve test error test error decrease level even additional hidden neurons Frequency XOR task challenge mechanosensorinspired networks difficult task build frequency ExclusiveOR XOR problem Fig B top XOR binary function returns true inputs different otherwise returns false XOR classical example function linearly separable thus harder learn inputs ms timeseries sampled kHz inputs either contain pure frequency f Hz f Hz mixed frequency signals f f white noise pure mixed frequency cases add noise SNR See Appendix details goal task output true input contains either pure tone false input contains mixed frequencies white noise tune GP covariance parameters f lo f hi Eq using crossvalidation cross validation procedure algorithmic details identical frequency detection task Using cross validation find optimal parameters f lo Hz f hi Hz ms incompatible weights take f lo Hz f hi Hz structured RFN significantly outperform classical RFN number hidden neurons show network performance using parameters Fig B Classification error achieved hidden neurons sharp contrast classical RFN requires hidden neurons achieve error incompatible weights network needs neurons achieve test error improve larger network sizes four input subclasses consistently fails classify pure Hz sinusoidal signals outside passband Image classification next test Vinspired receptive fields two standard digit classification tasks MNIST KMNIST MNIST KMNIST datasets contain images handwritten digits MNIST Arabic numerals whereas KMNIST Japanese hiragana phonetic characters datasets come split training test examples classes training examples per class Every example grayscale image centered characters hidden weight center c chosen uniformly random pixels ensures networks weights uniformly cover image space fact means network represent sum locallysmooth functions see Appendix use network hidden neurons tune GP covariance parameters f Eq using fold cross validation MNIST training set parameter ranges pixels optimal parameters found grid search find optimal parameters pixels f pixels refit optimal model using entire training set parameters MNIST used KMNIST task without additional tuning Vinspired achieves much lower average classification error compared classical RFN number hidden neurons show learning performance using parameters MNIST task Fig achieve error MNIST task requires neurons versus neurons classical RFN structured RFN achieves error neurons Qualitatively similar results hold KMNIST task Fig B although overall errors larger reflecting harder task achieve error KMNIST requires neurons versus neurons classical RFN structured RFN achieves error neurons Open separate window Fig Random V weights enable learning fewer neurons fewer examples digit classification tasks show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properly network performance suffers GP covariance parameters match task happens size parameter smaller stroke width spatial scale f doesnt match stroke variations character Taking incompatible parameters f Fig B structured weights performs worse classical RFN tasks hidden neurons achieves relatively poor test errors MNIST Fig KMNIST Fig B Structured weights improve generalization limited data Alongside learning fewer hidden neurons V structured RFNs also learn accurately fewer examples test fewshot learning using image classification datasets training examples reduced training examples per class test set GP parameters remain Structured encodings allow learning fewer samples unstructured encodings show fewshot learning results Fig C networks performance saturate past hundred hidden neurons MNIST lowest error achieved V structured RFN versus classical RFN using incompatible weights Fig C structured network acheives error using structured features KMNIST task opposed classical RFN using incompatible weights Fig Networks train faster initialized structured weights study effect structured weights initialization strategy fullytrained neural networks weights network vary hypothesized structured initialization allows networks learn faster ie training loss test error would decrease faster unstructured weights shown performance RFNs improves biologically inspired weight sampling However RFNs Eq readout weights modified training hidden weights W frozen initial value compare biologicallymotivated initialization classical initialization variance inversely proportional number hidden neurons w unstruct N initialization widely known Kaiming normal scheme thought stabilize training dynamics controlling magnitude gradients classical approach ensures Tr fair comparison scale structured weight covariance matrix Tr C studies RFNs trace equal weight scale absorbed readout weights due homogeneity ReLU compare structured unstructured weights MNIST KMNIST tasks common benchmarks fullytrained networks architecture single hidden layer feedforward neural network Fig hidden neurons crossentropy loss training sets minimized using simple gradient descent GD epochs fair comparison learning rate optimized network separately define area training loss curve metric speed learning perform grid search range e e learning rate minimizes metric resulting parameters structured unstructured incompatible networks respectively parameters image classification MNIST KMNIST tasks Vinitialized network minimizes loss function faster classically initialized network MNIST task V network achieves loss value epochs compared network Fig see qualitatively similar results KMNIST task end training Vinspired networks loss classically initialized network reaches Fig B find Vinitialized network performs better classical initialization covariance parameters match task incompatible parameters Vinitialized network achieves loss value MNIST KMNIST Open separate window Fig V weight initialization fullytrained networks enables faster training digit classification tasks show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible task minimize training loss faster Vinitialized network also generalizes well achieves lower test error end training MNIST achieves test error compared error classically initialized network using incompatible weights Fig C KMNIST see error compared error classical initalization using incompatible weights Fig see similar results across diverse hidden layer widths learning rates Fig Q Appendix benefits evident wider networks smaller learning rates Furthermore structured weights show similar results trained epochs rate neurons shown optimizers like minibatch Stochastic Gradient Descent SGD ADAM batch size rate neurons shown Structured initialization facilitates learning across wide range networks However improvement universal significant benefit found initializing early convolutional layers deep network AlexNet applying ImageNet dataset shown Appendix Fig U Appendix large amounts training data fact small fraction network initialized structured weights could explain null result Also many scenarios incompatible structured weights get performance par compatible ones end training poor inductive bias overcome Improving representation structured random weights shown structured receptive field weights improve performance RFNs fullytrained networks number supervised learning tasks long receptive fields compatible task performance gains unstructured features possible incompatible networks performs better even worse using classical unstructured weights results understood theoretical framework Structured weights effectively cause input x undergo linear transformation new representation x following Theorem examples new representation bandlimited due design covariance function V weights eigenvalues nonzero spectrum decays exponentially acts lowpass filter moving bandlimited representation filter noisehighfrequency componentsand reduce dimensionalitycoordinates x outside passband zero general noise dimensionality make learning harder easiest understand effects frequency detection task simplicity assume using stationary features warmup frequency detection task signal power contained f Hz frequency everything else due noise weights compatible task means w sum sines cosines frequencies k passband includes f narrower make bandwidth still retaining signal higher SNR x becomes since noise filtered see Appendix Discussion paper describe random generative model receptive fields sensory neurons Specifically model receptive field random filter sampled Gaussian process GP covariance structure matched statistics experimental neural data show two kinds sensory neuronsinsect mechanosensory simple cells mammalian Vhave receptive fields welldescribed GPs particular generated receptive fields secondorder statistics principal components match receptive field data Theoretically show individual neurons perform randomized transformation filtering inputs connection provides framework sensory neurons compute input transformations like Fourier wavelet transforms biologically plausible way numerical results using structured random receptive fields show offer better learning performance unstructured receptive fields several benchmarks structured networks achieve higher test performance fewer neurons fewer training examples unless frequency content receptive fields incompatible task networks fully trained initializing structured weights leads better network performance measured training loss generalization fewer iterations gradient descent Structured random features may understood theoretically transforming inputs informative basis retains important information stimulus filtering away irrelevant signals Modeling sensory neurons modalities random feature formulation natural extension traditional linearnonlinear LN neuron model approach may applied brain regions LN models successful instance sensory areas primarily feedforward connectivity like somatosensory auditory regions neurons auditory somatosensory systems selective spatial temporal structures stimuli spatial structure emerges networks trained artificial tactile tasks receptive fields could modeled GPs spatiotemporal covariance functions could useful artificial tasks spatiotemporal stimuli movies multivariate timeseries Neurons localized random temporal responses found compatible manifold coding decisionmaking task GPs complementary approach traditional sparse coding efficient coding hypotheses connections theories interesting future research Receptive fields development generative model offers new directions explore biological basis computational principles behind receptive fields Development lays basic architecture conserved animal animal yet details every neural connection specified leading amount inevitable randomness least initially receptive fields random constrained covariance natural ask biology implements Unsupervised Hebbian dynamics local inhibition allow networks learn principal components input interesting future direction similar learning rules may give rise overcomplete nonorthogonal structure similar studied may prove biologically plausible weights result taskdriven optimization assumes receptive field properties actually lie within synaptic weights spatial receptive fields assumption plausible temporal properties receptive fields likely result neurons intrinsic dynamics LN framework model Heterogeneous physiological eg resonator dynamics mechanical position shape mechanosensor relative body structure properties combine give diverse temporal receptive field structures Development thus leverages different mechanisms build structure receptive field properties sensory neurons Connections compressive sensing Random projections seen extensive use field compressive sensing highdimensional signal found measurements long sparse representation Random compression matrices known optimal properties however many cases structured randomness realistic Recent work shown structured random projections local wiring constraints one dimension compatible dictionary learning supporting previous empirical results work shows structured random receptive fields equivalent employing wavelet dictionary dense Gaussian projection Machine learning inductive bias important open question neuroscience machine learning certain networks characterized features architecture weights nonlinearities better others certain problems One perspective network good problem biased towards approximating functions close target known inductive bias depends alignment features encoded neurons task hand approach shows structured receptive fields equivalent linear transformation input build biases Furthermore describe nonlinear properties network using kernel varies depending receptive field structure target function small norm RKHS inductive bias easier learn small norm RKHS means target function varies smoothly inputs Smooth functions easier learn compared fast varying ones way receptive field structure influences ease learning target function conjecture receptive fields neuralinspired distributions affect RKHS geometry target functions norm small RKHS compared RKHS random whitenoise receptive fields leave future work verify conjecture detail Networks endowed principles neural computation like batch normalization pooling inputs residual connections found contain inductive biases certain learning problems Learning datadependent kernels another way add inductive bias also saw initializing fullytrained networks generative models improved speed convergence generalization compared unstructured initialization result consistent known results initialization effect generalization initialization literature mostly focused avoiding explodingvanishing gradients conjecture inductive bias structured connectivity places network closer good solution loss landscape random Vinspired receptive fields model seen similar happens convolutional neural network CNN similarities differences compared brains recent study showed CNNs fixed Vlike convolutional layer robust adversarial perturbations inputs similar vein work using randomly sampled Gabor receptive fields first layer deep network also shown improve performance wavelet scattering transform multilayer network wavelet coefficients passed nonlinearities model similar deep CNNs framework differs randomized model yields wavelets single scale similar studies robustness learning deep networks weights possible Adding layers model sampling weights variety spatial frequencies field sizes would yield random networks behave similar scattering transform offering another connection brain CNNs Directly learning filters Hermite wavelet basis led good perfomance ANNs little data idea extended multiple scales structured random features seen RFN version ideas supporting evidence principles used biology Limitations future directions several limitations random feature approach model neuron responses scalar firing rates instead discrete spikes ignore complex neuronal dynamics neuromodulatory context many details Like LN models random feature model assumes zero plasticity hidden layer neurons However associative learning drive changes receptive fields individual neurons sensory areas like V auditory cortex RFN purely feedforward account feedback connections Recent work suggests feedforward architecture lacks sufficient computational power serve detailed inputoutput model network cortical neurons might need additional layers convolutional filters difficult interpret parameters found fitting receptive field data connect experimental conditions Also GP model weights captures covariance second moments neglects higherorder statistics remains shown theory yield concrete predictions tested vivo experimental conditions random feature receptive field model randomized extension LN neuron model LN model fits parameterized function receptive field contrast random feature framework fits distribution entire population receptive fields generates realistic receptive fields distribution natural question compare goal capture individual differences neuronal receptive fields one resort LN model neurons receptive field fit data random feature model flexible provides direct connection random feature theory mathematically tractable generative connection kernel learning opens door using techniques mainstay machine learning theory literature instance estimate generalization error sample complexity context learning biologically realistic networks see several future directions structured random features connecting computational neuroscience machine learning already stated auditory somatosensory tactile regions good candidates study well developmental principles could give rise random yet structured receptive field properties account plasticity hidden layer one could also analyze neural tangent kernel NTK associated structured features kernels often used analyze ANNs trained gradient descent number hidden neurons large step size small incorporate lateral feedback connections weights could sampled GPs recurrent covariance functions theory may also help explain CNNs fixed Vlike convolutional layer robust adversarial input perturbations filtering high frequency corruptions seems likely structured random features also robust would interesting study intermediate layer weights fullytrained networks approximate samples GP studying covariance structure Finally one could try develop covariance functions optimize RFNs sophisticated learning tasks see near high performancelower error faster training etcon difficult tasks possible Methods methods described throughout Results section details additional results Appendix Supporting information Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF Click additional data file pdf Acknowledgments thank Dario Ringach providing macaque V data Brandon Pratt hawkmoth mechanosensor data grateful Ali Weber Steven Peterson Owen Levin Alice C Schwarze useful discussions thank Sarah Lindo Michalis Michaelos Carsen Stringer help mouse surgeries calcium imaging data processing respectively Funding Statement BP supported UW Applied Math Frederic Wan Endowed Fellowship Terry Keegan Memorial ARCS Endowment Fellowship Natural Science Foundation Graduate Research Fellowship Program Grant DGE MP supported Janelia Research Campus Howard Hughes Medical Institute BWB supported grants FA FA Air Force Office Scientific Research KDH supported Washington Research Foundation postdoctoral fellowship Western Washington University funders role study design data collection analysis decision publish preparation manuscript httpsamathwashingtonedusupportus httpswwwarcsfoundationorgnationalhomepage httpswwwnsfgrfporg httpswwwjaneliaorglabpachitariulab httpswwwafrlafmilAFOSR httpswwwwrfseattleorggrantswrfpostdoctoralfellowships httpscswwueduharri Data Availability relevant data within manuscript Supporting information files References Yuste R neuron doctrine neural networks Nature Reviews Neuroscience doi nrn PubMed CrossRef Google Scholar Fusi Miller EK Rigotti neurons mix high dimensionality higher cognition Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Saxena Cunningham JP Towards neural population doctrine Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Stringer C Pachitariu Steinmetz N Carandini Harris KD Highdimensional geometry population responses visual cortex Nature doi PMC free article PubMed CrossRef Google Scholar Sherrington C Integrative Action Nervous System Cambridge University Press Google Scholar Chichilnisky EJ simple white noise analysis neuronal light responses Network Computation Neural Systems PubMed Google Scholar Sakai HM Naka K Signal transmission catfish retina V Sensitivity circuit Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Clay Reid R Alonso JM Specificity monosynaptic connections thalamus visual cortex Nature doi PubMed CrossRef Google Scholar Jones JP Palmer LA evaluation twodimensional Gabor filter model simple receptive fields cat striate cortex Journal Neurophysiology PubMed Google Scholar Knudsen EI Konishi Centersurround organization auditory receptive fields owl Science doi science PubMed CrossRef Google Scholar H Central mechanisms tactile shape perception Current opinion neurobiology PubMed Google Scholar Rusanen J Frolov R Weckstrm Kinoshita Arikawa K Nonlinear amplification graded voltage signals firstorder visual interneurons butterfly Papilio xuthus Journal Experimental Biology PubMed Google Scholar Fox JL Fairhall AL Daniel TL Encoding properties haltere neurons enable motion feature detection biological gyroscope Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Pratt B Deora Mohren Daniel Neural evidence supports dual sensorymotor role insect wings Proceedings Royal Society B Biological Sciences doi rspb PMC free article PubMed CrossRef Google Scholar Clemens J Ronacher B Feature Extraction Integration Underlying Perceptual Decision Making Courtship Behavior Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Park Pillow JW Receptive field inference localized priors PLoS computational biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Bonin V Histed MH Yurgenson Reid RC Local Diversity FineScale Organization Receptive Fields Mouse Visual Cortex Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Rosenblatt F perceptron probabilistic model information storage organization brain Psychological Review doi h PubMed CrossRef Google Scholar Caron SJC Ruta V Abbott LF Axel R Random convergence olfactory inputs Drosophila mushroom body Nature doi nature PMC free article PubMed CrossRef Google Scholar LitwinKumar Harris KD Axel R Sompolinsky H Abbott LF Optimal Degrees Synaptic Connectivity Neuron e doi jneuron PMC free article PubMed CrossRef Google Scholar Broomhead DS Lowe Radial basis functions multivariable functional interpolation adaptive networks Royal Signals Radar Establishment Malvern United Kingdom Google Scholar Igelnik B Pao YH Stochastic choice basis functions adaptive function approximation functionallink net IEEE transactions neural networks doi PubMed CrossRef Google Scholar Rahimi Recht B Random Features LargeScale Kernel Machines Platt JC Koller Singer Roweis ST editors Advances Neural Information Processing Systems Curran Associates Inc p Google Scholar Liu F Huang X Chen Suykens JAK Random Features Kernel Approximation Survey Algorithms Theory Beyond arXiv cs stat PubMed Arora Du SS Hu W Li Z Wang R FineGrained Analysis Optimization Generalization Overparameterized TwoLayer Neural Networks arXiv cs stat Arora Du SS Hu W Li Z Salakhutdinov R Wang R Exact Computation Infinitely Wide Neural Net arXiv cs stat Chen L Xu Deep Neural Tangent Kernel Laplace Kernel RKHS arXiv cs math stat Neal RM Priors Infinite Networks New York NY Springer New York p Available CrossRef Google Scholar Williams CKI Computation Infinite Neural Networks Neural Computation doi CrossRef Google Scholar Rahimi Recht B Uniform approximation functions random bases th Annual Allerton Conference Communication Control Computing IEEE p Available httpieeexploreieeeorgdocument Bordelon B Canatar Pehlevan C Spectrum Dependent Learning Curves Kernel Regression Wide Neural Networks arXiv cs stat Bordelon B Pehlevan C Population Codes Enable Learning Examples Shaping Inductive Bias bioRxiv Google Scholar Canatar Bordelon B Pehlevan C Spectral Bias TaskModel Alignment Explain Generalization Kernel Regression Infinitely Wide Neural Networks arXiv condmat stat PMC free article PubMed Harris KD Additive function approximation brain arXiv cs qbio stat Hashemi Schaeffer H Shi R Topcu U Tran G Ward R Generalization Bounds Sparse Random Feature Expansions arXiv cs math stat Xie Muscinelli Decker Harris K LitwinKumar Taskdependent optimal representations cerebellar learning bioRxiv Google Scholar Jacobsen JH van Gemert J Lou Z Smeulders AWM Structured Receptive Fields CNNs arXiv cs Paninski L Convergence properties spiketriggered analysis techniques Network Computation Neural Systems p PubMed Google Scholar Rasmussen CE Williams CKI Gaussian Processes Machine Learning Adaptive Computation Machine Learning MIT Press Google Scholar Kosambi DD Statistics function space Journal Indian Mathematical Society New Series Google Scholar Wahba G Spline Models Observational Data SIAM Dickerson BH Fox JL Sponberg Functional diversity generic encoding insect campaniform sensilla Current Opinion Physiology doi jcophys CrossRef Google Scholar Yarger Fox JL Dipteran Halteres Perspectives Function Integration Unique Sensory Organ Integrative Comparative Biology doi icbicw PubMed CrossRef Google Scholar Fox JL Daniel TL neural basis gyroscopic force measurement halteres Holorusia Journal Comparative Physiology doi sz PubMed CrossRef Google Scholar Genton MG Classes Kernels Machine Learning Statistics Perspective Journal Machine Learning Research Dec Google Scholar Hubel DH Wiesel TN Receptive fields single neurones cats striate cortex Journal Physiology doi jphysiolsp PMC free article PubMed CrossRef Google Scholar Marr Hildreth E Brenner Theory edge detection Proceedings Royal Society London Series B Biological Sciences PubMed Google Scholar Martens JB Hermite transformtheory IEEE Transactions Acoustics Speech Signal Processing doi CrossRef Google Scholar Kleiner Brainard Pelli Whats new Psychtoolbox PerceptionECVP Abstract Supplement European Conference Visual Perception ECVP August Arezzo Italy Sofroniew NJ Flickinger King J Svoboda K large field view twophoton mesoscope subcellular resolution vivo imaging Elife e doi eLife PMC free article PubMed CrossRef Google Scholar Pachitariu Stringer C Dipoppa Schrder Rossi LF Dalgleish H et al Suitep beyond neurons standard twophoton microscopy BioRxiv Google Scholar Mohren TL Daniel TL Brunton SL Brunton BW Neuralinspired sensors enable sparse efficient classification spatiotemporal data Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar LeCun Cortes C Burges C MNIST handwritten digit database ATT Labs Online Available httpyannlecuncomexdbmnist Google Scholar Clanuwat BoberIrizar Kitamoto Lamb Yamamoto K Ha Deep Learning Classical Japanese Literature arXiv cs stat K Zhang X Ren Sun J Delving Deep Rectifiers Surpassing HumanLevel Performance ImageNet Classification IEEE International Conference Computer Vision ICCV p Krizhevsky One weird trick parallelizing convolutional neural networks arXiv cs Russakovsky Deng J Su H Krause J Satheesh et al ImageNet Large Scale Visual Recognition Challenge International Journal Computer Vision doi sy CrossRef Google Scholar Pruszynski JA Johansson RS Edgeorientation processing firstorder tactile neurons Nature Neuroscience doi nn PubMed CrossRef Google Scholar Zhao CW Daley MJ Pruszynski JA Neural network models tactile system develop firstorder units spatially complex receptive fields PLOS ONE e doi journalpone PMC free article PubMed CrossRef Google Scholar Xu Ruan C Korpeoglu E Kumar Achan K Temporal Kernel Approach Deep Learning Continuoustime Information arXiv cs Koay SA Charles Thiberge SY Brody CD Tank DW Sequential efficient neuralpopulation coding complex task information bioRxiv PubMed Google Scholar Olshausen BA Field DJ Sparse Coding Overcomplete Basis Set Strategy Employed V Vision Research doi PubMed CrossRef Google Scholar Barlow HB et al Possible principles underlying transformation sensory messages Sensory communication Google Scholar Chalk Marre Tkaik G Toward Unified Theory Efficient Predictive Sparse Coding Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Swanson LW Brain architecture Understanding basic plan vol xv New York NY US Oxford University Press Google Scholar Strausfeld NJ Arthropod Brains Evolution Functional Elegance Historical Significance Harvard University Press Available httpswwwjstororgstablejctvdpvh Zador critique pure learning artificial neural networks learn animal brains Nature Communications doi PMC free article PubMed CrossRef Google Scholar Oja E Principal components minor components linear neural networks Neural Networks doi CrossRef Google Scholar Pehlevan C Sengupta Chklovskii DB Similarity Matching Objectives Lead HebbianAntiHebbian Networks Neural Computation doi necoa PubMed CrossRef Google Scholar Ringach DL Haphazard Wiring Simple Receptive Fields Orientation Columns Visual Cortex Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Ostojic Brunel N Spiking Neuron Models LinearNonlinear Models PLOS Computational Biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Weber AI Pillow JW Capturing Dynamical Repertoire Single Neurons Generalized Linear Models Neural Computation doi necoa PubMed CrossRef Google Scholar Fairhall receptive field dead Long live receptive field Current Opinion Neurobiology ixxii doi jconb PMC free article PubMed CrossRef Google Scholar Barth FG Mechanics preprocess information fine tuning mechanoreceptors Journal Comparative Physiology doi sz PMC free article PubMed CrossRef Google Scholar Eldar YC Kutyniok G editors Compressed Sensing Theory Applications Cambridge University Press Google Scholar Foucart Rauhut H Mathematical Introduction Compressive Sensing Birkhuser Basel Ganguli Sompolinsky H Compressed Sensing Sparsity Dimensionality Neuronal Information Processing Data Analysis Annual Review Neuroscience doi annurevneuro PubMed CrossRef Google Scholar Fallah K Willats AA Liu N Rozell CJ Learning sparse codes compressed representations biologically plausible local wiring constraints bioRxiv Google Scholar Barranca VJ Kovai G Zhou Cai Improved Compressive Sensing Natural Scenes Using Localized Random Sampling Scientific Reports doi srep PMC free article PubMed CrossRef Google Scholar ShaweTaylor J Cristianini N Kernel Methods Pattern Analysis Cambridge University Press Google Scholar ShalevShwartz BenDavid Understanding Machine Learning Theory Algorithms Cambridge University Press Google Scholar Yamins DLK Hong H Cadieu CF Solomon EA Seibert DiCarlo JJ Performanceoptimized hierarchical models predict neural responses higher visual cortex Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar K Zhang X Ren Sun J Deep Residual Learning Image Recognition arXiv cs Sinha Duchi JC Learning Kernels Random Features Lee Sugiyama Luxburg U Guyon Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileedcbbafbacbePaperpdf Google Scholar Arpit Campos V Bengio Initialize Network Robust Initialization WeightNorm ResNets Wallach H Larochelle H Beygelzimer dAlchBuc F Fox E Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileefaccdaPaperpdf Google Scholar Glorot X Bengio Understanding difficulty training deep feedforward neural networks Proceedings Thirteenth International Conference Artificial Intelligence Statistics JMLR Workshop Conference Proceedings p Available httpproceedingsmlrpressvglorotahtml Olah C Mordvintsev Schubert L Feature Visualization Distill Lindsay GW Convolutional Neural Networks Model Visual System Past Present Future Journal Cognitive Neuroscience p PubMed Google Scholar Dapello J Marques Schrimpf Geiger F Cox DiCarlo JJ Simulating Primary Visual Cortex Front CNNs Improves Robustness Image Perturbations Advances Neural Information Processing Systems Google Scholar Illing B Gerstner W Brea J Biologically plausible deep learningBut far go shallow networks Neural Networks doi jneunet PubMed CrossRef Google Scholar Mallat Group Invariant Scattering Communications Pure Applied Mathematics doi cpa CrossRef Google Scholar Bruna J Mallat Invariant Scattering Convolution Networks IEEE Transactions Pattern Analysis Machine Intelligence doi TPAMI PubMed CrossRef Google Scholar Andn J Mallat Deep Scattering Spectrum IEEE Transactions Signal Processing doi TSP CrossRef Google Scholar Pintea SL Tomen N Goes SF Loog van Gemert JC Resolution learning deep convolutional networks using scalespace theory arXiv cs PubMed Goltstein PM Meijer GT Pennartz CM Conditioning sharpens spatial representation rewarded stimuli mouse primary visual cortex eLife e doi eLife PMC free article PubMed CrossRef Google Scholar Fritz J Shamma Elhilali Klein Rapid taskrelated plasticity spectrotemporal receptive fields primary auditory cortex Nature Neuroscience doi nn PubMed CrossRef Google Scholar Beniaguev Segev London Single cortical neurons deep artificial neural networks Neuron e doi jneuron PubMed CrossRef Google Scholar Jacot Gabriel F Hongler C Neural Tangent Kernel Convergence Generalization Neural Networks Advances Neural Information Processing Systems vol Curran Associates Inc Available httpspapersnipsccpaperhashabefaebbaecbdfaAbstracthtml Google Scholar Mattos CLC Dai Z Damianou Forth J Barreto GA Lawrence ND Recurrent Gaussian Processes arXiv cs stat Ringach DL Spatial structure symmetry simplecell receptive fields macaque primary visual cortex Journal Neurophysiology Jul doi jn PubMed CrossRef Google Scholar PLoS Comput Biol Oct e Decision Letter Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Mar Dear Mr Pandey Thank much submitting manuscript Structured random receptive fields enable informative sensory encodings consideration PLOS Computational Biology papers reviewed journal manuscript reviewed members editorial board several independent reviewers light reviews email would like invite resubmission significantlyrevised version takes account reviewers comments make decision publication seen revised manuscript response reviewers comments revised manuscript also likely sent reviewers evaluation ready resubmit please upload following letter containing detailed list responses review comments description changes made manuscript Please note forming response article accepted may opportunity make peer review history publicly available record include editor decision letters reviews responses reviewer comments eligible contact opt Two versions revised manuscript one either highlights tracked changes denoting text changed clean version uploaded manuscript file Important additional instructions given reviewer comments Please prepare submit revised manuscript within days anticipate delay please let us know expected resubmission date replying email Please note revised manuscripts received day due date may require evaluation peer review similar newly submitted manuscripts Thank submission hope editorial process constructive far welcome feedback time Please dont hesitate contact us questions comments Sincerely Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer authors make two main contributions First modeling receptive fields independent draws Gaussian process Second arguing structured receptive fields beneficial efficient learning downstream authors prove theorem provides interpretation random receptive fields performing filtering eigenbasis Gaussian process inductive bias argued reason downstream predictors performance may enhanced receptive fields compatible learning problem contributions potentially important neither two ideas sufficiently motivated developed Therefore paper feels light content suggestions improvement need modeling receptive fields random draws motivated found strange introduction paper reviews wide range literature including even technical references reproducing kernel Hilbert spaces whose relevance questionable main motivation work short sentence referring data ref went paper see motivates authors approach advantage modeling receptive fields random draws authors provide evidence models capture second order statistics experimental receptive fields clear advantage modeling approach compared say fitting LN model every neuron authors state performance metrics make comparisons relevant techniques Authors mention RKHSs times even theoretical discussion Appendix However clear connection important relevant Reading paragraph equation sorry line numbers provided discussion section feels like authors want claim learning efficiency results related learning RKHSs would nice see point fully developed Instead MNIST authors could use dataset naturalistic complex stimuli theoretical study makes receptive field covariance compatible incompatible downstream learning pursued may related RKHS picture authors hinting authors great list possible future directions One directions could pursued paper Black line Figure visible Reviewer submitted article authors present novel framework modeling receptive fields sensory cortices introducing concept structured receptive field drawn biased probability distribution model sensory neurons interpretable fashion still incorporating randomness crucial biological systems article broadly split two parts First structured receptive fields used fit experimental data biological receptive fields suitability machine learning applications discussed results interesting novel described points respects analysis explication results could improved Major points results matching biological receptive fields based qualitativenot quantitativecomparisons authors compare covariance matrices experimental data fitted model based similar look plot One way results might strengthened including null hypothesis like purely random receptive fields receptive fields based Gabor filters V authors nice job motivating theory theory random feature networks machine learning literature would also liked see authors discuss detail ideas context previous computational neuroscience approaches describing receptive fields including efficient coding sparse coding related approaches establishing structured weights lead faster learning Section convincing way show view would define metric speed learning eg area learning curve ii optimize learning rate separately network respect metric iii compare metric two networks separately optimized learning rates way simulations currently done leaves open possibility slower learning unstructured case might due suboptimal choice learning rate Minor points section authors explain L regularization parameter set possible presented results would change parameter set differently first paragraph Section claim made without specifying particular task adding inductive bias improves learning authors careful statements like elsewhere paper since obviously aware given illustrate point simulations inductive bias expected improve learning well matched particular computational task actually worsen learning case Section authors state GP covariance function reflects statistical regularities within sensory inputs network authors provide citation statement cases considered secondtolast paragraph section bandwidth fhi flo equation section stochastic coefficients cosomegak sinomegak independent end section authors precisely explain statement smoothness also controlled overall magnitude nonzero eigenvalues describing computation Cdata Section explanation meant centered receptive fields could helpful Section unit pixels seems incorrect since squared pixel generally meant pixel explanation mechanism leads prominent dark patches covariance matrices shown Figure would appreciated result Figures E extent Figure authors mean remarkably well might addressed response major point Figure E authors comment faster decay spectrum model Could due noise effect related calcium imaging text describing Figure stated Cdata shown zoomed nothing mentioned caption mention scale given one look supplemental figure get still imprecise idea whats going Given dark patches still clearly visible full covariance plotted supplemental figure would recommend either showing full version instead Figure describing zoomedin version clearly plots machine learning applications authors add green curves fact show higher test errors blue ones tautological since parameters latter optimized precisely minimize test error may value curves point illustrate poorly chosen inductive bias worse inductive bias doesnt seem borne pairs green vs orange curves figures doesnt seem point made text last paragraph appendix authors claim functions small Hnorm easier learn larger norm statement made precise perhaps specific regression algorithm Reviewer paper authors considered receptive fields random samples parametrized distributions gaussian process particular following modalities insect mechanosensors mammalian visual cortex neurons demonstrated random feature neurons RFN remove high frequency noise boost signals Finally also show RFNs enable efficient learning number neurons training time perspective topic general interest visual neuroscience growing subfield within computational neuroscience intersecting artificial neural networks comments questions motivations implications work attached theoretical analysis equivalence structured weight vector transformation filtering Gaussian Processing eigenbasis random projection onto spherical random gaussian interesting Aside connection kernel theory learning broader implications finding either sensory neuroscience deep learning fields would helpful get discussion around topic revised version motivation using Gaussian Process type generative model mathematical andor neuro perspectives possible sampling random filters Gaussian Process mathematically tractable also biologically plausible compared say learning filters taskdriven optimization would helpful get discussion around revised version surprising filter samples GPs covariance real data match various properties real neural populations might helpful control models fail capture biological fidelity example Fig revised version Minor comment name structured random receptive field properly defined paper bit vague initially thought random implies stochastic Dapello et al Overall think paper interesting recommend acceptance revisions authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes Reviewer None PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer Reviewer Figure Files revising submission please upload figure files Preflight Analysis Conversion Engine PACE digital diagnostic tool httpspacevapexcovantagecom PACE helps ensure figures meet PLOS requirements use PACE must first register user login navigate UPLOAD tab find detailed instructions use tool encounter issues questions using PACE please email us grosolpserugif Data Requirements Please note condition publication PLOS data policy requires make available data used draw conclusions outlined manuscript Data must deposited appropriate repository included within body manuscript uploaded supporting information includes numerical values used generate graphs histograms etc example PLOS Biology see httpwwwplosbiologyorgarticleinfoAdoiFFjournalpbios Reproducibility enhance reproducibility results recommend deposit laboratory protocols protocolsio protocol assigned identifier DOI cited independently future Additionally PLOS ONE offers option publish peerreviewed clinical study protocols Read information sharing protocols httpsplosorgprotocolsutmmediumeditorialemailutmsourceauthorlettersutmcampaignprotocols PLoS Comput Biol Oct e Author response Decision Letter Oct e Published online Oct doi journalpcbir Author response Decision Letter Copyright License information Disclaimer Copyright notice Jun Attachment Submitted filename coverletterresubmissionpdf Click additional data file pdf PLoS Comput Biol Oct e Decision Letter Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Aug Dear Dr Harris pleased inform manuscript Structured random receptive fields enable informative sensory encodings provisionally accepted publication PLOS Computational Biology manuscript formally accepted need complete formatting changes receive follow email member team touch set requests Please note manuscript scheduled publication made required changes swift response appreciated IMPORTANT editorial review process complete PLOS permit corrections spelling formatting significant scientific errors point onwards Requests major changes affect scientific understanding work cause delays publication date manuscript institutions press office journal office choose press release paper automatically opted early publication ask notify us institution planning press release article press must coordinated PLOS Thank supporting Open Access publishing looking forward publishing work PLOS Computational Biology Best regards Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer thank authors response appreciate effort put revision Although argued many asked beyond scope paper disagree agree necessary address concerns Clarification new additions paper including Appendix adequate dont comments suggestions recommend acceptance reading paper rebuttal think negative recommending rejection first review apologize think fair assessment would major revision Reviewer authors done excellent job responding earlier comments pleased recommend manuscript accepted publication especially appreciated new appendix section greatly strengthens mathematical foundations work Two minor comments follow things noticed reading manuscript might help improve paper final publication bar plot part Fig illustrating results three models described line perhaps also including sketch receptive fields two null models look like would helpful way visually summarize result similar visualization could helpful Fig Perhaps missed Figures exactly percentages reported panel calculated Comparing orange curves green curves doesnt appear numbers correspond curves appear next straightforward way could discern authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer PLoS Comput Biol Oct e Acceptance letter Oct e Published online Oct doi journalpcbir Acceptance letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Oct PCOMPBIOLDR Structured random receptive fields enable informative sensory encodings Dear Dr Harris pleased inform manuscript formally accepted publication PLOS Computational Biology manuscript production department notified publication date due course corresponding author soon receiving typeset proof review ensure errors introduced production Please review PDF proof manuscript carefully last chance correct errors Please note major changes affect scientific understanding work likely cause delays publication date manuscript Soon final files uploaded unless opted early version manuscript published online date early version articles publication date final article published URL versions paper accessible readers Thank supporting PLOS Computational Biology openaccess publishing looking forward publishing work kind regards Anita Estes PLOS Computational Biology Carlyle House Carlyle Road Cambridge CB DN United Kingdom grosolploibpmocsolp Phone ploscompbiolorg PLOSCompBiol Articles PLOS Computational Biology provided courtesy PLOSPLoS Comput Biol Oct e Published online Oct doi journalpcbi PMCID PMC PMID Structured random receptive fields enable informative sensory encodings Biraj Pandey Formal analysis Investigation Software Validation Visualization Writing original draft Writing review editing Marius Pachitariu Data curation Writing review editing Bingni W Brunton Formal analysis Resources Supervision Visualization Writing review editing Kameron Decker Harris Conceptualization Formal analysis Investigation Supervision Visualization Writing original draft Writing review editing Biraj Pandey Department Applied Mathematics University Washington Seattle Washington United States America Find articles Biraj Pandey Marius Pachitariu Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Find articles Marius Pachitariu Bingni W Brunton Department Biology University Washington Seattle Washington United States America Find articles Bingni W Brunton Kameron Decker Harris Department Computer Science Western Washington University Bellingham Washington United States America Find articles Kameron Decker Harris Xuexin Wei Editor Author information Article notes Copyright License information Disclaimer Department Applied Mathematics University Washington Seattle Washington United States America Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Department Biology University Washington Seattle Washington United States America Department Computer Science Western Washington University Bellingham Washington United States America University Texas Austin UNITED STATES Corresponding author authors declared competing interests exist Email udeuwwsirrahnoremak Received Oct Accepted Aug Copyright Pandey et al open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Associated Data Supplementary Materials Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF pcbispdf GUID EEABECFA Attachment Submitted filename coverletterresubmissionpdf pcbispdf GUID FDDFCFDCBEDFB Data Availability Statement relevant data within manuscript Supporting information files Abstract Brains must represent outside world animals survive thrive early sensory systems neural populations diverse receptive fields structured detect important features inputs yet significant variability ignored classical models sensory neurons model neuronal receptive fields random variable samples parameterized distributions demonstrate model two sensory modalities using data insect mechanosensors mammalian primary visual cortex approach leads significant theoretical connection foundational concepts receptive fields random features leading theory understanding artificial neural networks modeled neurons perform randomized wavelet transform inputs removes high frequency noise boosts signal random feature neurons enable learning fewer training samples smaller networks artificial tasks structured random model receptive fields provides unifying mathematically tractable framework understand sensory encodings across spatial temporal domains Author summary Evolution ensured animal brains dedicated extracting useful information raw sensory stimuli discarding everything else Models sensory neurons key part theories brain represents world work model tuning properties sensory neurons way incorporates randomness builds bridge leading mathematical theory understanding artificial neural networks learn models capture important properties large populations real neurons presented varying stimuli Moreover give precise mathematical formula sensory neurons two distinct areas one involving gyroscopic organ insects visual processing center mammals transform inputs also find artificial models imbued properties real neurons learn efficiently shorter training time fewer examples mathematical theory explains findings work expands understanding sensory representation large networks benefits neuroscience machine learning communities Introduction long argued brain uses large population neurons represent world view sensory stimuli encoded responses population used downstream areas diverse tasks including learning decisionmaking movement control sensory areas different neurons responding differing stimuli also providing measure redundancy However still lack clear understanding response properties wellsuited different sensory modalities One way approach sensory encoding understanding neuron would respond arbitrary stimuli Experimentally typically present many stimuli animal measure responses sensory neurons attempt estimate kind model neurons respond arbitrary stimulus common assumption neuron computes linear filter stimulus drives spiking nonlinear spikegenerating mechanism Mathematically assumption summarized number measured spikes stimulus x equal w x weight vector w nonlinearity weights w define filtering properties neuron also known receptive field model known linearnonlinear LN model also common form artificial neuron artificial neural networks ANNs LN models used extensively describe firing diverse neurons various sensory modalities vertebrates invertebrates mammalian visual system LN models used characterize retinal ganglion cells lateral geniculate neurons simple cells primary visual cortex V also used characterize auditory sensory neurons avian midbrain somatosensory neurons cortex insects used understand response properties visual interneurons mechanosensory neurons involved proprioception auditory neurons courtship behavior Given stimulus presented neural response data one estimate receptive fields population neurons Simple visual receptive fields classically understood similar wavelets particular spatial frequency angular selectivity mechanosensory areas receptive fields selective temporal frequency short time window Commonly parametric modeling Gabor wavelets smoothing regularization etc used produce clean receptive fields Yet data alone show noisy receptive fields perhaps best modeled using random distribution show modeling receptive fields random samples produces realistic receptive fields reflect structure noisiness seen experimental data importantly perspective creates significant theoretical connections foundational ideas neuroscience artificial intelligence connection helps us understand receptive fields structures structure relates kinds stimuli relevant animal Modeling filtering properties population LN neurons samples random distribution leads study networks random weights machine learning ML networks known random feature networks RFNs study RFNs rapidly gained popularity recent years large part offers theoretically tractable way study learning properties ANNs weights tuned using data RFN contains many neurons approximate functions live wellunderstood function space function space called reproducing kernel Hilbert space RKHS depends network details particular weight ie receptive field distribution Learning framed approximating functions space limited data Several recent works highlight RFN theorys usefulness understanding learning neural systems Bordelon Canatar Pehlevan series papers shown neural codes allow learning examples spectral properties secondorder statistics aligns spectral properties task applied V found neural code aligned tasks depend low spatial frequency components Harris constructed RFN model sparse networks found associative centers like cerebellum insect mushroom body showed areas may behave like additive kernels architecture also considered Hashemi et al classes kernels beneficial learning high dimensions learn fewer examples remain resilient input noise adversarial perturbation Xie et al investigated relationship fraction active neurons model cerebellumcontrolled neuron thresholdsand generalization performance learning movement trajectories vast majority network studies random weights weights w drawn Gaussian distribution independent entries sampling equivalent fully unstructured receptive field looks like white noise Closely related work previous study ANNs showed directly learning structured receptive fields could improve image classification deep networks receptive fields parametrized sum Gaussian derivatives fourth order led better performance rival architectures low data regimes paper study effect structured yet random receptive fields lead informative sensory encodings Specifically consider receptive fields generated Gaussian process GP thought drawing weights w Gaussian distribution particular covariance matrix show networks random weights project input new basis filter particular components theory introduces realistic structure receptive fields random feature models crucial current understanding artificial networks Next show receptive field datasets two disparate sensory systems mechanosensory neurons insect wings V cortical neurons mice monkeys wellmodeled GPs covariance functions wavelet eigenbases Given success modeling data GP apply weight distributions RFNs used synthetic learning tasks find structured weights improve learning reducing number training examples size network needed learn task Thus structured random weights offer realistic generative model receptive fields multiple sensory areas understand performing random change basis change basis enables network represent important properties stimulus demonstrate useful learning Results construct generative model receptive fields sensory neurons use weights ANN refer network structured random feature network first review basics random feature networks details rationale behind generative model process generate hidden weights main theory result networks weights transform inputs new basis filter particular components thus bridging sensory neuroscience theory neural networks Next show neurons two receptive field datasetsinsect mechanosensory neurons mammalian V cortical neuronsare welldescribed generative model close resemblance secondorder statistics sampled receptive fields principal components data model Finally show performance structured random feature networks several synthetic learning tasks hidden weights generative model allows network learn fewer training examples smaller network sizes Theoretical analysis consider receptive fields generated GPs order connect foundational concept sensory neuroscience theory random features artificial neural networks GPs thought samples Gaussian distribution particular covariance matrix initialize hidden weights RFNs using GPs show using GP causes network project input new basis filter particular components basis determined covariance matrix Gaussian useful removing irrelevant noisy components input use results study space functions RFNs containing many neurons learn connecting construction theory kernel methods Random feature networks start introducing main learning algorithm neuronal model work RFN Consider twolayer feedforward ANN Traditionally weights initialized randomly learned backpropagation minimizing loss objective sharp contrast RFNs hidden layer weights sampled randomly distribution fixed hidden unit computes random feature input output layer weights trained Fig Note weights randomly drawn neurons response deterministic function input given weights Open separate window Fig Random feature networks structured weights study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distribution Mathematically hidden layer activations output given h W x h x R stimulus h h h h R hidden neuron responses R predicted output use rectified linear ReLU nonlinearity x max x applied entrywise Eq hidden layer weights W w w w R drawn randomly fixed readout weights trained RFNs RFN experiments train readout weights R offset R using support vector machine SVM classifier squared hinge loss penalty regularization strength tuned range fold crossvalidation RFNs include threshold hidden neurons although could help certain contexts vast majority studies RFNs neurons weights w R initialized iid spherical Gaussian distribution w N call networks built way classical unstructured RFNs Fig propose variation hidden weights initialized w N C C R positive semidefinite covariance matrix call networks structured RFNs Fig mean weights random specified covariance compare unstructured structured weights equal footing normalize covariance matrices Tr C Tr ensures mean square amplitude weights E w Receptive fields modeled linear weights Sensory neurons respond preferentially specific features inputs stimulus selectivity often summarized neurons receptive field describes features sensory space elicits responses stimulated Mathematically receptive fields modeled linear filter stimulus space Linear filters also integral component widely used LN model sensory processing According model firing rate neuron nonlinear function applied projection stimulus onto lowdimensional subspace linear filter linear filter model receptive fields explain responses individual neurons diverse stimuli used describe disparate sensory systems like visual auditory somatosensory systems diverse species including birds mammals insects stimuli uncorrelated filters estimated computing spike triggered average STA average stimulus elicited spike neuron stimuli correlated STA filter whitened inverse stimulus covariance matrix Often STAs denoised fitting parametric function STA Gabor wavelets simple cells V model receptive field neuron weight vector w nonlinear function Instead fitting parametric function construct covariance functions realization resulting Gaussian process resembles biological receptive field Fig Structured weights project filter input covariance eigenbasis generate network weights Gaussian processes GP whose covariance functions inspired receptive fields sensory neurons brain definition GP stochastic process finite observations follow Gaussian distribution find networks weights project inputs new basis filter irrelevant components see adds inductive bias classical RFNs tasks naturalistic inputs improves learning view weight vector w finitedimensional discretization continuous function w sample GP continuous function domain compact subset R assume discretized using grid equally spaced points w w Let input realvalued function x domain could represent finite timeseries image luminance retina complicated spatiotemporal sets like movie continuous setting dimensional inner product w x w x gets replaced L inner product w x w x Every GP fully specified mean covariance function C always assume mean zero study different covariance functions KosambiKarhunenLove theorem realization zeromean GP random series representation w z terms standard Gaussian random variables z N functions weights pairs eigenvalue eigenfunction pairs covariance operator C L L C f C f continuous analog covariance matrix C C positive definite opposed semidefinite eigenfunctions form complete basis L Using Eq inner product stimulus neurons weights w x z x z x z x x x Eq shows structured weights compute projection input x onto eigenfunction x reweight filter eigenvalue taking inner product random Gaussian weights z illuminating see continuous equations look like dimensional discrete setting Samples finitedimensional GP used hidden weights RFNs w N C First GP series representation Eq becomes w z matrices eigenvalues eigenvectors z N Gaussian random vector definition covariance matrix C E w w equal steps linear algebra Finally Eq analogous w x z x Since orthogonal matrix x equivalent change basis diagonal matrix shrinks expands certain directions perform filtering summarized following theorem Theorem Basis change formula Assume w N C C eigenvalue decomposition x R define x x w x z x z N Theorem says projecting input onto structured weight vector first filtering input GP eigenbasis random projection onto spherical random Gaussian form GP eigenbasis determined choice covariance function covariance function compatible input structure hidden weights filter irrelevant features noise stimuli amplifying descriptive features inductive bias facilitates inference stimuli downstream predictor spherical Gaussian distribution canonical choice unstructured RFNs simple way evaluate effective kernel structured RFNs k struct x x k unstruct x x see Appendix expression structured kernel provides concrete connection kernel theory learning using nonlinear neural networks readers interested kernel theories full example simulation results work given Appendix show exponential reduction number samples needed learn frequency detection using structured versus unstructured basis Fig Appendix Examples random yet structured receptive fields goal model weights artificial neurons way inspired biological neurons receptive fields Structured RFNs sample hidden weights GPs structured covariance construct covariance functions make generated weights resemble neuronal receptive fields start toy example stationary GP wellunderstood Fourier eigenbasis show receptive fields generated GP selective frequencies timeseries signals construct locally stationary covariance models insect mechanosensory V neuron receptive fields models shown good match experimental data Warmup Frequency selectivity stationary covariance illustrate results theoretical analysis start toy example temporal receptive fields selective particular frequencies example may familiar readers comfortable Fourier series basic signal processing Let input finite continuous timeseries x interval L use covariance function C k k cos k stationary process k k L k th natural frequency k weight coefficients covariance function Eq stationary means depends difference timepoints Applying compound angle formula get C k k cos k cos k sin k sin k Since sinusoidal functions cos k sin k form orthonormal basis L Eq eigendecomposition covariance eigenfunctions sines cosines eigenvalues k Eq know structured weights covariance form random series w k k z k cos k z k sin k z k z k N Thus receptive fields made sinusoids weighted k Gaussian variables z k z k Suppose want receptive fields retain specific frequency information signal filter rest Take k k k f lo k f hi call bandlimited spectrum passband f lo f hi bandwidth f hi f lo bandwidth increases receptive fields become less smooth since made wider range frequencies k nonzero decay certain rate rate controls smoothness resulting GP receptive fields act input signals x implicitly transform inputs Fourier basis filter frequencies based magnitude k bandlimited setting frequencies outside passband filtered makes receptive fields selective particular range frequencies ignore others hand classical random features weight frequencies equally even though natural settings high frequency signals corrupted noise Insect mechanosensors next consider particular biological sensor sensitive timehistory forces Campaniform sensilla CS domeshaped mechanoreceptors detect local stress strain insect exoskeleton embedded cuticle deformation cuticle bending torsion induces depolarizing currents CS opening mechanosensitive ion channels CS encode proprioceptive information useful body state estimation movement control diverse tasks like walking kicking jumping flying model receptive fields CS believed critical flight control namely ones found base halteres wings Fig Halteres wings flap rhythmically flight rotations insects body induce torsional forces felt active sensory structures CS detect small strain forces thereby encoding angular velocity insect body Experimental results show haltere wing CS selective broad range oscillatory frequencies STAs smooth oscillatory selective frequency decay time Fig B Open separate window Fig Random receptive field model insect mechanosensors Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shown model temporal receptive fields locally stationary GP bandlimited spectrum Examples receptive fields generated GP shown Fig C inputs CS modeled finite continuous timeseries x finite interval L neuron weights generated covariance function C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum k k L k th natural frequency warmup frequency selectivity weights accounted parameters f lo f hi bandwidth f hi f lo increases receptive fields built wider selection frequencies makes receptive fields less smooth Fig field localized near decay determined parameter increases receptive field selective larger time windows eigenbasis covariance function Eq similar Fourier eigenbasis modulated decaying exponential eigenbasis orthonormal basis span k e cos k k e sin k nonorthogonal set functions L hidden weights transform timeseries inputs eigenbasis discard components outside passband frequencies f lo f hi fit covariance model receptive field data CS neurons wings hawkmoth Manduca sexta data Briefly CS receptive fields estimated spiketriggered average STA experimental mechanical stimuli wings stimuli generated bandpassed white noise Hz characterize receptive fields population CS neurons compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field number samples case kHz ms samples normalization sets overall scale covariance matrix data covariance matrix shows tridiagonal structure Fig main diagonal positive diagonals negative diagonals decay away top left matrix Open separate window Fig Spectral properties mechanosensory RFs model similar compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behavior fit covariance model data optimize parameters f lo f hi finding f lo Hz f hi Hz ms best fit sensilla data minimizing Frobenius norm difference C data model see Appendix resulting model covariance matrix Fig B matches data covariance matrix Fig remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig B Appendix simulate effect finite number neurons generate weight vectors equal number neurons recorded recompute model covariance matrix Fig C call finite neuron model covariance matrix C finite shows bump bloblike structures evident C data C model result suggests bumpy structures attributed small number recorded neurons hypothesize effects would disappear larger dataset C data would closely resemble C model comparison also calculate Frobenius difference null models unstructured covariance model Fourier model unstructured model Frobenius norm difference Fourier model sensilla covariance model much lower difference compared null models fitting data accurately show covariance matrices sampled receptive fields null models Fig C E Appendix Comparing eigenvectors eigenvalues data model covariance matrices find spectral properties C model C finite similar C data eigenvalue curves models match data quite well Fig E curves directly comparable covariance normalized trace makes sum eigenvalues unity data model covariance matrices lowdimensional first data eigenvectors explain variance top explain top eigenvectors model finite sample match data quite well Fig Primary visual cortex turn visually driven neurons mammalian primary cortex Primary visual cortex V earliest cortical area processing visual information Fig neurons V detect small changes visual features like orientations spatial frequencies contrast size Open separate window Fig Random receptive field model Primary Visual Cortex V Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fields model receptive fields simple cells V clear excitatory inhibitory regions light shone excitatory regions increase cells response viceversa Fig B shape regions determines orientation selectivity widths determine frequency selectivity receptive fields centered location visual field decay away integrate visual stimuli within small region center Gabor functions widely used mathematical model receptive fields simple cells model receptive fields using another locally stationary GP show examples generated receptive fields Fig C Consider inputs cortical cells continuous twodimensional image x domain L L x R Since image realvalued x grayscale contrast single color channel pixel values neuron weights generated covariance function following form C exp f smooth receptive fields exp c c localized center c receptive field center defined c size receptive field determined parameter increases receptive field extends farther center c Fig Spatial frequency selectivity accounted bandwidth parameter f f decreases spatial frequency receptive field goes making weights less smooth Fig E eigendecomposition covariance function Eq leads orthonormal basis single scale Hermite wavelets c wavelet eigenfunctions Hermite polynomials modulated decaying Gaussian k e c H k c k c k H k k th physicists Hermite polynomial eigenfunctions nonzero centers c shifted versions Eq detailed derivation values constants c c c normalization Appendix use Eq model receptive field data V neurons recorded calcium imaging transgenic mice expressing GCaMPs mice headfixed running airfloating ball presented unique white noise images pixels using Psychtoolbox pixels white black equal probability Images upsampled resolution screens via bilinear interpolation stimulus corrected eyemovements online using custom code responses cells collected using twophoton mesoscope preprocessed using Suitep Receptive fields calculated white noise images deconvolved calcium responses cells using STA covariance analysis picked cells signaltonoise SNR threshold gave us cells SNR calculated smaller set images presented twice using method preprocessing step moved center mass every receptive field center visual field compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field case pixels pixels data covariance matrix resembles tridiagonal matrix However diagonals nonzero equally spaced segments Additionally values decay away center matrix show C data zoomed nonzero region around center matrix Fig corresponds pixel region around center full pixel matrix full covariance matrix shown Fig F Appendix Open separate window Fig Spectral properties V RFs model similar compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons red covariance model number offdiagonals center rate decay away center determined parameters f c respectively covariance pixels decays function distance c leads equallyspaced nonzero segments hand covariance also decays function distance pixels brings diagonal structure model frequency parameter f increases number offdiagonals increases Pixels generated weights become correlated weights become spatially smoother size parameter increases diagonals decay slower center c increasing correlations center pixel leading significant weights occupy visual field optimize parameters fit data finding f pixels minimizing Frobenius norm difference C data model need optimize center parameter c since preprocess data receptive fields centered c center grid resulting model covariance matrix Fig B data covariance matrix Fig match remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig G Appendix simulate effect finite number neurons generate weights equal number neurons data compute C finite shown Fig C finite matrix C finite looks even like C data shows negative covariances far center result finite sample size comparison also calculate normalized Frobenius difference null models unstructured covariance model translation invariant V model translation invariant model remove spatially localizing exponential Eq fit spatial frequency parameter f unstructured model Frobenius norm difference translation invariant model V covariance model much lower difference better fit data show covariance matrices sampled receptive fields null models Fig H J Appendix Similar spectral properties evident eigenvectors eigenvalues C model C finite C data analytical forms derived Eq Fig E covariances normalized unit trace Note analytical eigenfunctions shown finer grid model data analysis performed continuous space differences eigenfunctions eigenvalues analytical model results due discretization Examining eigenvectors Fig also see good match although rotations differences ordering eigenvectors explain variance receptive field data reference top eigenvectors explain variance data variance model eigenvalue curves models analytical forms match data Fig E reasonably well although well mechanosensors Appendix repeat analysis receptive fields measured different stimulus sets mouse different experimental dataset nonhuman primate V findings consistent results shown Fig K P Appendix Advantages structured random weights artificial learning tasks hypothesis neuronal inductive bias structured receptive fields allows networks learn fewer neurons training examples steps gradient descent classification tasks naturalistic inputs examine hypothesis compare performance structured receptive fields classical ones several classification tasks find artificial learning tasks structured random networks learn accurately smaller network sizes fewer training examples gradient steps Frequency detection CS naturally encode timehistory strain forces acting insect body sensors inspired temporal filtering properties shown accurately classify spatiotemporal data Inspired result test sensillainspired mechanosensory receptive fields timeseries classification task Fig top example presented network ms timeseries sampled kHz goal detect whether example contains sinusoidal signal positive examples sinusoidal signals f Hz corrupted noise SNR dB negative examples Gaussian white noise matched amplitude positive examples Note frequency detection task linearly separable random phases positive negative examples See Appendix additional details including definition SNR crossvalidation used find optimal parameters f lo Hz f hi Hz ms Open separate window Fig Random mechanosensory weights enable learning fewer neurons timeseries classification tasks show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible task number hidden neurons structured RFN significantly outperforms classical RFN show test performance using tuned parameters Fig Even noisy task achieves test error using hidden neurons Meanwhile classical network takes neurons achieve similar error Predictably performance suffers weights incompatible task show results f lo Hz f hi Hz Fig incompatible RFN performs better chance error much worse classical RFN takes neurons achieve test error test error decrease level even additional hidden neurons Frequency XOR task challenge mechanosensorinspired networks difficult task build frequency ExclusiveOR XOR problem Fig B top XOR binary function returns true inputs different otherwise returns false XOR classical example function linearly separable thus harder learn inputs ms timeseries sampled kHz inputs either contain pure frequency f Hz f Hz mixed frequency signals f f white noise pure mixed frequency cases add noise SNR See Appendix details goal task output true input contains either pure tone false input contains mixed frequencies white noise tune GP covariance parameters f lo f hi Eq using crossvalidation cross validation procedure algorithmic details identical frequency detection task Using cross validation find optimal parameters f lo Hz f hi Hz ms incompatible weights take f lo Hz f hi Hz structured RFN significantly outperform classical RFN number hidden neurons show network performance using parameters Fig B Classification error achieved hidden neurons sharp contrast classical RFN requires hidden neurons achieve error incompatible weights network needs neurons achieve test error improve larger network sizes four input subclasses consistently fails classify pure Hz sinusoidal signals outside passband Image classification next test Vinspired receptive fields two standard digit classification tasks MNIST KMNIST MNIST KMNIST datasets contain images handwritten digits MNIST Arabic numerals whereas KMNIST Japanese hiragana phonetic characters datasets come split training test examples classes training examples per class Every example grayscale image centered characters hidden weight center c chosen uniformly random pixels ensures networks weights uniformly cover image space fact means network represent sum locallysmooth functions see Appendix use network hidden neurons tune GP covariance parameters f Eq using fold cross validation MNIST training set parameter ranges pixels optimal parameters found grid search find optimal parameters pixels f pixels refit optimal model using entire training set parameters MNIST used KMNIST task without additional tuning Vinspired achieves much lower average classification error compared classical RFN number hidden neurons show learning performance using parameters MNIST task Fig achieve error MNIST task requires neurons versus neurons classical RFN structured RFN achieves error neurons Qualitatively similar results hold KMNIST task Fig B although overall errors larger reflecting harder task achieve error KMNIST requires neurons versus neurons classical RFN structured RFN achieves error neurons Open separate window Fig Random V weights enable learning fewer neurons fewer examples digit classification tasks show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properly network performance suffers GP covariance parameters match task happens size parameter smaller stroke width spatial scale f doesnt match stroke variations character Taking incompatible parameters f Fig B structured weights performs worse classical RFN tasks hidden neurons achieves relatively poor test errors MNIST Fig KMNIST Fig B Structured weights improve generalization limited data Alongside learning fewer hidden neurons V structured RFNs also learn accurately fewer examples test fewshot learning using image classification datasets training examples reduced training examples per class test set GP parameters remain Structured encodings allow learning fewer samples unstructured encodings show fewshot learning results Fig C networks performance saturate past hundred hidden neurons MNIST lowest error achieved V structured RFN versus classical RFN using incompatible weights Fig C structured network acheives error using structured features KMNIST task opposed classical RFN using incompatible weights Fig Networks train faster initialized structured weights study effect structured weights initialization strategy fullytrained neural networks weights network vary hypothesized structured initialization allows networks learn faster ie training loss test error would decrease faster unstructured weights shown performance RFNs improves biologically inspired weight sampling However RFNs Eq readout weights modified training hidden weights W frozen initial value compare biologicallymotivated initialization classical initialization variance inversely proportional number hidden neurons w unstruct N initialization widely known Kaiming normal scheme thought stabilize training dynamics controlling magnitude gradients classical approach ensures Tr fair comparison scale structured weight covariance matrix Tr C studies RFNs trace equal weight scale absorbed readout weights due homogeneity ReLU compare structured unstructured weights MNIST KMNIST tasks common benchmarks fullytrained networks architecture single hidden layer feedforward neural network Fig hidden neurons crossentropy loss training sets minimized using simple gradient descent GD epochs fair comparison learning rate optimized network separately define area training loss curve metric speed learning perform grid search range e e learning rate minimizes metric resulting parameters structured unstructured incompatible networks respectively parameters image classification MNIST KMNIST tasks Vinitialized network minimizes loss function faster classically initialized network MNIST task V network achieves loss value epochs compared network Fig see qualitatively similar results KMNIST task end training Vinspired networks loss classically initialized network reaches Fig B find Vinitialized network performs better classical initialization covariance parameters match task incompatible parameters Vinitialized network achieves loss value MNIST KMNIST Open separate window Fig V weight initialization fullytrained networks enables faster training digit classification tasks show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible task minimize training loss faster Vinitialized network also generalizes well achieves lower test error end training MNIST achieves test error compared error classically initialized network using incompatible weights Fig C KMNIST see error compared error classical initalization using incompatible weights Fig see similar results across diverse hidden layer widths learning rates Fig Q Appendix benefits evident wider networks smaller learning rates Furthermore structured weights show similar results trained epochs rate neurons shown optimizers like minibatch Stochastic Gradient Descent SGD ADAM batch size rate neurons shown Structured initialization facilitates learning across wide range networks However improvement universal significant benefit found initializing early convolutional layers deep network AlexNet applying ImageNet dataset shown Appendix Fig U Appendix large amounts training data fact small fraction network initialized structured weights could explain null result Also many scenarios incompatible structured weights get performance par compatible ones end training poor inductive bias overcome Improving representation structured random weights shown structured receptive field weights improve performance RFNs fullytrained networks number supervised learning tasks long receptive fields compatible task performance gains unstructured features possible incompatible networks performs better even worse using classical unstructured weights results understood theoretical framework Structured weights effectively cause input x undergo linear transformation new representation x following Theorem examples new representation bandlimited due design covariance function V weights eigenvalues nonzero spectrum decays exponentially acts lowpass filter moving bandlimited representation filter noisehighfrequency componentsand reduce dimensionalitycoordinates x outside passband zero general noise dimensionality make learning harder easiest understand effects frequency detection task simplicity assume using stationary features warmup frequency detection task signal power contained f Hz frequency everything else due noise weights compatible task means w sum sines cosines frequencies k passband includes f narrower make bandwidth still retaining signal higher SNR x becomes since noise filtered see Appendix Discussion paper describe random generative model receptive fields sensory neurons Specifically model receptive field random filter sampled Gaussian process GP covariance structure matched statistics experimental neural data show two kinds sensory neuronsinsect mechanosensory simple cells mammalian Vhave receptive fields welldescribed GPs particular generated receptive fields secondorder statistics principal components match receptive field data Theoretically show individual neurons perform randomized transformation filtering inputs connection provides framework sensory neurons compute input transformations like Fourier wavelet transforms biologically plausible way numerical results using structured random receptive fields show offer better learning performance unstructured receptive fields several benchmarks structured networks achieve higher test performance fewer neurons fewer training examples unless frequency content receptive fields incompatible task networks fully trained initializing structured weights leads better network performance measured training loss generalization fewer iterations gradient descent Structured random features may understood theoretically transforming inputs informative basis retains important information stimulus filtering away irrelevant signals Modeling sensory neurons modalities random feature formulation natural extension traditional linearnonlinear LN neuron model approach may applied brain regions LN models successful instance sensory areas primarily feedforward connectivity like somatosensory auditory regions neurons auditory somatosensory systems selective spatial temporal structures stimuli spatial structure emerges networks trained artificial tactile tasks receptive fields could modeled GPs spatiotemporal covariance functions could useful artificial tasks spatiotemporal stimuli movies multivariate timeseries Neurons localized random temporal responses found compatible manifold coding decisionmaking task GPs complementary approach traditional sparse coding efficient coding hypotheses connections theories interesting future research Receptive fields development generative model offers new directions explore biological basis computational principles behind receptive fields Development lays basic architecture conserved animal animal yet details every neural connection specified leading amount inevitable randomness least initially receptive fields random constrained covariance natural ask biology implements Unsupervised Hebbian dynamics local inhibition allow networks learn principal components input interesting future direction similar learning rules may give rise overcomplete nonorthogonal structure similar studied may prove biologically plausible weights result taskdriven optimization assumes receptive field properties actually lie within synaptic weights spatial receptive fields assumption plausible temporal properties receptive fields likely result neurons intrinsic dynamics LN framework model Heterogeneous physiological eg resonator dynamics mechanical position shape mechanosensor relative body structure properties combine give diverse temporal receptive field structures Development thus leverages different mechanisms build structure receptive field properties sensory neurons Connections compressive sensing Random projections seen extensive use field compressive sensing highdimensional signal found measurements long sparse representation Random compression matrices known optimal properties however many cases structured randomness realistic Recent work shown structured random projections local wiring constraints one dimension compatible dictionary learning supporting previous empirical results work shows structured random receptive fields equivalent employing wavelet dictionary dense Gaussian projection Machine learning inductive bias important open question neuroscience machine learning certain networks characterized features architecture weights nonlinearities better others certain problems One perspective network good problem biased towards approximating functions close target known inductive bias depends alignment features encoded neurons task hand approach shows structured receptive fields equivalent linear transformation input build biases Furthermore describe nonlinear properties network using kernel varies depending receptive field structure target function small norm RKHS inductive bias easier learn small norm RKHS means target function varies smoothly inputs Smooth functions easier learn compared fast varying ones way receptive field structure influences ease learning target function conjecture receptive fields neuralinspired distributions affect RKHS geometry target functions norm small RKHS compared RKHS random whitenoise receptive fields leave future work verify conjecture detail Networks endowed principles neural computation like batch normalization pooling inputs residual connections found contain inductive biases certain learning problems Learning datadependent kernels another way add inductive bias also saw initializing fullytrained networks generative models improved speed convergence generalization compared unstructured initialization result consistent known results initialization effect generalization initialization literature mostly focused avoiding explodingvanishing gradients conjecture inductive bias structured connectivity places network closer good solution loss landscape random Vinspired receptive fields model seen similar happens convolutional neural network CNN similarities differences compared brains recent study showed CNNs fixed Vlike convolutional layer robust adversarial perturbations inputs similar vein work using randomly sampled Gabor receptive fields first layer deep network also shown improve performance wavelet scattering transform multilayer network wavelet coefficients passed nonlinearities model similar deep CNNs framework differs randomized model yields wavelets single scale similar studies robustness learning deep networks weights possible Adding layers model sampling weights variety spatial frequencies field sizes would yield random networks behave similar scattering transform offering another connection brain CNNs Directly learning filters Hermite wavelet basis led good perfomance ANNs little data idea extended multiple scales structured random features seen RFN version ideas supporting evidence principles used biology Limitations future directions several limitations random feature approach model neuron responses scalar firing rates instead discrete spikes ignore complex neuronal dynamics neuromodulatory context many details Like LN models random feature model assumes zero plasticity hidden layer neurons However associative learning drive changes receptive fields individual neurons sensory areas like V auditory cortex RFN purely feedforward account feedback connections Recent work suggests feedforward architecture lacks sufficient computational power serve detailed inputoutput model network cortical neurons might need additional layers convolutional filters difficult interpret parameters found fitting receptive field data connect experimental conditions Also GP model weights captures covariance second moments neglects higherorder statistics remains shown theory yield concrete predictions tested vivo experimental conditions random feature receptive field model randomized extension LN neuron model LN model fits parameterized function receptive field contrast random feature framework fits distribution entire population receptive fields generates realistic receptive fields distribution natural question compare goal capture individual differences neuronal receptive fields one resort LN model neurons receptive field fit data random feature model flexible provides direct connection random feature theory mathematically tractable generative connection kernel learning opens door using techniques mainstay machine learning theory literature instance estimate generalization error sample complexity context learning biologically realistic networks see several future directions structured random features connecting computational neuroscience machine learning already stated auditory somatosensory tactile regions good candidates study well developmental principles could give rise random yet structured receptive field properties account plasticity hidden layer one could also analyze neural tangent kernel NTK associated structured features kernels often used analyze ANNs trained gradient descent number hidden neurons large step size small incorporate lateral feedback connections weights could sampled GPs recurrent covariance functions theory may also help explain CNNs fixed Vlike convolutional layer robust adversarial input perturbations filtering high frequency corruptions seems likely structured random features also robust would interesting study intermediate layer weights fullytrained networks approximate samples GP studying covariance structure Finally one could try develop covariance functions optimize RFNs sophisticated learning tasks see near high performancelower error faster training etcon difficult tasks possible Methods methods described throughout Results section details additional results Appendix Supporting information Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF Click additional data file pdf Acknowledgments thank Dario Ringach providing macaque V data Brandon Pratt hawkmoth mechanosensor data grateful Ali Weber Steven Peterson Owen Levin Alice C Schwarze useful discussions thank Sarah Lindo Michalis Michaelos Carsen Stringer help mouse surgeries calcium imaging data processing respectively Funding Statement BP supported UW Applied Math Frederic Wan Endowed Fellowship Terry Keegan Memorial ARCS Endowment Fellowship Natural Science Foundation Graduate Research Fellowship Program Grant DGE MP supported Janelia Research Campus Howard Hughes Medical Institute BWB supported grants FA FA Air Force Office Scientific Research KDH supported Washington Research Foundation postdoctoral fellowship Western Washington University funders role study design data collection analysis decision publish preparation manuscript httpsamathwashingtonedusupportus httpswwwarcsfoundationorgnationalhomepage httpswwwnsfgrfporg httpswwwjaneliaorglabpachitariulab httpswwwafrlafmilAFOSR httpswwwwrfseattleorggrantswrfpostdoctoralfellowships httpscswwueduharri Data Availability relevant data within manuscript Supporting information files References Yuste R neuron doctrine neural networks Nature Reviews Neuroscience doi nrn PubMed CrossRef Google Scholar Fusi Miller EK Rigotti neurons mix high dimensionality higher cognition Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Saxena Cunningham JP Towards neural population doctrine Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Stringer C Pachitariu Steinmetz N Carandini Harris KD Highdimensional geometry population responses visual cortex Nature doi PMC free article PubMed CrossRef Google Scholar Sherrington C Integrative Action Nervous System Cambridge University Press Google Scholar Chichilnisky EJ simple white noise analysis neuronal light responses Network Computation Neural Systems PubMed Google Scholar Sakai HM Naka K Signal transmission catfish retina V Sensitivity circuit Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Clay Reid R Alonso JM Specificity monosynaptic connections thalamus visual cortex Nature doi PubMed CrossRef Google Scholar Jones JP Palmer LA evaluation twodimensional Gabor filter model simple receptive fields cat striate cortex Journal Neurophysiology PubMed Google Scholar Knudsen EI Konishi Centersurround organization auditory receptive fields owl Science doi science PubMed CrossRef Google Scholar H Central mechanisms tactile shape perception Current opinion neurobiology PubMed Google Scholar Rusanen J Frolov R Weckstrm Kinoshita Arikawa K Nonlinear amplification graded voltage signals firstorder visual interneurons butterfly Papilio xuthus Journal Experimental Biology PubMed Google Scholar Fox JL Fairhall AL Daniel TL Encoding properties haltere neurons enable motion feature detection biological gyroscope Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Pratt B Deora Mohren Daniel Neural evidence supports dual sensorymotor role insect wings Proceedings Royal Society B Biological Sciences doi rspb PMC free article PubMed CrossRef Google Scholar Clemens J Ronacher B Feature Extraction Integration Underlying Perceptual Decision Making Courtship Behavior Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Park Pillow JW Receptive field inference localized priors PLoS computational biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Bonin V Histed MH Yurgenson Reid RC Local Diversity FineScale Organization Receptive Fields Mouse Visual Cortex Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Rosenblatt F perceptron probabilistic model information storage organization brain Psychological Review doi h PubMed CrossRef Google Scholar Caron SJC Ruta V Abbott LF Axel R Random convergence olfactory inputs Drosophila mushroom body Nature doi nature PMC free article PubMed CrossRef Google Scholar LitwinKumar Harris KD Axel R Sompolinsky H Abbott LF Optimal Degrees Synaptic Connectivity Neuron e doi jneuron PMC free article PubMed CrossRef Google Scholar Broomhead DS Lowe Radial basis functions multivariable functional interpolation adaptive networks Royal Signals Radar Establishment Malvern United Kingdom Google Scholar Igelnik B Pao YH Stochastic choice basis functions adaptive function approximation functionallink net IEEE transactions neural networks doi PubMed CrossRef Google Scholar Rahimi Recht B Random Features LargeScale Kernel Machines Platt JC Koller Singer Roweis ST editors Advances Neural Information Processing Systems Curran Associates Inc p Google Scholar Liu F Huang X Chen Suykens JAK Random Features Kernel Approximation Survey Algorithms Theory Beyond arXiv cs stat PubMed Arora Du SS Hu W Li Z Wang R FineGrained Analysis Optimization Generalization Overparameterized TwoLayer Neural Networks arXiv cs stat Arora Du SS Hu W Li Z Salakhutdinov R Wang R Exact Computation Infinitely Wide Neural Net arXiv cs stat Chen L Xu Deep Neural Tangent Kernel Laplace Kernel RKHS arXiv cs math stat Neal RM Priors Infinite Networks New York NY Springer New York p Available CrossRef Google Scholar Williams CKI Computation Infinite Neural Networks Neural Computation doi CrossRef Google Scholar Rahimi Recht B Uniform approximation functions random bases th Annual Allerton Conference Communication Control Computing IEEE p Available httpieeexploreieeeorgdocument Bordelon B Canatar Pehlevan C Spectrum Dependent Learning Curves Kernel Regression Wide Neural Networks arXiv cs stat Bordelon B Pehlevan C Population Codes Enable Learning Examples Shaping Inductive Bias bioRxiv Google Scholar Canatar Bordelon B Pehlevan C Spectral Bias TaskModel Alignment Explain Generalization Kernel Regression Infinitely Wide Neural Networks arXiv condmat stat PMC free article PubMed Harris KD Additive function approximation brain arXiv cs qbio stat Hashemi Schaeffer H Shi R Topcu U Tran G Ward R Generalization Bounds Sparse Random Feature Expansions arXiv cs math stat Xie Muscinelli Decker Harris K LitwinKumar Taskdependent optimal representations cerebellar learning bioRxiv Google Scholar Jacobsen JH van Gemert J Lou Z Smeulders AWM Structured Receptive Fields CNNs arXiv cs Paninski L Convergence properties spiketriggered analysis techniques Network Computation Neural Systems p PubMed Google Scholar Rasmussen CE Williams CKI Gaussian Processes Machine Learning Adaptive Computation Machine Learning MIT Press Google Scholar Kosambi DD Statistics function space Journal Indian Mathematical Society New Series Google Scholar Wahba G Spline Models Observational Data SIAM Dickerson BH Fox JL Sponberg Functional diversity generic encoding insect campaniform sensilla Current Opinion Physiology doi jcophys CrossRef Google Scholar Yarger Fox JL Dipteran Halteres Perspectives Function Integration Unique Sensory Organ Integrative Comparative Biology doi icbicw PubMed CrossRef Google Scholar Fox JL Daniel TL neural basis gyroscopic force measurement halteres Holorusia Journal Comparative Physiology doi sz PubMed CrossRef Google Scholar Genton MG Classes Kernels Machine Learning Statistics Perspective Journal Machine Learning Research Dec Google Scholar Hubel DH Wiesel TN Receptive fields single neurones cats striate cortex Journal Physiology doi jphysiolsp PMC free article PubMed CrossRef Google Scholar Marr Hildreth E Brenner Theory edge detection Proceedings Royal Society London Series B Biological Sciences PubMed Google Scholar Martens JB Hermite transformtheory IEEE Transactions Acoustics Speech Signal Processing doi CrossRef Google Scholar Kleiner Brainard Pelli Whats new Psychtoolbox PerceptionECVP Abstract Supplement European Conference Visual Perception ECVP August Arezzo Italy Sofroniew NJ Flickinger King J Svoboda K large field view twophoton mesoscope subcellular resolution vivo imaging Elife e doi eLife PMC free article PubMed CrossRef Google Scholar Pachitariu Stringer C Dipoppa Schrder Rossi LF Dalgleish H et al Suitep beyond neurons standard twophoton microscopy BioRxiv Google Scholar Mohren TL Daniel TL Brunton SL Brunton BW Neuralinspired sensors enable sparse efficient classification spatiotemporal data Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar LeCun Cortes C Burges C MNIST handwritten digit database ATT Labs Online Available httpyannlecuncomexdbmnist Google Scholar Clanuwat BoberIrizar Kitamoto Lamb Yamamoto K Ha Deep Learning Classical Japanese Literature arXiv cs stat K Zhang X Ren Sun J Delving Deep Rectifiers Surpassing HumanLevel Performance ImageNet Classification IEEE International Conference Computer Vision ICCV p Krizhevsky One weird trick parallelizing convolutional neural networks arXiv cs Russakovsky Deng J Su H Krause J Satheesh et al ImageNet Large Scale Visual Recognition Challenge International Journal Computer Vision doi sy CrossRef Google Scholar Pruszynski JA Johansson RS Edgeorientation processing firstorder tactile neurons Nature Neuroscience doi nn PubMed CrossRef Google Scholar Zhao CW Daley MJ Pruszynski JA Neural network models tactile system develop firstorder units spatially complex receptive fields PLOS ONE e doi journalpone PMC free article PubMed CrossRef Google Scholar Xu Ruan C Korpeoglu E Kumar Achan K Temporal Kernel Approach Deep Learning Continuoustime Information arXiv cs Koay SA Charles Thiberge SY Brody CD Tank DW Sequential efficient neuralpopulation coding complex task information bioRxiv PubMed Google Scholar Olshausen BA Field DJ Sparse Coding Overcomplete Basis Set Strategy Employed V Vision Research doi PubMed CrossRef Google Scholar Barlow HB et al Possible principles underlying transformation sensory messages Sensory communication Google Scholar Chalk Marre Tkaik G Toward Unified Theory Efficient Predictive Sparse Coding Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Swanson LW Brain architecture Understanding basic plan vol xv New York NY US Oxford University Press Google Scholar Strausfeld NJ Arthropod Brains Evolution Functional Elegance Historical Significance Harvard University Press Available httpswwwjstororgstablejctvdpvh Zador critique pure learning artificial neural networks learn animal brains Nature Communications doi PMC free article PubMed CrossRef Google Scholar Oja E Principal components minor components linear neural networks Neural Networks doi CrossRef Google Scholar Pehlevan C Sengupta Chklovskii DB Similarity Matching Objectives Lead HebbianAntiHebbian Networks Neural Computation doi necoa PubMed CrossRef Google Scholar Ringach DL Haphazard Wiring Simple Receptive Fields Orientation Columns Visual Cortex Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Ostojic Brunel N Spiking Neuron Models LinearNonlinear Models PLOS Computational Biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Weber AI Pillow JW Capturing Dynamical Repertoire Single Neurons Generalized Linear Models Neural Computation doi necoa PubMed CrossRef Google Scholar Fairhall receptive field dead Long live receptive field Current Opinion Neurobiology ixxii doi jconb PMC free article PubMed CrossRef Google Scholar Barth FG Mechanics preprocess information fine tuning mechanoreceptors Journal Comparative Physiology doi sz PMC free article PubMed CrossRef Google Scholar Eldar YC Kutyniok G editors Compressed Sensing Theory Applications Cambridge University Press Google Scholar Foucart Rauhut H Mathematical Introduction Compressive Sensing Birkhuser Basel Ganguli Sompolinsky H Compressed Sensing Sparsity Dimensionality Neuronal Information Processing Data Analysis Annual Review Neuroscience doi annurevneuro PubMed CrossRef Google Scholar Fallah K Willats AA Liu N Rozell CJ Learning sparse codes compressed representations biologically plausible local wiring constraints bioRxiv Google Scholar Barranca VJ Kovai G Zhou Cai Improved Compressive Sensing Natural Scenes Using Localized Random Sampling Scientific Reports doi srep PMC free article PubMed CrossRef Google Scholar ShaweTaylor J Cristianini N Kernel Methods Pattern Analysis Cambridge University Press Google Scholar ShalevShwartz BenDavid Understanding Machine Learning Theory Algorithms Cambridge University Press Google Scholar Yamins DLK Hong H Cadieu CF Solomon EA Seibert DiCarlo JJ Performanceoptimized hierarchical models predict neural responses higher visual cortex Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar K Zhang X Ren Sun J Deep Residual Learning Image Recognition arXiv cs Sinha Duchi JC Learning Kernels Random Features Lee Sugiyama Luxburg U Guyon Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileedcbbafbacbePaperpdf Google Scholar Arpit Campos V Bengio Initialize Network Robust Initialization WeightNorm ResNets Wallach H Larochelle H Beygelzimer dAlchBuc F Fox E Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileefaccdaPaperpdf Google Scholar Glorot X Bengio Understanding difficulty training deep feedforward neural networks Proceedings Thirteenth International Conference Artificial Intelligence Statistics JMLR Workshop Conference Proceedings p Available httpproceedingsmlrpressvglorotahtml Olah C Mordvintsev Schubert L Feature Visualization Distill Lindsay GW Convolutional Neural Networks Model Visual System Past Present Future Journal Cognitive Neuroscience p PubMed Google Scholar Dapello J Marques Schrimpf Geiger F Cox DiCarlo JJ Simulating Primary Visual Cortex Front CNNs Improves Robustness Image Perturbations Advances Neural Information Processing Systems Google Scholar Illing B Gerstner W Brea J Biologically plausible deep learningBut far go shallow networks Neural Networks doi jneunet PubMed CrossRef Google Scholar Mallat Group Invariant Scattering Communications Pure Applied Mathematics doi cpa CrossRef Google Scholar Bruna J Mallat Invariant Scattering Convolution Networks IEEE Transactions Pattern Analysis Machine Intelligence doi TPAMI PubMed CrossRef Google Scholar Andn J Mallat Deep Scattering Spectrum IEEE Transactions Signal Processing doi TSP CrossRef Google Scholar Pintea SL Tomen N Goes SF Loog van Gemert JC Resolution learning deep convolutional networks using scalespace theory arXiv cs PubMed Goltstein PM Meijer GT Pennartz CM Conditioning sharpens spatial representation rewarded stimuli mouse primary visual cortex eLife e doi eLife PMC free article PubMed CrossRef Google Scholar Fritz J Shamma Elhilali Klein Rapid taskrelated plasticity spectrotemporal receptive fields primary auditory cortex Nature Neuroscience doi nn PubMed CrossRef Google Scholar Beniaguev Segev London Single cortical neurons deep artificial neural networks Neuron e doi jneuron PubMed CrossRef Google Scholar Jacot Gabriel F Hongler C Neural Tangent Kernel Convergence Generalization Neural Networks Advances Neural Information Processing Systems vol Curran Associates Inc Available httpspapersnipsccpaperhashabefaebbaecbdfaAbstracthtml Google Scholar Mattos CLC Dai Z Damianou Forth J Barreto GA Lawrence ND Recurrent Gaussian Processes arXiv cs stat Ringach DL Spatial structure symmetry simplecell receptive fields macaque primary visual cortex Journal Neurophysiology Jul doi jn PubMed CrossRef Google Scholar PLoS Comput Biol Oct e Published online Oct doi journalpcbi PMCID PMC PMID Structured random receptive fields enable informative sensory encodings Biraj Pandey Formal analysis Investigation Software Validation Visualization Writing original draft Writing review editing Marius Pachitariu Data curation Writing review editing Bingni W Brunton Formal analysis Resources Supervision Visualization Writing review editing Kameron Decker Harris Conceptualization Formal analysis Investigation Supervision Visualization Writing original draft Writing review editing Biraj Pandey Department Applied Mathematics University Washington Seattle Washington United States America Find articles Biraj Pandey Marius Pachitariu Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Find articles Marius Pachitariu Bingni W Brunton Department Biology University Washington Seattle Washington United States America Find articles Bingni W Brunton Kameron Decker Harris Department Computer Science Western Washington University Bellingham Washington United States America Find articles Kameron Decker Harris Xuexin Wei Editor Author information Article notes Copyright License information Disclaimer Department Applied Mathematics University Washington Seattle Washington United States America Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Department Biology University Washington Seattle Washington United States America Department Computer Science Western Washington University Bellingham Washington United States America University Texas Austin UNITED STATES Corresponding author authors declared competing interests exist Email udeuwwsirrahnoremak Received Oct Accepted Aug Copyright Pandey et al open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source creditedPLoS Comput Biol Oct e Published online Oct doi journalpcbi PMCID PMC PMID PLoS Comput Biol Oct e Published online Oct doi journalpcbiPLoS Comput Biol Oct e Published online Oct doi journalpcbiPLoS Comput Biol Oct e PLoS Comput BiolPublished online Oct doi journalpcbiPublished online Oct doi journalpcbidoi journalpcbiPMCID PMC PMID PMCID PMCPMCID PMCPMID Structured random receptive fields enable informative sensory encodingsBiraj Pandey Formal analysis Investigation Software Validation Visualization Writing original draft Writing review editing Marius Pachitariu Data curation Writing review editing Bingni W Brunton Formal analysis Resources Supervision Visualization Writing review editing Kameron Decker Harris Conceptualization Formal analysis Investigation Supervision Visualization Writing original draft Writing review editing Biraj Pandey Department Applied Mathematics University Washington Seattle Washington United States America Find articles Biraj Pandey Marius Pachitariu Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Find articles Marius Pachitariu Bingni W Brunton Department Biology University Washington Seattle Washington United States America Find articles Bingni W Brunton Kameron Decker Harris Department Computer Science Western Washington University Bellingham Washington United States America Find articles Kameron Decker HarrisBiraj Pandey Formal analysis Investigation Software Validation Visualization Writing original draft Writing review editing Marius Pachitariu Data curation Writing review editing Bingni W Brunton Formal analysis Resources Supervision Visualization Writing review editing Kameron Decker Harris Conceptualization Formal analysis Investigation Supervision Visualization Writing original draft Writing review editing Biraj PandeyFormal analysisInvestigationSoftwareValidationVisualizationWriting original draftWriting review editing Marius PachitariuData curationWriting review editing Bingni W BruntonFormal analysisResourcesSupervisionVisualizationWriting review editing Kameron Decker HarrisConceptualizationFormal analysisInvestigationSupervisionVisualizationWriting original draftWriting review editing Biraj Pandey Department Applied Mathematics University Washington Seattle Washington United States America Find articles Biraj Pandey Marius Pachitariu Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Find articles Marius Pachitariu Bingni W Brunton Department Biology University Washington Seattle Washington United States America Find articles Bingni W Brunton Kameron Decker Harris Department Computer Science Western Washington University Bellingham Washington United States America Find articles Kameron Decker HarrisBiraj Pandey Department Applied Mathematics University Washington Seattle Washington United States America Find articles Biraj PandeyBiraj Pandey Department Applied Mathematics University Washington Seattle Washington United States America Find articles Biraj PandeyBiraj PandeyMarius Pachitariu Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Find articles Marius PachitariuMarius Pachitariu Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Find articles Marius PachitariuMarius PachitariuBingni W Brunton Department Biology University Washington Seattle Washington United States America Find articles Bingni W BruntonBingni W Brunton Department Biology University Washington Seattle Washington United States America Find articles Bingni W BruntonBingni W BruntonKameron Decker Harris Department Computer Science Western Washington University Bellingham Washington United States America Find articles Kameron Decker HarrisKameron Decker Harris Department Computer Science Western Washington University Bellingham Washington United States America Find articles Kameron Decker HarrisKameron Decker HarrisXuexin Wei EditorEditorAuthor information Article notes Copyright License information Disclaimer Department Applied Mathematics University Washington Seattle Washington United States America Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Department Biology University Washington Seattle Washington United States America Department Computer Science Western Washington University Bellingham Washington United States America University Texas Austin UNITED STATES Corresponding author authors declared competing interests exist Email udeuwwsirrahnoremak Received Oct Accepted Aug Copyright Pandey et al open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source creditedAuthor information Article notes Copyright License information DisclaimerAuthor informationArticle notesCopyright License informationDisclaimer Department Applied Mathematics University Washington Seattle Washington United States America Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Department Biology University Washington Seattle Washington United States America Department Computer Science Western Washington University Bellingham Washington United States America University Texas Austin UNITED STATES Corresponding author authors declared competing interests exist Email udeuwwsirrahnoremak Department Applied Mathematics University Washington Seattle Washington United States America Janelia Research Campus Howard Hughes Medical Institute Ashburn Virginia United States America Department Biology University Washington Seattle Washington United States America Department Computer Science Western Washington University Bellingham Washington United States America University Texas Austin UNITED STATES Corresponding authorThe authors declared competing interests exist Email udeuwwsirrahnoremakudeuwwsirrahnoremakReceived Oct Accepted Aug Received Oct Accepted Aug Copyright Pandey et al open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source creditedCopyright Pandey et alCopyrightThis open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source creditedCreative Commons Attribution LicenseAssociated Data Supplementary Materials Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF pcbispdf GUID EEABECFA Attachment Submitted filename coverletterresubmissionpdf pcbispdf GUID FDDFCFDCBEDFB Data Availability Statement relevant data within manuscript Supporting information filesAssociated DataSupplementary Materials Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF pcbispdf GUID EEABECFA Attachment Submitted filename coverletterresubmissionpdf pcbispdf GUID FDDFCFDCBEDFBSupplementary MaterialsSupplementary MaterialsS Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF pcbispdf GUID EEABECFA Attachment Submitted filename coverletterresubmissionpdf pcbispdf GUID FDDFCFDCBEDFBS Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF pcbispdf GUID EEABECFAS Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDFS Appendix Fig ASimulation results simplified frequency detection taskFig BReceptive fields mechanosensory neuronsFig CCovariance matrix mechanosensory receptive fields unstructured modelFig DCovariance matrix mechanosensory receptive fields Fourier modelFig EReceptive fields mechanosensory neurons unstructured model Fourier modelFig FCovariance matrix V receptive fields model white noise stimuliEq Fig GReceptive fields V neurons white noise stimuliFig HCovariance matrix V receptive fields unstructured model white noise stimuliFig ICovariance matrix V receptive fields translation invariant V model white noise stimuliFig JReceptive fields V neurons unstructured model translation invariant V model white noise stimuliFig KSpectral properties V receptive fields model Ringach datasetEq Fig LReceptive fields V neurons Ringach datasetFig MSpectral properties V receptive fields model natural image stimuliEq Fig NReceptive fields V neurons natural images stimuliFig OSpectral properties V receptive fields model DHT stimuliEq Fig PReceptive fields V neurons DHT stimuliFig QTraining loss MNIST fullytrained neural networks initialized V weightsFig RTest error MNIST fullytrained neural networks initialized V weightsFig STraining loss KMNIST fullytrained neural networks initialized V weightsFig TTest error KMNIST fullytrained neural networks initialized V weightsFig UInitializing AlexNet using structured random features shows little benefit ImageNetPDFpcbispdf Mpcbispdf MGUID EEABECFAAttachment Submitted filename coverletterresubmissionpdf pcbispdf GUID FDDFCFDCBEDFBAttachment Submitted filename coverletterresubmissionpdfAttachment coverletterresubmissionpdfpcbispdf Mpcbispdf MGUID FDDFCFDCBEDFBData Availability Statement relevant data within manuscript Supporting information filesData Availability StatementData Availability StatementAll relevant data within manuscript Supporting information filesAll relevant data within manuscript Supporting information filesSupporting informationAbstract Brains must represent outside world animals survive thrive early sensory systems neural populations diverse receptive fields structured detect important features inputs yet significant variability ignored classical models sensory neurons model neuronal receptive fields random variable samples parameterized distributions demonstrate model two sensory modalities using data insect mechanosensors mammalian primary visual cortex approach leads significant theoretical connection foundational concepts receptive fields random features leading theory understanding artificial neural networks modeled neurons perform randomized wavelet transform inputs removes high frequency noise boosts signal random feature neurons enable learning fewer training samples smaller networks artificial tasks structured random model receptive fields provides unifying mathematically tractable framework understand sensory encodings across spatial temporal domainsAbstractBrains must represent outside world animals survive thrive early sensory systems neural populations diverse receptive fields structured detect important features inputs yet significant variability ignored classical models sensory neurons model neuronal receptive fields random variable samples parameterized distributions demonstrate model two sensory modalities using data insect mechanosensors mammalian primary visual cortex approach leads significant theoretical connection foundational concepts receptive fields random features leading theory understanding artificial neural networks modeled neurons perform randomized wavelet transform inputs removes high frequency noise boosts signal random feature neurons enable learning fewer training samples smaller networks artificial tasks structured random model receptive fields provides unifying mathematically tractable framework understand sensory encodings across spatial temporal domainsBrains must represent outside world animals survive thrive early sensory systems neural populations diverse receptive fields structured detect important features inputs yet significant variability ignored classical models sensory neurons model neuronal receptive fields random variable samples parameterized distributions demonstrate model two sensory modalities using data insect mechanosensors mammalian primary visual cortex approach leads significant theoretical connection foundational concepts receptive fields random features leading theory understanding artificial neural networks modeled neurons perform randomized wavelet transform inputs removes high frequency noise boosts signal random feature neurons enable learning fewer training samples smaller networks artificial tasks structured random model receptive fields provides unifying mathematically tractable framework understand sensory encodings across spatial temporal domainsAuthor summary Evolution ensured animal brains dedicated extracting useful information raw sensory stimuli discarding everything else Models sensory neurons key part theories brain represents world work model tuning properties sensory neurons way incorporates randomness builds bridge leading mathematical theory understanding artificial neural networks learn models capture important properties large populations real neurons presented varying stimuli Moreover give precise mathematical formula sensory neurons two distinct areas one involving gyroscopic organ insects visual processing center mammals transform inputs also find artificial models imbued properties real neurons learn efficiently shorter training time fewer examples mathematical theory explains findings work expands understanding sensory representation large networks benefits neuroscience machine learning communitiesAuthor summaryEvolution ensured animal brains dedicated extracting useful information raw sensory stimuli discarding everything else Models sensory neurons key part theories brain represents world work model tuning properties sensory neurons way incorporates randomness builds bridge leading mathematical theory understanding artificial neural networks learn models capture important properties large populations real neurons presented varying stimuli Moreover give precise mathematical formula sensory neurons two distinct areas one involving gyroscopic organ insects visual processing center mammals transform inputs also find artificial models imbued properties real neurons learn efficiently shorter training time fewer examples mathematical theory explains findings work expands understanding sensory representation large networks benefits neuroscience machine learning communitiesEvolution ensured animal brains dedicated extracting useful information raw sensory stimuli discarding everything else Models sensory neurons key part theories brain represents world work model tuning properties sensory neurons way incorporates randomness builds bridge leading mathematical theory understanding artificial neural networks learn models capture important properties large populations real neurons presented varying stimuli Moreover give precise mathematical formula sensory neurons two distinct areas one involving gyroscopic organ insects visual processing center mammals transform inputs also find artificial models imbued properties real neurons learn efficiently shorter training time fewer examples mathematical theory explains findings work expands understanding sensory representation large networks benefits neuroscience machine learning communitiesIntroduction long argued brain uses large population neurons represent world view sensory stimuli encoded responses population used downstream areas diverse tasks including learning decisionmaking movement control sensory areas different neurons responding differing stimuli also providing measure redundancy However still lack clear understanding response properties wellsuited different sensory modalities One way approach sensory encoding understanding neuron would respond arbitrary stimuli Experimentally typically present many stimuli animal measure responses sensory neurons attempt estimate kind model neurons respond arbitrary stimulus common assumption neuron computes linear filter stimulus drives spiking nonlinear spikegenerating mechanism Mathematically assumption summarized number measured spikes stimulus x equal w x weight vector w nonlinearity weights w define filtering properties neuron also known receptive field model known linearnonlinear LN model also common form artificial neuron artificial neural networks ANNs LN models used extensively describe firing diverse neurons various sensory modalities vertebrates invertebrates mammalian visual system LN models used characterize retinal ganglion cells lateral geniculate neurons simple cells primary visual cortex V also used characterize auditory sensory neurons avian midbrain somatosensory neurons cortex insects used understand response properties visual interneurons mechanosensory neurons involved proprioception auditory neurons courtship behavior Given stimulus presented neural response data one estimate receptive fields population neurons Simple visual receptive fields classically understood similar wavelets particular spatial frequency angular selectivity mechanosensory areas receptive fields selective temporal frequency short time window Commonly parametric modeling Gabor wavelets smoothing regularization etc used produce clean receptive fields Yet data alone show noisy receptive fields perhaps best modeled using random distribution show modeling receptive fields random samples produces realistic receptive fields reflect structure noisiness seen experimental data importantly perspective creates significant theoretical connections foundational ideas neuroscience artificial intelligence connection helps us understand receptive fields structures structure relates kinds stimuli relevant animal Modeling filtering properties population LN neurons samples random distribution leads study networks random weights machine learning ML networks known random feature networks RFNs study RFNs rapidly gained popularity recent years large part offers theoretically tractable way study learning properties ANNs weights tuned using data RFN contains many neurons approximate functions live wellunderstood function space function space called reproducing kernel Hilbert space RKHS depends network details particular weight ie receptive field distribution Learning framed approximating functions space limited data Several recent works highlight RFN theorys usefulness understanding learning neural systems Bordelon Canatar Pehlevan series papers shown neural codes allow learning examples spectral properties secondorder statistics aligns spectral properties task applied V found neural code aligned tasks depend low spatial frequency components Harris constructed RFN model sparse networks found associative centers like cerebellum insect mushroom body showed areas may behave like additive kernels architecture also considered Hashemi et al classes kernels beneficial learning high dimensions learn fewer examples remain resilient input noise adversarial perturbation Xie et al investigated relationship fraction active neurons model cerebellumcontrolled neuron thresholdsand generalization performance learning movement trajectories vast majority network studies random weights weights w drawn Gaussian distribution independent entries sampling equivalent fully unstructured receptive field looks like white noise Closely related work previous study ANNs showed directly learning structured receptive fields could improve image classification deep networks receptive fields parametrized sum Gaussian derivatives fourth order led better performance rival architectures low data regimes paper study effect structured yet random receptive fields lead informative sensory encodings Specifically consider receptive fields generated Gaussian process GP thought drawing weights w Gaussian distribution particular covariance matrix show networks random weights project input new basis filter particular components theory introduces realistic structure receptive fields random feature models crucial current understanding artificial networks Next show receptive field datasets two disparate sensory systems mechanosensory neurons insect wings V cortical neurons mice monkeys wellmodeled GPs covariance functions wavelet eigenbases Given success modeling data GP apply weight distributions RFNs used synthetic learning tasks find structured weights improve learning reducing number training examples size network needed learn task Thus structured random weights offer realistic generative model receptive fields multiple sensory areas understand performing random change basis change basis enables network represent important properties stimulus demonstrate useful learningIntroductionIt long argued brain uses large population neurons represent world view sensory stimuli encoded responses population used downstream areas diverse tasks including learning decisionmaking movement control sensory areas different neurons responding differing stimuli also providing measure redundancy However still lack clear understanding response properties wellsuited different sensory modalitiesOne way approach sensory encoding understanding neuron would respond arbitrary stimuli Experimentally typically present many stimuli animal measure responses sensory neurons attempt estimate kind model neurons respond arbitrary stimulus common assumption neuron computes linear filter stimulus drives spiking nonlinear spikegenerating mechanism Mathematically assumption summarized number measured spikes stimulus x equal w x weight vector w nonlinearity weights w define filtering properties neuron also known receptive field model known linearnonlinear LN model also common form artificial neuron artificial neural networks ANNs LN models used extensively describe firing diverse neurons various sensory modalities vertebrates invertebrates mammalian visual system LN models used characterize retinal ganglion cells lateral geniculate neurons simple cells primary visual cortex V also used characterize auditory sensory neurons avian midbrain somatosensory neurons cortex insects used understand response properties visual interneurons mechanosensory neurons involved proprioception auditory neurons courtship behavior xxwwTTxxwwwwreceptive fieldlinearnonlinearGiven stimulus presented neural response data one estimate receptive fields population neurons Simple visual receptive fields classically understood similar wavelets particular spatial frequency angular selectivity mechanosensory areas receptive fields selective temporal frequency short time window Commonly parametric modeling Gabor wavelets smoothing regularization etc used produce clean receptive fields Yet data alone show noisy receptive fields perhaps best modeled using random distribution show modeling receptive fields random samples produces realistic receptive fields reflect structure noisiness seen experimental data importantly perspective creates significant theoretical connections foundational ideas neuroscience artificial intelligence connection helps us understand receptive fields structures structure relates kinds stimuli relevant animalModeling filtering properties population LN neurons samples random distribution leads study networks random weights machine learning ML networks known random feature networks RFNs study RFNs rapidly gained popularity recent years large part offers theoretically tractable way study learning properties ANNs weights tuned using data RFN contains many neurons approximate functions live wellunderstood function space function space called reproducing kernel Hilbert space RKHS depends network details particular weight ie receptive field distribution Learning framed approximating functions space limited datarandom feature networksreproducing kernel Hilbert spaceSeveral recent works highlight RFN theorys usefulness understanding learning neural systems Bordelon Canatar Pehlevan series papers shown neural codes allow learning examples spectral properties secondorder statistics aligns spectral properties task applied V found neural code aligned tasks depend low spatial frequency components Harris constructed RFN model sparse networks found associative centers like cerebellum insect mushroom body showed areas may behave like additive kernels architecture also considered Hashemi et al classes kernels beneficial learning high dimensions learn fewer examples remain resilient input noise adversarial perturbation Xie et al investigated relationship fraction active neurons model cerebellumcontrolled neuron thresholdsand generalization performance learning movement trajectories vast majority network studies random weights weights w drawn Gaussian distribution independent entries sampling equivalent fully unstructured receptive field looks like white noisewwunstructuredClosely related work previous study ANNs showed directly learning structured receptive fields could improve image classification deep networks receptive fields parametrized sum Gaussian derivatives fourth order led better performance rival architectures low data regimesIn paper study effect structured yet random receptive fields lead informative sensory encodings Specifically consider receptive fields generated Gaussian process GP thought drawing weights w Gaussian distribution particular covariance matrix show networks random weights project input new basis filter particular components theory introduces realistic structure receptive fields random feature models crucial current understanding artificial networks Next show receptive field datasets two disparate sensory systems mechanosensory neurons insect wings V cortical neurons mice monkeys wellmodeled GPs covariance functions wavelet eigenbases Given success modeling data GP apply weight distributions RFNs used synthetic learning tasks find structured weights improve learning reducing number training examples size network needed learn task Thus structured random weights offer realistic generative model receptive fields multiple sensory areas understand performing random change basis change basis enables network represent important properties stimulus demonstrate useful learningstructured yet randomwwResults construct generative model receptive fields sensory neurons use weights ANN refer network structured random feature network first review basics random feature networks details rationale behind generative model process generate hidden weights main theory result networks weights transform inputs new basis filter particular components thus bridging sensory neuroscience theory neural networks Next show neurons two receptive field datasetsinsect mechanosensory neurons mammalian V cortical neuronsare welldescribed generative model close resemblance secondorder statistics sampled receptive fields principal components data model Finally show performance structured random feature networks several synthetic learning tasks hidden weights generative model allows network learn fewer training examples smaller network sizes Theoretical analysis consider receptive fields generated GPs order connect foundational concept sensory neuroscience theory random features artificial neural networks GPs thought samples Gaussian distribution particular covariance matrix initialize hidden weights RFNs using GPs show using GP causes network project input new basis filter particular components basis determined covariance matrix Gaussian useful removing irrelevant noisy components input use results study space functions RFNs containing many neurons learn connecting construction theory kernel methods Random feature networks start introducing main learning algorithm neuronal model work RFN Consider twolayer feedforward ANN Traditionally weights initialized randomly learned backpropagation minimizing loss objective sharp contrast RFNs hidden layer weights sampled randomly distribution fixed hidden unit computes random feature input output layer weights trained Fig Note weights randomly drawn neurons response deterministic function input given weights Open separate window Fig Random feature networks structured weights study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distribution Mathematically hidden layer activations output given h W x h x R stimulus h h h h R hidden neuron responses R predicted output use rectified linear ReLU nonlinearity x max x applied entrywise Eq hidden layer weights W w w w R drawn randomly fixed readout weights trained RFNs RFN experiments train readout weights R offset R using support vector machine SVM classifier squared hinge loss penalty regularization strength tuned range fold crossvalidation RFNs include threshold hidden neurons although could help certain contexts vast majority studies RFNs neurons weights w R initialized iid spherical Gaussian distribution w N call networks built way classical unstructured RFNs Fig propose variation hidden weights initialized w N C C R positive semidefinite covariance matrix call networks structured RFNs Fig mean weights random specified covariance compare unstructured structured weights equal footing normalize covariance matrices Tr C Tr ensures mean square amplitude weights E w Receptive fields modeled linear weights Sensory neurons respond preferentially specific features inputs stimulus selectivity often summarized neurons receptive field describes features sensory space elicits responses stimulated Mathematically receptive fields modeled linear filter stimulus space Linear filters also integral component widely used LN model sensory processing According model firing rate neuron nonlinear function applied projection stimulus onto lowdimensional subspace linear filter linear filter model receptive fields explain responses individual neurons diverse stimuli used describe disparate sensory systems like visual auditory somatosensory systems diverse species including birds mammals insects stimuli uncorrelated filters estimated computing spike triggered average STA average stimulus elicited spike neuron stimuli correlated STA filter whitened inverse stimulus covariance matrix Often STAs denoised fitting parametric function STA Gabor wavelets simple cells V model receptive field neuron weight vector w nonlinear function Instead fitting parametric function construct covariance functions realization resulting Gaussian process resembles biological receptive field Fig Structured weights project filter input covariance eigenbasis generate network weights Gaussian processes GP whose covariance functions inspired receptive fields sensory neurons brain definition GP stochastic process finite observations follow Gaussian distribution find networks weights project inputs new basis filter irrelevant components see adds inductive bias classical RFNs tasks naturalistic inputs improves learning view weight vector w finitedimensional discretization continuous function w sample GP continuous function domain compact subset R assume discretized using grid equally spaced points w w Let input realvalued function x domain could represent finite timeseries image luminance retina complicated spatiotemporal sets like movie continuous setting dimensional inner product w x w x gets replaced L inner product w x w x Every GP fully specified mean covariance function C always assume mean zero study different covariance functions KosambiKarhunenLove theorem realization zeromean GP random series representation w z terms standard Gaussian random variables z N functions weights pairs eigenvalue eigenfunction pairs covariance operator C L L C f C f continuous analog covariance matrix C C positive definite opposed semidefinite eigenfunctions form complete basis L Using Eq inner product stimulus neurons weights w x z x z x z x x x Eq shows structured weights compute projection input x onto eigenfunction x reweight filter eigenvalue taking inner product random Gaussian weights z illuminating see continuous equations look like dimensional discrete setting Samples finitedimensional GP used hidden weights RFNs w N C First GP series representation Eq becomes w z matrices eigenvalues eigenvectors z N Gaussian random vector definition covariance matrix C E w w equal steps linear algebra Finally Eq analogous w x z x Since orthogonal matrix x equivalent change basis diagonal matrix shrinks expands certain directions perform filtering summarized following theorem Theorem Basis change formula Assume w N C C eigenvalue decomposition x R define x x w x z x z N Theorem says projecting input onto structured weight vector first filtering input GP eigenbasis random projection onto spherical random Gaussian form GP eigenbasis determined choice covariance function covariance function compatible input structure hidden weights filter irrelevant features noise stimuli amplifying descriptive features inductive bias facilitates inference stimuli downstream predictor spherical Gaussian distribution canonical choice unstructured RFNs simple way evaluate effective kernel structured RFNs k struct x x k unstruct x x see Appendix expression structured kernel provides concrete connection kernel theory learning using nonlinear neural networks readers interested kernel theories full example simulation results work given Appendix show exponential reduction number samples needed learn frequency detection using structured versus unstructured basis Fig Appendix Examples random yet structured receptive fields goal model weights artificial neurons way inspired biological neurons receptive fields Structured RFNs sample hidden weights GPs structured covariance construct covariance functions make generated weights resemble neuronal receptive fields start toy example stationary GP wellunderstood Fourier eigenbasis show receptive fields generated GP selective frequencies timeseries signals construct locally stationary covariance models insect mechanosensory V neuron receptive fields models shown good match experimental data Warmup Frequency selectivity stationary covariance illustrate results theoretical analysis start toy example temporal receptive fields selective particular frequencies example may familiar readers comfortable Fourier series basic signal processing Let input finite continuous timeseries x interval L use covariance function C k k cos k stationary process k k L k th natural frequency k weight coefficients covariance function Eq stationary means depends difference timepoints Applying compound angle formula get C k k cos k cos k sin k sin k Since sinusoidal functions cos k sin k form orthonormal basis L Eq eigendecomposition covariance eigenfunctions sines cosines eigenvalues k Eq know structured weights covariance form random series w k k z k cos k z k sin k z k z k N Thus receptive fields made sinusoids weighted k Gaussian variables z k z k Suppose want receptive fields retain specific frequency information signal filter rest Take k k k f lo k f hi call bandlimited spectrum passband f lo f hi bandwidth f hi f lo bandwidth increases receptive fields become less smooth since made wider range frequencies k nonzero decay certain rate rate controls smoothness resulting GP receptive fields act input signals x implicitly transform inputs Fourier basis filter frequencies based magnitude k bandlimited setting frequencies outside passband filtered makes receptive fields selective particular range frequencies ignore others hand classical random features weight frequencies equally even though natural settings high frequency signals corrupted noise Insect mechanosensors next consider particular biological sensor sensitive timehistory forces Campaniform sensilla CS domeshaped mechanoreceptors detect local stress strain insect exoskeleton embedded cuticle deformation cuticle bending torsion induces depolarizing currents CS opening mechanosensitive ion channels CS encode proprioceptive information useful body state estimation movement control diverse tasks like walking kicking jumping flying model receptive fields CS believed critical flight control namely ones found base halteres wings Fig Halteres wings flap rhythmically flight rotations insects body induce torsional forces felt active sensory structures CS detect small strain forces thereby encoding angular velocity insect body Experimental results show haltere wing CS selective broad range oscillatory frequencies STAs smooth oscillatory selective frequency decay time Fig B Open separate window Fig Random receptive field model insect mechanosensors Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shown model temporal receptive fields locally stationary GP bandlimited spectrum Examples receptive fields generated GP shown Fig C inputs CS modeled finite continuous timeseries x finite interval L neuron weights generated covariance function C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum k k L k th natural frequency warmup frequency selectivity weights accounted parameters f lo f hi bandwidth f hi f lo increases receptive fields built wider selection frequencies makes receptive fields less smooth Fig field localized near decay determined parameter increases receptive field selective larger time windows eigenbasis covariance function Eq similar Fourier eigenbasis modulated decaying exponential eigenbasis orthonormal basis span k e cos k k e sin k nonorthogonal set functions L hidden weights transform timeseries inputs eigenbasis discard components outside passband frequencies f lo f hi fit covariance model receptive field data CS neurons wings hawkmoth Manduca sexta data Briefly CS receptive fields estimated spiketriggered average STA experimental mechanical stimuli wings stimuli generated bandpassed white noise Hz characterize receptive fields population CS neurons compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field number samples case kHz ms samples normalization sets overall scale covariance matrix data covariance matrix shows tridiagonal structure Fig main diagonal positive diagonals negative diagonals decay away top left matrix Open separate window Fig Spectral properties mechanosensory RFs model similar compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behavior fit covariance model data optimize parameters f lo f hi finding f lo Hz f hi Hz ms best fit sensilla data minimizing Frobenius norm difference C data model see Appendix resulting model covariance matrix Fig B matches data covariance matrix Fig remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig B Appendix simulate effect finite number neurons generate weight vectors equal number neurons recorded recompute model covariance matrix Fig C call finite neuron model covariance matrix C finite shows bump bloblike structures evident C data C model result suggests bumpy structures attributed small number recorded neurons hypothesize effects would disappear larger dataset C data would closely resemble C model comparison also calculate Frobenius difference null models unstructured covariance model Fourier model unstructured model Frobenius norm difference Fourier model sensilla covariance model much lower difference compared null models fitting data accurately show covariance matrices sampled receptive fields null models Fig C E Appendix Comparing eigenvectors eigenvalues data model covariance matrices find spectral properties C model C finite similar C data eigenvalue curves models match data quite well Fig E curves directly comparable covariance normalized trace makes sum eigenvalues unity data model covariance matrices lowdimensional first data eigenvectors explain variance top explain top eigenvectors model finite sample match data quite well Fig Primary visual cortex turn visually driven neurons mammalian primary cortex Primary visual cortex V earliest cortical area processing visual information Fig neurons V detect small changes visual features like orientations spatial frequencies contrast size Open separate window Fig Random receptive field model Primary Visual Cortex V Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fields model receptive fields simple cells V clear excitatory inhibitory regions light shone excitatory regions increase cells response viceversa Fig B shape regions determines orientation selectivity widths determine frequency selectivity receptive fields centered location visual field decay away integrate visual stimuli within small region center Gabor functions widely used mathematical model receptive fields simple cells model receptive fields using another locally stationary GP show examples generated receptive fields Fig C Consider inputs cortical cells continuous twodimensional image x domain L L x R Since image realvalued x grayscale contrast single color channel pixel values neuron weights generated covariance function following form C exp f smooth receptive fields exp c c localized center c receptive field center defined c size receptive field determined parameter increases receptive field extends farther center c Fig Spatial frequency selectivity accounted bandwidth parameter f f decreases spatial frequency receptive field goes making weights less smooth Fig E eigendecomposition covariance function Eq leads orthonormal basis single scale Hermite wavelets c wavelet eigenfunctions Hermite polynomials modulated decaying Gaussian k e c H k c k c k H k k th physicists Hermite polynomial eigenfunctions nonzero centers c shifted versions Eq detailed derivation values constants c c c normalization Appendix use Eq model receptive field data V neurons recorded calcium imaging transgenic mice expressing GCaMPs mice headfixed running airfloating ball presented unique white noise images pixels using Psychtoolbox pixels white black equal probability Images upsampled resolution screens via bilinear interpolation stimulus corrected eyemovements online using custom code responses cells collected using twophoton mesoscope preprocessed using Suitep Receptive fields calculated white noise images deconvolved calcium responses cells using STA covariance analysis picked cells signaltonoise SNR threshold gave us cells SNR calculated smaller set images presented twice using method preprocessing step moved center mass every receptive field center visual field compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field case pixels pixels data covariance matrix resembles tridiagonal matrix However diagonals nonzero equally spaced segments Additionally values decay away center matrix show C data zoomed nonzero region around center matrix Fig corresponds pixel region around center full pixel matrix full covariance matrix shown Fig F Appendix Open separate window Fig Spectral properties V RFs model similar compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons red covariance model number offdiagonals center rate decay away center determined parameters f c respectively covariance pixels decays function distance c leads equallyspaced nonzero segments hand covariance also decays function distance pixels brings diagonal structure model frequency parameter f increases number offdiagonals increases Pixels generated weights become correlated weights become spatially smoother size parameter increases diagonals decay slower center c increasing correlations center pixel leading significant weights occupy visual field optimize parameters fit data finding f pixels minimizing Frobenius norm difference C data model need optimize center parameter c since preprocess data receptive fields centered c center grid resulting model covariance matrix Fig B data covariance matrix Fig match remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig G Appendix simulate effect finite number neurons generate weights equal number neurons data compute C finite shown Fig C finite matrix C finite looks even like C data shows negative covariances far center result finite sample size comparison also calculate normalized Frobenius difference null models unstructured covariance model translation invariant V model translation invariant model remove spatially localizing exponential Eq fit spatial frequency parameter f unstructured model Frobenius norm difference translation invariant model V covariance model much lower difference better fit data show covariance matrices sampled receptive fields null models Fig H J Appendix Similar spectral properties evident eigenvectors eigenvalues C model C finite C data analytical forms derived Eq Fig E covariances normalized unit trace Note analytical eigenfunctions shown finer grid model data analysis performed continuous space differences eigenfunctions eigenvalues analytical model results due discretization Examining eigenvectors Fig also see good match although rotations differences ordering eigenvectors explain variance receptive field data reference top eigenvectors explain variance data variance model eigenvalue curves models analytical forms match data Fig E reasonably well although well mechanosensors Appendix repeat analysis receptive fields measured different stimulus sets mouse different experimental dataset nonhuman primate V findings consistent results shown Fig K P Appendix Advantages structured random weights artificial learning tasks hypothesis neuronal inductive bias structured receptive fields allows networks learn fewer neurons training examples steps gradient descent classification tasks naturalistic inputs examine hypothesis compare performance structured receptive fields classical ones several classification tasks find artificial learning tasks structured random networks learn accurately smaller network sizes fewer training examples gradient steps Frequency detection CS naturally encode timehistory strain forces acting insect body sensors inspired temporal filtering properties shown accurately classify spatiotemporal data Inspired result test sensillainspired mechanosensory receptive fields timeseries classification task Fig top example presented network ms timeseries sampled kHz goal detect whether example contains sinusoidal signal positive examples sinusoidal signals f Hz corrupted noise SNR dB negative examples Gaussian white noise matched amplitude positive examples Note frequency detection task linearly separable random phases positive negative examples See Appendix additional details including definition SNR crossvalidation used find optimal parameters f lo Hz f hi Hz ms Open separate window Fig Random mechanosensory weights enable learning fewer neurons timeseries classification tasks show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible task number hidden neurons structured RFN significantly outperforms classical RFN show test performance using tuned parameters Fig Even noisy task achieves test error using hidden neurons Meanwhile classical network takes neurons achieve similar error Predictably performance suffers weights incompatible task show results f lo Hz f hi Hz Fig incompatible RFN performs better chance error much worse classical RFN takes neurons achieve test error test error decrease level even additional hidden neurons Frequency XOR task challenge mechanosensorinspired networks difficult task build frequency ExclusiveOR XOR problem Fig B top XOR binary function returns true inputs different otherwise returns false XOR classical example function linearly separable thus harder learn inputs ms timeseries sampled kHz inputs either contain pure frequency f Hz f Hz mixed frequency signals f f white noise pure mixed frequency cases add noise SNR See Appendix details goal task output true input contains either pure tone false input contains mixed frequencies white noise tune GP covariance parameters f lo f hi Eq using crossvalidation cross validation procedure algorithmic details identical frequency detection task Using cross validation find optimal parameters f lo Hz f hi Hz ms incompatible weights take f lo Hz f hi Hz structured RFN significantly outperform classical RFN number hidden neurons show network performance using parameters Fig B Classification error achieved hidden neurons sharp contrast classical RFN requires hidden neurons achieve error incompatible weights network needs neurons achieve test error improve larger network sizes four input subclasses consistently fails classify pure Hz sinusoidal signals outside passband Image classification next test Vinspired receptive fields two standard digit classification tasks MNIST KMNIST MNIST KMNIST datasets contain images handwritten digits MNIST Arabic numerals whereas KMNIST Japanese hiragana phonetic characters datasets come split training test examples classes training examples per class Every example grayscale image centered characters hidden weight center c chosen uniformly random pixels ensures networks weights uniformly cover image space fact means network represent sum locallysmooth functions see Appendix use network hidden neurons tune GP covariance parameters f Eq using fold cross validation MNIST training set parameter ranges pixels optimal parameters found grid search find optimal parameters pixels f pixels refit optimal model using entire training set parameters MNIST used KMNIST task without additional tuning Vinspired achieves much lower average classification error compared classical RFN number hidden neurons show learning performance using parameters MNIST task Fig achieve error MNIST task requires neurons versus neurons classical RFN structured RFN achieves error neurons Qualitatively similar results hold KMNIST task Fig B although overall errors larger reflecting harder task achieve error KMNIST requires neurons versus neurons classical RFN structured RFN achieves error neurons Open separate window Fig Random V weights enable learning fewer neurons fewer examples digit classification tasks show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properly network performance suffers GP covariance parameters match task happens size parameter smaller stroke width spatial scale f doesnt match stroke variations character Taking incompatible parameters f Fig B structured weights performs worse classical RFN tasks hidden neurons achieves relatively poor test errors MNIST Fig KMNIST Fig B Structured weights improve generalization limited data Alongside learning fewer hidden neurons V structured RFNs also learn accurately fewer examples test fewshot learning using image classification datasets training examples reduced training examples per class test set GP parameters remain Structured encodings allow learning fewer samples unstructured encodings show fewshot learning results Fig C networks performance saturate past hundred hidden neurons MNIST lowest error achieved V structured RFN versus classical RFN using incompatible weights Fig C structured network acheives error using structured features KMNIST task opposed classical RFN using incompatible weights Fig Networks train faster initialized structured weights study effect structured weights initialization strategy fullytrained neural networks weights network vary hypothesized structured initialization allows networks learn faster ie training loss test error would decrease faster unstructured weights shown performance RFNs improves biologically inspired weight sampling However RFNs Eq readout weights modified training hidden weights W frozen initial value compare biologicallymotivated initialization classical initialization variance inversely proportional number hidden neurons w unstruct N initialization widely known Kaiming normal scheme thought stabilize training dynamics controlling magnitude gradients classical approach ensures Tr fair comparison scale structured weight covariance matrix Tr C studies RFNs trace equal weight scale absorbed readout weights due homogeneity ReLU compare structured unstructured weights MNIST KMNIST tasks common benchmarks fullytrained networks architecture single hidden layer feedforward neural network Fig hidden neurons crossentropy loss training sets minimized using simple gradient descent GD epochs fair comparison learning rate optimized network separately define area training loss curve metric speed learning perform grid search range e e learning rate minimizes metric resulting parameters structured unstructured incompatible networks respectively parameters image classification MNIST KMNIST tasks Vinitialized network minimizes loss function faster classically initialized network MNIST task V network achieves loss value epochs compared network Fig see qualitatively similar results KMNIST task end training Vinspired networks loss classically initialized network reaches Fig B find Vinitialized network performs better classical initialization covariance parameters match task incompatible parameters Vinitialized network achieves loss value MNIST KMNIST Open separate window Fig V weight initialization fullytrained networks enables faster training digit classification tasks show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible task minimize training loss faster Vinitialized network also generalizes well achieves lower test error end training MNIST achieves test error compared error classically initialized network using incompatible weights Fig C KMNIST see error compared error classical initalization using incompatible weights Fig see similar results across diverse hidden layer widths learning rates Fig Q Appendix benefits evident wider networks smaller learning rates Furthermore structured weights show similar results trained epochs rate neurons shown optimizers like minibatch Stochastic Gradient Descent SGD ADAM batch size rate neurons shown Structured initialization facilitates learning across wide range networks However improvement universal significant benefit found initializing early convolutional layers deep network AlexNet applying ImageNet dataset shown Appendix Fig U Appendix large amounts training data fact small fraction network initialized structured weights could explain null result Also many scenarios incompatible structured weights get performance par compatible ones end training poor inductive bias overcome Improving representation structured random weights shown structured receptive field weights improve performance RFNs fullytrained networks number supervised learning tasks long receptive fields compatible task performance gains unstructured features possible incompatible networks performs better even worse using classical unstructured weights results understood theoretical framework Structured weights effectively cause input x undergo linear transformation new representation x following Theorem examples new representation bandlimited due design covariance function V weights eigenvalues nonzero spectrum decays exponentially acts lowpass filter moving bandlimited representation filter noisehighfrequency componentsand reduce dimensionalitycoordinates x outside passband zero general noise dimensionality make learning harder easiest understand effects frequency detection task simplicity assume using stationary features warmup frequency detection task signal power contained f Hz frequency everything else due noise weights compatible task means w sum sines cosines frequencies k passband includes f narrower make bandwidth still retaining signal higher SNR x becomes since noise filtered see Appendix ResultsWe construct generative model receptive fields sensory neurons use weights ANN refer network structured random feature network first review basics random feature networks details rationale behind generative model process generate hidden weights main theory result networks weights transform inputs new basis filter particular components thus bridging sensory neuroscience theory neural networks Next show neurons two receptive field datasetsinsect mechanosensory neurons mammalian V cortical neuronsare welldescribed generative model close resemblance secondorder statistics sampled receptive fields principal components data model Finally show performance structured random feature networks several synthetic learning tasks hidden weights generative model allows network learn fewer training examples smaller network sizesstructuredTheoretical analysis consider receptive fields generated GPs order connect foundational concept sensory neuroscience theory random features artificial neural networks GPs thought samples Gaussian distribution particular covariance matrix initialize hidden weights RFNs using GPs show using GP causes network project input new basis filter particular components basis determined covariance matrix Gaussian useful removing irrelevant noisy components input use results study space functions RFNs containing many neurons learn connecting construction theory kernel methods Random feature networks start introducing main learning algorithm neuronal model work RFN Consider twolayer feedforward ANN Traditionally weights initialized randomly learned backpropagation minimizing loss objective sharp contrast RFNs hidden layer weights sampled randomly distribution fixed hidden unit computes random feature input output layer weights trained Fig Note weights randomly drawn neurons response deterministic function input given weights Open separate window Fig Random feature networks structured weights study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distribution Mathematically hidden layer activations output given h W x h x R stimulus h h h h R hidden neuron responses R predicted output use rectified linear ReLU nonlinearity x max x applied entrywise Eq hidden layer weights W w w w R drawn randomly fixed readout weights trained RFNs RFN experiments train readout weights R offset R using support vector machine SVM classifier squared hinge loss penalty regularization strength tuned range fold crossvalidation RFNs include threshold hidden neurons although could help certain contexts vast majority studies RFNs neurons weights w R initialized iid spherical Gaussian distribution w N call networks built way classical unstructured RFNs Fig propose variation hidden weights initialized w N C C R positive semidefinite covariance matrix call networks structured RFNs Fig mean weights random specified covariance compare unstructured structured weights equal footing normalize covariance matrices Tr C Tr ensures mean square amplitude weights E w Receptive fields modeled linear weights Sensory neurons respond preferentially specific features inputs stimulus selectivity often summarized neurons receptive field describes features sensory space elicits responses stimulated Mathematically receptive fields modeled linear filter stimulus space Linear filters also integral component widely used LN model sensory processing According model firing rate neuron nonlinear function applied projection stimulus onto lowdimensional subspace linear filter linear filter model receptive fields explain responses individual neurons diverse stimuli used describe disparate sensory systems like visual auditory somatosensory systems diverse species including birds mammals insects stimuli uncorrelated filters estimated computing spike triggered average STA average stimulus elicited spike neuron stimuli correlated STA filter whitened inverse stimulus covariance matrix Often STAs denoised fitting parametric function STA Gabor wavelets simple cells V model receptive field neuron weight vector w nonlinear function Instead fitting parametric function construct covariance functions realization resulting Gaussian process resembles biological receptive field Fig Structured weights project filter input covariance eigenbasis generate network weights Gaussian processes GP whose covariance functions inspired receptive fields sensory neurons brain definition GP stochastic process finite observations follow Gaussian distribution find networks weights project inputs new basis filter irrelevant components see adds inductive bias classical RFNs tasks naturalistic inputs improves learning view weight vector w finitedimensional discretization continuous function w sample GP continuous function domain compact subset R assume discretized using grid equally spaced points w w Let input realvalued function x domain could represent finite timeseries image luminance retina complicated spatiotemporal sets like movie continuous setting dimensional inner product w x w x gets replaced L inner product w x w x Every GP fully specified mean covariance function C always assume mean zero study different covariance functions KosambiKarhunenLove theorem realization zeromean GP random series representation w z terms standard Gaussian random variables z N functions weights pairs eigenvalue eigenfunction pairs covariance operator C L L C f C f continuous analog covariance matrix C C positive definite opposed semidefinite eigenfunctions form complete basis L Using Eq inner product stimulus neurons weights w x z x z x z x x x Eq shows structured weights compute projection input x onto eigenfunction x reweight filter eigenvalue taking inner product random Gaussian weights z illuminating see continuous equations look like dimensional discrete setting Samples finitedimensional GP used hidden weights RFNs w N C First GP series representation Eq becomes w z matrices eigenvalues eigenvectors z N Gaussian random vector definition covariance matrix C E w w equal steps linear algebra Finally Eq analogous w x z x Since orthogonal matrix x equivalent change basis diagonal matrix shrinks expands certain directions perform filtering summarized following theorem Theorem Basis change formula Assume w N C C eigenvalue decomposition x R define x x w x z x z N Theorem says projecting input onto structured weight vector first filtering input GP eigenbasis random projection onto spherical random Gaussian form GP eigenbasis determined choice covariance function covariance function compatible input structure hidden weights filter irrelevant features noise stimuli amplifying descriptive features inductive bias facilitates inference stimuli downstream predictor spherical Gaussian distribution canonical choice unstructured RFNs simple way evaluate effective kernel structured RFNs k struct x x k unstruct x x see Appendix expression structured kernel provides concrete connection kernel theory learning using nonlinear neural networks readers interested kernel theories full example simulation results work given Appendix show exponential reduction number samples needed learn frequency detection using structured versus unstructured basis Fig Appendix Theoretical analysisWe consider receptive fields generated GPs order connect foundational concept sensory neuroscience theory random features artificial neural networks GPs thought samples Gaussian distribution particular covariance matrix initialize hidden weights RFNs using GPs show using GP causes network project input new basis filter particular components basis determined covariance matrix Gaussian useful removing irrelevant noisy components input use results study space functions RFNs containing many neurons learn connecting construction theory kernel methodsRandom feature networks start introducing main learning algorithm neuronal model work RFN Consider twolayer feedforward ANN Traditionally weights initialized randomly learned backpropagation minimizing loss objective sharp contrast RFNs hidden layer weights sampled randomly distribution fixed hidden unit computes random feature input output layer weights trained Fig Note weights randomly drawn neurons response deterministic function input given weights Open separate window Fig Random feature networks structured weights study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distribution Mathematically hidden layer activations output given h W x h x R stimulus h h h h R hidden neuron responses R predicted output use rectified linear ReLU nonlinearity x max x applied entrywise Eq hidden layer weights W w w w R drawn randomly fixed readout weights trained RFNs RFN experiments train readout weights R offset R using support vector machine SVM classifier squared hinge loss penalty regularization strength tuned range fold crossvalidation RFNs include threshold hidden neurons although could help certain contexts vast majority studies RFNs neurons weights w R initialized iid spherical Gaussian distribution w N call networks built way classical unstructured RFNs Fig propose variation hidden weights initialized w N C C R positive semidefinite covariance matrix call networks structured RFNs Fig mean weights random specified covariance compare unstructured structured weights equal footing normalize covariance matrices Tr C Tr ensures mean square amplitude weights E w Random feature networks start introducing main learning algorithm neuronal model work RFN Consider twolayer feedforward ANN Traditionally weights initialized randomly learned backpropagation minimizing loss objective sharp contrast RFNs hidden layer weights sampled randomly distribution fixed hidden unit computes random feature input output layer weights trained Fig Note weights randomly drawn neurons response deterministic function input given weightsFig Fig Open separate window Fig Random feature networks structured weights study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distributionOpen separate windowOpen separate windowOpen separate windowFig Random feature networks structured weights study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distributionFig Fig Random feature networks structured weights study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distributionRandom feature networks structured weightsWe study random feature networks models learning sensory regions networks neurons weight w fixed random sample specified distribution readout weights trained particular specify distributions Gaussian Processes GPs whose covariances inspired biological neurons thus realization GP resembles biological receptive field build GP models two sensory areas specialize processing timeseries image inputs initialize w structured GPs compare initialization unstructured whitenoise distributionwwwwstructuredunstructuredMathematically hidden layer activations output given h W x h h W x h h W x h h W x h h W x h h W x h h W x h hWxy TTh x R stimulus h h h h R hidden neuron responses R predicted output use rectified linear ReLU nonlinearity x max x applied entrywise Eq hidden layer weights W w w w R drawn randomly fixed readout weights trained RFNsx R dx R dx R dxR dRdh h h h R mh h h h R mh h h h R mh h h h h h h h hh hh mhmTR mRmy Ry Ry Ry yRxxEq W w w w R dW w w w R dW w w w R dW w w w w w w w ww ww mwmTR dRm dmdIn RFN experiments train readout weights R offset R using support vector machine SVM classifier squared hinge loss penalty regularization strength tuned range fold crossvalidation RFNs include threshold hidden neurons although could help certain contexts R R R mR mRm R R R RIn vast majority studies RFNs neurons weights w R initialized iid spherical Gaussian distribution w N call networks built way classical unstructured RFNs Fig propose variation hidden weights initialized w N C C R positive semidefinite covariance matrix call networks structured RFNs Fig mean weights random specified covariance compare unstructured structured weights equal footing normalize covariance matrices Tr C Tr ensures mean square amplitude weights E w w R dw R dw R dwR dRdw N w N w N wNI dIdclassical unstructuredFig Fig w N C w N C w N C wNCC R dC R dC R dCR dRd dddstructuredFig Fig CCIIdddE w dE w dE w dEw dReceptive fields modeled linear weights Sensory neurons respond preferentially specific features inputs stimulus selectivity often summarized neurons receptive field describes features sensory space elicits responses stimulated Mathematically receptive fields modeled linear filter stimulus space Linear filters also integral component widely used LN model sensory processing According model firing rate neuron nonlinear function applied projection stimulus onto lowdimensional subspace linear filter linear filter model receptive fields explain responses individual neurons diverse stimuli used describe disparate sensory systems like visual auditory somatosensory systems diverse species including birds mammals insects stimuli uncorrelated filters estimated computing spike triggered average STA average stimulus elicited spike neuron stimuli correlated STA filter whitened inverse stimulus covariance matrix Often STAs denoised fitting parametric function STA Gabor wavelets simple cells V model receptive field neuron weight vector w nonlinear function Instead fitting parametric function construct covariance functions realization resulting Gaussian process resembles biological receptive field Fig Receptive fields modeled linear weights Sensory neurons respond preferentially specific features inputs stimulus selectivity often summarized neurons receptive field describes features sensory space elicits responses stimulated Mathematically receptive fields modeled linear filter stimulus space Linear filters also integral component widely used LN model sensory processing According model firing rate neuron nonlinear function applied projection stimulus onto lowdimensional subspace linear filterA linear filter model receptive fields explain responses individual neurons diverse stimuli used describe disparate sensory systems like visual auditory somatosensory systems diverse species including birds mammals insects stimuli uncorrelated filters estimated computing spike triggered average STA average stimulus elicited spike neuron stimuli correlated STA filter whitened inverse stimulus covariance matrix Often STAs denoised fitting parametric function STA Gabor wavelets simple cells V model receptive field neuron weight vector w nonlinear function Instead fitting parametric function construct covariance functions realization resulting Gaussian process resembles biological receptive field Fig iwwiiFig Fig Structured weights project filter input covariance eigenbasis generate network weights Gaussian processes GP whose covariance functions inspired receptive fields sensory neurons brain definition GP stochastic process finite observations follow Gaussian distribution find networks weights project inputs new basis filter irrelevant components see adds inductive bias classical RFNs tasks naturalistic inputs improves learning view weight vector w finitedimensional discretization continuous function w sample GP continuous function domain compact subset R assume discretized using grid equally spaced points w w Let input realvalued function x domain could represent finite timeseries image luminance retina complicated spatiotemporal sets like movie continuous setting dimensional inner product w x w x gets replaced L inner product w x w x Every GP fully specified mean covariance function C always assume mean zero study different covariance functions KosambiKarhunenLove theorem realization zeromean GP random series representation w z terms standard Gaussian random variables z N functions weights pairs eigenvalue eigenfunction pairs covariance operator C L L C f C f continuous analog covariance matrix C C positive definite opposed semidefinite eigenfunctions form complete basis L Using Eq inner product stimulus neurons weights w x z x z x z x x x Eq shows structured weights compute projection input x onto eigenfunction x reweight filter eigenvalue taking inner product random Gaussian weights z illuminating see continuous equations look like dimensional discrete setting Samples finitedimensional GP used hidden weights RFNs w N C First GP series representation Eq becomes w z matrices eigenvalues eigenvectors z N Gaussian random vector definition covariance matrix C E w w equal steps linear algebra Finally Eq analogous w x z x Since orthogonal matrix x equivalent change basis diagonal matrix shrinks expands certain directions perform filtering summarized following theorem Theorem Basis change formula Assume w N C C eigenvalue decomposition x R define x x w x z x z N Theorem says projecting input onto structured weight vector first filtering input GP eigenbasis random projection onto spherical random Gaussian form GP eigenbasis determined choice covariance function covariance function compatible input structure hidden weights filter irrelevant features noise stimuli amplifying descriptive features inductive bias facilitates inference stimuli downstream predictor spherical Gaussian distribution canonical choice unstructured RFNs simple way evaluate effective kernel structured RFNs k struct x x k unstruct x x see Appendix expression structured kernel provides concrete connection kernel theory learning using nonlinear neural networks readers interested kernel theories full example simulation results work given Appendix show exponential reduction number samples needed learn frequency detection using structured versus unstructured basis Fig Appendix Structured weights project filter input covariance eigenbasis generate network weights Gaussian processes GP whose covariance functions inspired receptive fields sensory neurons brain definition GP stochastic process finite observations follow Gaussian distribution find networks weights project inputs new basis filter irrelevant components see adds inductive bias classical RFNs tasks naturalistic inputs improves learningWe view weight vector w finitedimensional discretization continuous function w sample GP continuous function domain compact subset R assume discretized using grid equally spaced points w w Let input realvalued function x domain could represent finite timeseries image luminance retina complicated spatiotemporal sets like movie continuous setting dimensional inner product w x w x gets replaced L inner product w x w x wwwtTR DR DR DRDTdttddTwiiwtiixtTDDDdw x w x iw x w x iw x w x iw TwTx di idw iwix ixiLTwxt TtTwtxttEvery GP fully specified mean covariance function C always assume mean zero study different covariance functions KosambiKarhunenLove theorem realization zeromean GP random series representation Cttw z w z w z w z w z w z w z w iz izi ii ii terms standard Gaussian random variables z N functions weights pairs eigenvalue eigenfunction pairs covariance operator C L L z N z N z N z iziN iitii iiC L L C L L C L L CL L TL L C f C f C f C f C f C f C f C f C f C f C f C f C f C f C f Cf Tt TtTC tt tf tdt continuous analog covariance matrix C C positive definite opposed semidefinite eigenfunctions form complete basis L Using Eq inner product stimulus neurons weights CCCtt iiiLTEq w x z x z x z x x x w x z x z x z x x x w x z x z x z x x x w x z x z x z x x x w x z x z x z x x x w x z x z x z x x x w x z x z x z x x x w x wx iz izi ii iix iz izi ii x iix iz izix ix xiwherex ix xi ii x iix Eq shows structured weights compute projection input x onto eigenfunction x reweight filter eigenvalue taking inner product random Gaussian weights z Eq projectionxiixfilteriiziiIt illuminating see continuous equations look like dimensional discrete setting Samples finitedimensional GP used hidden weights RFNs w N C First GP series representation Eq becomes w z matrices eigenvalues eigenvectors z N Gaussian random vector definition covariance matrix C E w w equal steps linear algebra Finally Eq analogous w x z x Since orthogonal matrix x equivalent change basis diagonal matrix shrinks expands certain directions perform filtering summarized following theoremdw N C w N C w N C wNCEq ww zzz N z N z N zNI dIdC E w w C E w w C E w w CEww TwTTTEq wwTTxxzzTTTTxxTTxxTheorem Basis change formula Assume w N C C eigenvalue decomposition x R define Theorem Basis change formulaAssume w N C w N C w N C wNCwithCCTTits eigenvalue decomposition x R x R dx R dxR dRddefinex x x x x x x x x x x x x x x x TTxThen w x z x z N w x z x w x z x w x z x w TwTxz TzTx xfor z N z N z N zNI dIdTheorem says projecting input onto structured weight vector first filtering input GP eigenbasis random projection onto spherical random Gaussian form GP eigenbasis determined choice covariance function covariance function compatible input structure hidden weights filter irrelevant features noise stimuli amplifying descriptive features inductive bias facilitates inference stimuli downstream predictor spherical Gaussian distribution canonical choice unstructured RFNs simple way evaluate effective kernel structured RFNs k struct x x k unstruct x x see Appendix k struct x x k unstruct x x k struct x x k unstruct x x k struct x x k unstruct x x k structkstruct x x xx xk unstructkunstruct x x x xx x xS AppendixOur expression structured kernel provides concrete connection kernel theory learning using nonlinear neural networks readers interested kernel theories full example simulation results work given Appendix show exponential reduction number samples needed learn frequency detection using structured versus unstructured basis Fig Appendix AppendixS AppendixExamples random yet structured receptive fields goal model weights artificial neurons way inspired biological neurons receptive fields Structured RFNs sample hidden weights GPs structured covariance construct covariance functions make generated weights resemble neuronal receptive fields start toy example stationary GP wellunderstood Fourier eigenbasis show receptive fields generated GP selective frequencies timeseries signals construct locally stationary covariance models insect mechanosensory V neuron receptive fields models shown good match experimental data Warmup Frequency selectivity stationary covariance illustrate results theoretical analysis start toy example temporal receptive fields selective particular frequencies example may familiar readers comfortable Fourier series basic signal processing Let input finite continuous timeseries x interval L use covariance function C k k cos k stationary process k k L k th natural frequency k weight coefficients covariance function Eq stationary means depends difference timepoints Applying compound angle formula get C k k cos k cos k sin k sin k Since sinusoidal functions cos k sin k form orthonormal basis L Eq eigendecomposition covariance eigenfunctions sines cosines eigenvalues k Eq know structured weights covariance form random series w k k z k cos k z k sin k z k z k N Thus receptive fields made sinusoids weighted k Gaussian variables z k z k Suppose want receptive fields retain specific frequency information signal filter rest Take k k k f lo k f hi call bandlimited spectrum passband f lo f hi bandwidth f hi f lo bandwidth increases receptive fields become less smooth since made wider range frequencies k nonzero decay certain rate rate controls smoothness resulting GP receptive fields act input signals x implicitly transform inputs Fourier basis filter frequencies based magnitude k bandlimited setting frequencies outside passband filtered makes receptive fields selective particular range frequencies ignore others hand classical random features weight frequencies equally even though natural settings high frequency signals corrupted noise Insect mechanosensors next consider particular biological sensor sensitive timehistory forces Campaniform sensilla CS domeshaped mechanoreceptors detect local stress strain insect exoskeleton embedded cuticle deformation cuticle bending torsion induces depolarizing currents CS opening mechanosensitive ion channels CS encode proprioceptive information useful body state estimation movement control diverse tasks like walking kicking jumping flying model receptive fields CS believed critical flight control namely ones found base halteres wings Fig Halteres wings flap rhythmically flight rotations insects body induce torsional forces felt active sensory structures CS detect small strain forces thereby encoding angular velocity insect body Experimental results show haltere wing CS selective broad range oscillatory frequencies STAs smooth oscillatory selective frequency decay time Fig B Open separate window Fig Random receptive field model insect mechanosensors Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shown model temporal receptive fields locally stationary GP bandlimited spectrum Examples receptive fields generated GP shown Fig C inputs CS modeled finite continuous timeseries x finite interval L neuron weights generated covariance function C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum k k L k th natural frequency warmup frequency selectivity weights accounted parameters f lo f hi bandwidth f hi f lo increases receptive fields built wider selection frequencies makes receptive fields less smooth Fig field localized near decay determined parameter increases receptive field selective larger time windows eigenbasis covariance function Eq similar Fourier eigenbasis modulated decaying exponential eigenbasis orthonormal basis span k e cos k k e sin k nonorthogonal set functions L hidden weights transform timeseries inputs eigenbasis discard components outside passband frequencies f lo f hi fit covariance model receptive field data CS neurons wings hawkmoth Manduca sexta data Briefly CS receptive fields estimated spiketriggered average STA experimental mechanical stimuli wings stimuli generated bandpassed white noise Hz characterize receptive fields population CS neurons compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field number samples case kHz ms samples normalization sets overall scale covariance matrix data covariance matrix shows tridiagonal structure Fig main diagonal positive diagonals negative diagonals decay away top left matrix Open separate window Fig Spectral properties mechanosensory RFs model similar compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behavior fit covariance model data optimize parameters f lo f hi finding f lo Hz f hi Hz ms best fit sensilla data minimizing Frobenius norm difference C data model see Appendix resulting model covariance matrix Fig B matches data covariance matrix Fig remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig B Appendix simulate effect finite number neurons generate weight vectors equal number neurons recorded recompute model covariance matrix Fig C call finite neuron model covariance matrix C finite shows bump bloblike structures evident C data C model result suggests bumpy structures attributed small number recorded neurons hypothesize effects would disappear larger dataset C data would closely resemble C model comparison also calculate Frobenius difference null models unstructured covariance model Fourier model unstructured model Frobenius norm difference Fourier model sensilla covariance model much lower difference compared null models fitting data accurately show covariance matrices sampled receptive fields null models Fig C E Appendix Comparing eigenvectors eigenvalues data model covariance matrices find spectral properties C model C finite similar C data eigenvalue curves models match data quite well Fig E curves directly comparable covariance normalized trace makes sum eigenvalues unity data model covariance matrices lowdimensional first data eigenvectors explain variance top explain top eigenvectors model finite sample match data quite well Fig Primary visual cortex turn visually driven neurons mammalian primary cortex Primary visual cortex V earliest cortical area processing visual information Fig neurons V detect small changes visual features like orientations spatial frequencies contrast size Open separate window Fig Random receptive field model Primary Visual Cortex V Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fields model receptive fields simple cells V clear excitatory inhibitory regions light shone excitatory regions increase cells response viceversa Fig B shape regions determines orientation selectivity widths determine frequency selectivity receptive fields centered location visual field decay away integrate visual stimuli within small region center Gabor functions widely used mathematical model receptive fields simple cells model receptive fields using another locally stationary GP show examples generated receptive fields Fig C Consider inputs cortical cells continuous twodimensional image x domain L L x R Since image realvalued x grayscale contrast single color channel pixel values neuron weights generated covariance function following form C exp f smooth receptive fields exp c c localized center c receptive field center defined c size receptive field determined parameter increases receptive field extends farther center c Fig Spatial frequency selectivity accounted bandwidth parameter f f decreases spatial frequency receptive field goes making weights less smooth Fig E eigendecomposition covariance function Eq leads orthonormal basis single scale Hermite wavelets c wavelet eigenfunctions Hermite polynomials modulated decaying Gaussian k e c H k c k c k H k k th physicists Hermite polynomial eigenfunctions nonzero centers c shifted versions Eq detailed derivation values constants c c c normalization Appendix use Eq model receptive field data V neurons recorded calcium imaging transgenic mice expressing GCaMPs mice headfixed running airfloating ball presented unique white noise images pixels using Psychtoolbox pixels white black equal probability Images upsampled resolution screens via bilinear interpolation stimulus corrected eyemovements online using custom code responses cells collected using twophoton mesoscope preprocessed using Suitep Receptive fields calculated white noise images deconvolved calcium responses cells using STA covariance analysis picked cells signaltonoise SNR threshold gave us cells SNR calculated smaller set images presented twice using method preprocessing step moved center mass every receptive field center visual field compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field case pixels pixels data covariance matrix resembles tridiagonal matrix However diagonals nonzero equally spaced segments Additionally values decay away center matrix show C data zoomed nonzero region around center matrix Fig corresponds pixel region around center full pixel matrix full covariance matrix shown Fig F Appendix Open separate window Fig Spectral properties V RFs model similar compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons red covariance model number offdiagonals center rate decay away center determined parameters f c respectively covariance pixels decays function distance c leads equallyspaced nonzero segments hand covariance also decays function distance pixels brings diagonal structure model frequency parameter f increases number offdiagonals increases Pixels generated weights become correlated weights become spatially smoother size parameter increases diagonals decay slower center c increasing correlations center pixel leading significant weights occupy visual field optimize parameters fit data finding f pixels minimizing Frobenius norm difference C data model need optimize center parameter c since preprocess data receptive fields centered c center grid resulting model covariance matrix Fig B data covariance matrix Fig match remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig G Appendix simulate effect finite number neurons generate weights equal number neurons data compute C finite shown Fig C finite matrix C finite looks even like C data shows negative covariances far center result finite sample size comparison also calculate normalized Frobenius difference null models unstructured covariance model translation invariant V model translation invariant model remove spatially localizing exponential Eq fit spatial frequency parameter f unstructured model Frobenius norm difference translation invariant model V covariance model much lower difference better fit data show covariance matrices sampled receptive fields null models Fig H J Appendix Similar spectral properties evident eigenvectors eigenvalues C model C finite C data analytical forms derived Eq Fig E covariances normalized unit trace Note analytical eigenfunctions shown finer grid model data analysis performed continuous space differences eigenfunctions eigenvalues analytical model results due discretization Examining eigenvectors Fig also see good match although rotations differences ordering eigenvectors explain variance receptive field data reference top eigenvectors explain variance data variance model eigenvalue curves models analytical forms match data Fig E reasonably well although well mechanosensors Appendix repeat analysis receptive fields measured different stimulus sets mouse different experimental dataset nonhuman primate V findings consistent results shown Fig K P Appendix Examples random yet structured receptive fieldsOur goal model weights artificial neurons way inspired biological neurons receptive fields Structured RFNs sample hidden weights GPs structured covariance construct covariance functions make generated weights resemble neuronal receptive fields start toy example stationary GP wellunderstood Fourier eigenbasis show receptive fields generated GP selective frequencies timeseries signals construct locally stationary covariance models insect mechanosensory V neuron receptive fields models shown good match experimental dataWarmup Frequency selectivity stationary covariance illustrate results theoretical analysis start toy example temporal receptive fields selective particular frequencies example may familiar readers comfortable Fourier series basic signal processing Let input finite continuous timeseries x interval L use covariance function C k k cos k stationary process k k L k th natural frequency k weight coefficients covariance function Eq stationary means depends difference timepoints Applying compound angle formula get C k k cos k cos k sin k sin k Since sinusoidal functions cos k sin k form orthonormal basis L Eq eigendecomposition covariance eigenfunctions sines cosines eigenvalues k Eq know structured weights covariance form random series w k k z k cos k z k sin k z k z k N Thus receptive fields made sinusoids weighted k Gaussian variables z k z k Suppose want receptive fields retain specific frequency information signal filter rest Take k k k f lo k f hi call bandlimited spectrum passband f lo f hi bandwidth f hi f lo bandwidth increases receptive fields become less smooth since made wider range frequencies k nonzero decay certain rate rate controls smoothness resulting GP receptive fields act input signals x implicitly transform inputs Fourier basis filter frequencies based magnitude k bandlimited setting frequencies outside passband filtered makes receptive fields selective particular range frequencies ignore others hand classical random features weight frequencies equally even though natural settings high frequency signals corrupted noiseWarmup Frequency selectivity stationary covariance illustrate results theoretical analysis start toy example temporal receptive fields selective particular frequencies example may familiar readers comfortable Fourier series basic signal processing Let input finite continuous timeseries x interval L use covariance function xtTLC k k cos k stationary process C k k cos k stationary process C k k cos k stationary processC k k cos k stationary processC k k cos k stationary processC C Ctt k k cos k stationary process k k cos k stationary process k k cos k stationary process k k cos k k k cos k k k k k kcos kk tt tstationary processstationaryprocess k k L k th natural frequency k weight coefficients covariance function Eq stationary means depends difference timepoints Applying compound angle formula get kkkLk k k k kEq stationaryttC k k cos k cos k sin k sin k C k k cos k cos k sin k sin k C k k cos k cos k sin k sin k C k k cos k cos k sin k sin k C k k cos k cos k sin k sin k C k k cos k cos k sin k sin k C k k cos k cos k sin k sin k C tt k k k k kcos k kktcos k kkt tsin k kktsin k kkt tSince sinusoidal functions cos k sin k form orthonormal basis L Eq eigendecomposition covariance eigenfunctions sines cosines eigenvalues k Eq know structured weights covariance form random series kktkktLTEq k k k kEq w k k z k cos k z k sin k w k k z k cos k z k sin k w k k z k cos k z k sin k w k k z k cos k z k sin k w k k z k cos k z k sin k w k k z k cos k z k sin k w k k z k cos k z k sin k w k k k kkz kzkcos k kktz k zksin k kkt z k z k N Thus receptive fields made sinusoids weighted k Gaussian variables z k z k z k z k N z k z k N z k z k N z kzkz k zkN kkz k z k z k z k z k z k z kzkz k zkSuppose want receptive fields retain specific frequency information signal filter rest Take k k k f lo k f hi call bandlimited spectrum passband f lo f hi bandwidth f hi f lo bandwidth increases receptive fields become less smooth since made wider range frequencies k nonzero decay certain rate rate controls smoothness resulting GP kkkkkflokkfhibandlimitedflofhifhiflokkWhen receptive fields act input signals x implicitly transform inputs Fourier basis filter frequencies based magnitude k bandlimited setting frequencies outside passband filtered makes receptive fields selective particular range frequencies ignore others hand classical random features weight frequencies equally even though natural settings high frequency signals corrupted noisextkkInsect mechanosensors next consider particular biological sensor sensitive timehistory forces Campaniform sensilla CS domeshaped mechanoreceptors detect local stress strain insect exoskeleton embedded cuticle deformation cuticle bending torsion induces depolarizing currents CS opening mechanosensitive ion channels CS encode proprioceptive information useful body state estimation movement control diverse tasks like walking kicking jumping flying model receptive fields CS believed critical flight control namely ones found base halteres wings Fig Halteres wings flap rhythmically flight rotations insects body induce torsional forces felt active sensory structures CS detect small strain forces thereby encoding angular velocity insect body Experimental results show haltere wing CS selective broad range oscillatory frequencies STAs smooth oscillatory selective frequency decay time Fig B Open separate window Fig Random receptive field model insect mechanosensors Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shown model temporal receptive fields locally stationary GP bandlimited spectrum Examples receptive fields generated GP shown Fig C inputs CS modeled finite continuous timeseries x finite interval L neuron weights generated covariance function C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum k k L k th natural frequency warmup frequency selectivity weights accounted parameters f lo f hi bandwidth f hi f lo increases receptive fields built wider selection frequencies makes receptive fields less smooth Fig field localized near decay determined parameter increases receptive field selective larger time windows eigenbasis covariance function Eq similar Fourier eigenbasis modulated decaying exponential eigenbasis orthonormal basis span k e cos k k e sin k nonorthogonal set functions L hidden weights transform timeseries inputs eigenbasis discard components outside passband frequencies f lo f hi fit covariance model receptive field data CS neurons wings hawkmoth Manduca sexta data Briefly CS receptive fields estimated spiketriggered average STA experimental mechanical stimuli wings stimuli generated bandpassed white noise Hz characterize receptive fields population CS neurons compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field number samples case kHz ms samples normalization sets overall scale covariance matrix data covariance matrix shows tridiagonal structure Fig main diagonal positive diagonals negative diagonals decay away top left matrix Open separate window Fig Spectral properties mechanosensory RFs model similar compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behavior fit covariance model data optimize parameters f lo f hi finding f lo Hz f hi Hz ms best fit sensilla data minimizing Frobenius norm difference C data model see Appendix resulting model covariance matrix Fig B matches data covariance matrix Fig remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig B Appendix simulate effect finite number neurons generate weight vectors equal number neurons recorded recompute model covariance matrix Fig C call finite neuron model covariance matrix C finite shows bump bloblike structures evident C data C model result suggests bumpy structures attributed small number recorded neurons hypothesize effects would disappear larger dataset C data would closely resemble C model comparison also calculate Frobenius difference null models unstructured covariance model Fourier model unstructured model Frobenius norm difference Fourier model sensilla covariance model much lower difference compared null models fitting data accurately show covariance matrices sampled receptive fields null models Fig C E Appendix Comparing eigenvectors eigenvalues data model covariance matrices find spectral properties C model C finite similar C data eigenvalue curves models match data quite well Fig E curves directly comparable covariance normalized trace makes sum eigenvalues unity data model covariance matrices lowdimensional first data eigenvectors explain variance top explain top eigenvectors model finite sample match data quite well Fig Insect mechanosensors next consider particular biological sensor sensitive timehistory forces Campaniform sensilla CS domeshaped mechanoreceptors detect local stress strain insect exoskeleton embedded cuticle deformation cuticle bending torsion induces depolarizing currents CS opening mechanosensitive ion channels CS encode proprioceptive information useful body state estimation movement control diverse tasks like walking kicking jumping flying model receptive fields CS believed critical flight control namely ones found base halteres wings Fig Halteres wings flap rhythmically flight rotations insects body induce torsional forces felt active sensory structures CS detect small strain forces thereby encoding angular velocity insect body Experimental results show haltere wing CS selective broad range oscillatory frequencies STAs smooth oscillatory selective frequency decay time Fig B Fig AFig AFig BFig BOpen separate window Fig Random receptive field model insect mechanosensors Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shownOpen separate windowOpen separate windowOpen separate windowFig Random receptive field model insect mechanosensors Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shownFig Fig Random receptive field model insect mechanosensors Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shownRandom receptive field model insect mechanosensorsA Diagram cranefly Tipula hespera Locations mechanosensors campaniform sensilla marked blue wings halteres B Two receptive fields campaniform sensilla shown blue smooth oscillatory decay time model random samples distributions parameterized frequency decay parameters Data hawkmoth cranefly sensilla similar responses C Two random samples model distribution shown red smoothness receptive fields controlled frequency parameter decay parameter controls rate decay origin shownTipula hesperaWe model temporal receptive fields locally stationary GP bandlimited spectrum Examples receptive fields generated GP shown Fig C inputs CS modeled finite continuous timeseries x finite interval L neuron weights generated covariance function Fig CFig CxtTLC exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum C exp localized k k cos k stationary process k f lo k f hi otherwise bandlimited flatpower spectrum C tt texp localizedexp exp exp tt tlocalized k k cos k stationary process k k cos k k k cos k k k k k kcos kk tt tstationary processstationaryprocess kk f lo k f hi otherwise bandlimited flatpower spectrum f lo k f hi otherwise f lo k f hi otherwise f lo k f hi otherwise f lo k f hif lo k f hif lo k f hif loflo kkf hifhi otherwiseotherwiseotherwisebandlimited flatpower spectrumbandlimitedflatpowerspectrum k k L k th natural frequency warmup frequency selectivity weights accounted parameters f lo f hi bandwidth f hi f lo increases receptive fields built wider selection frequencies makes receptive fields less smooth Fig field localized near decay determined parameter increases receptive field selective larger time windowskkkLkflofhifhifloFig DFig DttThe eigenbasis covariance function Eq similar Fourier eigenbasis modulated decaying exponential eigenbasis orthonormal basis span k e cos k k e sin k nonorthogonal set functions L hidden weights transform timeseries inputs eigenbasis discard components outside passband frequencies f lo f hi Eq kke tkktkke tkktLTflofhiWe fit covariance model receptive field data CS neurons wings hawkmoth Manduca sexta data Briefly CS receptive fields estimated spiketriggered average STA experimental mechanical stimuli wings stimuli generated bandpassed white noise HzManduca sextaTo characterize receptive fields population CS neurons compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field number samples case kHz ms samples normalization sets overall scale covariance matrix data covariance matrix shows tridiagonal structure Fig main diagonal positive diagonals negative diagonals decay away top left matrixCCdataFig AFig AOpen separate window Fig Spectral properties mechanosensory RFs model similar compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behaviorOpen separate windowOpen separate windowOpen separate windowFig Spectral properties mechanosensory RFs model similar compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behaviorFig Fig Spectral properties mechanosensory RFs model similar compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behaviorSpectral properties mechanosensory RFs model similarWe compare covariance matrices generated receptive fields mechanosensors B model Eq C random samples model covariance matrices show tridiagonal structure decays away origin first five principal components three covariance matrices similar explain variance RF data E leading eigenvalue spectra data models show similar behaviorEq fit covariance model data optimize parameters f lo f hi finding f lo Hz f hi Hz ms best fit sensilla data minimizing Frobenius norm difference C data model see Appendix resulting model covariance matrix Fig B matches data covariance matrix Fig remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig B Appendix simulate effect finite number neurons generate weight vectors equal number neurons recorded recompute model covariance matrix Fig C call finite neuron model covariance matrix C finite shows bump bloblike structures evident C data C model result suggests bumpy structures attributed small number recorded neurons hypothesize effects would disappear larger dataset C data would closely resemble C model flofhiflofhiCCdataS AppendixFig BFig BFig AFig ACCdataS AppendixFig CFig CCCfiniteCCdataCCmodelCCdataCCmodelFor comparison also calculate Frobenius difference null models unstructured covariance model Fourier model unstructured model Frobenius norm difference Fourier model sensilla covariance model much lower difference compared null models fitting data accurately show covariance matrices sampled receptive fields null models Fig C E Appendix AppendixComparing eigenvectors eigenvalues data model covariance matrices find spectral properties C model C finite similar C data eigenvalue curves models match data quite well Fig E curves directly comparable covariance normalized trace makes sum eigenvalues unity data model covariance matrices lowdimensional first data eigenvectors explain variance top explain top eigenvectors model finite sample match data quite well Fig CCmodelCCfiniteCCdataFig EFig EFig DFig DPrimary visual cortex turn visually driven neurons mammalian primary cortex Primary visual cortex V earliest cortical area processing visual information Fig neurons V detect small changes visual features like orientations spatial frequencies contrast size Open separate window Fig Random receptive field model Primary Visual Cortex V Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fields model receptive fields simple cells V clear excitatory inhibitory regions light shone excitatory regions increase cells response viceversa Fig B shape regions determines orientation selectivity widths determine frequency selectivity receptive fields centered location visual field decay away integrate visual stimuli within small region center Gabor functions widely used mathematical model receptive fields simple cells model receptive fields using another locally stationary GP show examples generated receptive fields Fig C Consider inputs cortical cells continuous twodimensional image x domain L L x R Since image realvalued x grayscale contrast single color channel pixel values neuron weights generated covariance function following form C exp f smooth receptive fields exp c c localized center c receptive field center defined c size receptive field determined parameter increases receptive field extends farther center c Fig Spatial frequency selectivity accounted bandwidth parameter f f decreases spatial frequency receptive field goes making weights less smooth Fig E eigendecomposition covariance function Eq leads orthonormal basis single scale Hermite wavelets c wavelet eigenfunctions Hermite polynomials modulated decaying Gaussian k e c H k c k c k H k k th physicists Hermite polynomial eigenfunctions nonzero centers c shifted versions Eq detailed derivation values constants c c c normalization Appendix use Eq model receptive field data V neurons recorded calcium imaging transgenic mice expressing GCaMPs mice headfixed running airfloating ball presented unique white noise images pixels using Psychtoolbox pixels white black equal probability Images upsampled resolution screens via bilinear interpolation stimulus corrected eyemovements online using custom code responses cells collected using twophoton mesoscope preprocessed using Suitep Receptive fields calculated white noise images deconvolved calcium responses cells using STA covariance analysis picked cells signaltonoise SNR threshold gave us cells SNR calculated smaller set images presented twice using method preprocessing step moved center mass every receptive field center visual field compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field case pixels pixels data covariance matrix resembles tridiagonal matrix However diagonals nonzero equally spaced segments Additionally values decay away center matrix show C data zoomed nonzero region around center matrix Fig corresponds pixel region around center full pixel matrix full covariance matrix shown Fig F Appendix Open separate window Fig Spectral properties V RFs model similar compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons red covariance model number offdiagonals center rate decay away center determined parameters f c respectively covariance pixels decays function distance c leads equallyspaced nonzero segments hand covariance also decays function distance pixels brings diagonal structure model frequency parameter f increases number offdiagonals increases Pixels generated weights become correlated weights become spatially smoother size parameter increases diagonals decay slower center c increasing correlations center pixel leading significant weights occupy visual field optimize parameters fit data finding f pixels minimizing Frobenius norm difference C data model need optimize center parameter c since preprocess data receptive fields centered c center grid resulting model covariance matrix Fig B data covariance matrix Fig match remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig G Appendix simulate effect finite number neurons generate weights equal number neurons data compute C finite shown Fig C finite matrix C finite looks even like C data shows negative covariances far center result finite sample size comparison also calculate normalized Frobenius difference null models unstructured covariance model translation invariant V model translation invariant model remove spatially localizing exponential Eq fit spatial frequency parameter f unstructured model Frobenius norm difference translation invariant model V covariance model much lower difference better fit data show covariance matrices sampled receptive fields null models Fig H J Appendix Similar spectral properties evident eigenvectors eigenvalues C model C finite C data analytical forms derived Eq Fig E covariances normalized unit trace Note analytical eigenfunctions shown finer grid model data analysis performed continuous space differences eigenfunctions eigenvalues analytical model results due discretization Examining eigenvectors Fig also see good match although rotations differences ordering eigenvectors explain variance receptive field data reference top eigenvectors explain variance data variance model eigenvalue curves models analytical forms match data Fig E reasonably well although well mechanosensors Appendix repeat analysis receptive fields measured different stimulus sets mouse different experimental dataset nonhuman primate V findings consistent results shown Fig K P Appendix Primary visual cortex turn visually driven neurons mammalian primary cortex Primary visual cortex V earliest cortical area processing visual information Fig neurons V detect small changes visual features like orientations spatial frequencies contrast sizeFig AFig AOpen separate window Fig Random receptive field model Primary Visual Cortex V Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fieldsOpen separate windowOpen separate windowOpen separate windowFig Random receptive field model Primary Visual Cortex V Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fieldsFig Fig Random receptive field model Primary Visual Cortex V Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fieldsRandom receptive field model Primary Visual Cortex VA Diagram mouse brain V shown blue B Receptive fields two mouse V neurons calculated response white noise stimuli fields localized region visual field show regions C Random samples model Eq distribution Increasing receptive field size parameter model leads larger fields E Increasing model spatial frequency parameter leads variable fieldsEq model receptive fields simple cells V clear excitatory inhibitory regions light shone excitatory regions increase cells response viceversa Fig B shape regions determines orientation selectivity widths determine frequency selectivity receptive fields centered location visual field decay away integrate visual stimuli within small region center Gabor functions widely used mathematical model receptive fields simple cells Fig BFig BWe model receptive fields using another locally stationary GP show examples generated receptive fields Fig C Consider inputs cortical cells continuous twodimensional image x domain L L x R Since image realvalued x grayscale contrast single color channel pixel values neuron weights generated covariance function following form Fig CFig CxttTLLx Rx Rx RxTRxttC exp f smooth receptive fields exp c c localized center c C exp f smooth receptive fields exp c c localized center c C exp f smooth receptive fields exp c c localized center c C exp f smooth receptive fields exp c c localized center c C exp f smooth receptive fields exp c c localized center c C exp f smooth receptive fields exp c c localized center c C exp f smooth receptive fields exp c c localized center c C tt texp f smooth receptive fieldsexp f exp f exp f f f tt f f fsmooth receptive fieldssmoothreceptivefieldsexp c c localized center cexp c c exp c c exp c c c c c c c c tc c c tc slocalized center clocalizedtoacentercThe receptive field center defined c size receptive field determined parameter increases receptive field extends farther center c Fig Spatial frequency selectivity accounted bandwidth parameter f f decreases spatial frequency receptive field goes making weights less smooth Fig E ccssccFig DFig DffFig EFig EThe eigendecomposition covariance function Eq leads orthonormal basis single scale Hermite wavelets c wavelet eigenfunctions Hermite polynomials modulated decaying Gaussian Eq Hermite waveletscc k e c H k c k c k k e c H k c k c k k e c H k c k c k k e c H k c k c k k e c H k c k c k k e c H k c k c k k e c H k c k c k kk Di iDe c e c c ct itiH k iHk iki c c ct itiand k kk Di iDc k ick iki H k k th physicists Hermite polynomial eigenfunctions nonzero centers c shifted versions Eq detailed derivation values constants c c c normalization Appendix HkkkccEq cccS AppendixWe use Eq model receptive field data V neurons recorded calcium imaging transgenic mice expressing GCaMPs mice headfixed running airfloating ball presented unique white noise images pixels using Psychtoolbox pixels white black equal probability Images upsampled resolution screens via bilinear interpolation stimulus corrected eyemovements online using custom code responses cells collected using twophoton mesoscope preprocessed using Suitep Receptive fields calculated white noise images deconvolved calcium responses cells using STA covariance analysis picked cells signaltonoise SNR threshold gave us cells SNR calculated smaller set images presented twice using method preprocessing step moved center mass every receptive field center visual fieldEq compute data covariance matrix C data taking inner product receptive fields normalize trace dimension receptive field case pixels pixels data covariance matrix resembles tridiagonal matrix However diagonals nonzero equally spaced segments Additionally values decay away center matrix show C data zoomed nonzero region around center matrix Fig corresponds pixel region around center full pixel matrix full covariance matrix shown Fig F Appendix CCdataCCdataFig AFig AppendixOpen separate window Fig Spectral properties V RFs model similar compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons redOpen separate windowOpen separate windowOpen separate windowFig Spectral properties V RFs model similar compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons redFig Fig Spectral properties V RFs model similar compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons redSpectral properties V RFs model similarWe compare covariance matrices generated receptive fields mouse V neurons B GP model Eq C random samples model resemble tridiagonal matrix whose diagonals nonzero equallyspaced segments leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row differ model due discretization cases finite sampling neurons E eigenspectrum model matches well data staircase pattern model comes repeated eigenvalues frequency model curve infinite neurons black obscured model curve neurons redEq covariance model number offdiagonals center rate decay away center determined parameters f c respectively covariance pixels decays function distance c leads equallyspaced nonzero segments hand covariance also decays function distance pixels brings diagonal structure model frequency parameter f increases number offdiagonals increases Pixels generated weights become correlated weights become spatially smoother size parameter increases diagonals decay slower center c increasing correlations center pixel leading significant weights occupy visual fieldfsccccfsccWe optimize parameters fit data finding f pixels minimizing Frobenius norm difference C data model need optimize center parameter c since preprocess data receptive fields centered c center grid resulting model covariance matrix Fig B data covariance matrix Fig match remarkably well qualitatively normalized Frobenius norm difference C data model Examples biological receptive fields random samples fitted covariance model shown Fig G Appendix simulate effect finite number neurons generate weights equal number neurons data compute C finite shown Fig C finite matrix C finite looks even like C data shows negative covariances far center result finite sample size allsfCCdataccccFig BFig BFig AFig ACCdataS AppendixCCfiniteFig CFig CCCfiniteCCdataFor comparison also calculate normalized Frobenius difference null models unstructured covariance model translation invariant V model translation invariant model remove spatially localizing exponential Eq fit spatial frequency parameter f unstructured model Frobenius norm difference translation invariant model V covariance model much lower difference better fit data show covariance matrices sampled receptive fields null models Fig H J Appendix Eq fS AppendixSimilar spectral properties evident eigenvectors eigenvalues C model C finite C data analytical forms derived Eq Fig E covariances normalized unit trace Note analytical eigenfunctions shown finer grid model data analysis performed continuous space differences eigenfunctions eigenvalues analytical model results due discretization Examining eigenvectors Fig also see good match although rotations differences ordering eigenvectors explain variance receptive field data reference top eigenvectors explain variance data variance model eigenvalue curves models analytical forms match data Fig E reasonably well although well mechanosensors Appendix repeat analysis receptive fields measured different stimulus sets mouse different experimental dataset nonhuman primate V findings consistent results shown Fig K P Appendix CCmodelCCfinitefiniteCCdatadataEq Fig EFig EFig DFig DFig EFig ES AppendixS AppendixAdvantages structured random weights artificial learning tasks hypothesis neuronal inductive bias structured receptive fields allows networks learn fewer neurons training examples steps gradient descent classification tasks naturalistic inputs examine hypothesis compare performance structured receptive fields classical ones several classification tasks find artificial learning tasks structured random networks learn accurately smaller network sizes fewer training examples gradient steps Frequency detection CS naturally encode timehistory strain forces acting insect body sensors inspired temporal filtering properties shown accurately classify spatiotemporal data Inspired result test sensillainspired mechanosensory receptive fields timeseries classification task Fig top example presented network ms timeseries sampled kHz goal detect whether example contains sinusoidal signal positive examples sinusoidal signals f Hz corrupted noise SNR dB negative examples Gaussian white noise matched amplitude positive examples Note frequency detection task linearly separable random phases positive negative examples See Appendix additional details including definition SNR crossvalidation used find optimal parameters f lo Hz f hi Hz ms Open separate window Fig Random mechanosensory weights enable learning fewer neurons timeseries classification tasks show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible task number hidden neurons structured RFN significantly outperforms classical RFN show test performance using tuned parameters Fig Even noisy task achieves test error using hidden neurons Meanwhile classical network takes neurons achieve similar error Predictably performance suffers weights incompatible task show results f lo Hz f hi Hz Fig incompatible RFN performs better chance error much worse classical RFN takes neurons achieve test error test error decrease level even additional hidden neurons Frequency XOR task challenge mechanosensorinspired networks difficult task build frequency ExclusiveOR XOR problem Fig B top XOR binary function returns true inputs different otherwise returns false XOR classical example function linearly separable thus harder learn inputs ms timeseries sampled kHz inputs either contain pure frequency f Hz f Hz mixed frequency signals f f white noise pure mixed frequency cases add noise SNR See Appendix details goal task output true input contains either pure tone false input contains mixed frequencies white noise tune GP covariance parameters f lo f hi Eq using crossvalidation cross validation procedure algorithmic details identical frequency detection task Using cross validation find optimal parameters f lo Hz f hi Hz ms incompatible weights take f lo Hz f hi Hz structured RFN significantly outperform classical RFN number hidden neurons show network performance using parameters Fig B Classification error achieved hidden neurons sharp contrast classical RFN requires hidden neurons achieve error incompatible weights network needs neurons achieve test error improve larger network sizes four input subclasses consistently fails classify pure Hz sinusoidal signals outside passband Image classification next test Vinspired receptive fields two standard digit classification tasks MNIST KMNIST MNIST KMNIST datasets contain images handwritten digits MNIST Arabic numerals whereas KMNIST Japanese hiragana phonetic characters datasets come split training test examples classes training examples per class Every example grayscale image centered characters hidden weight center c chosen uniformly random pixels ensures networks weights uniformly cover image space fact means network represent sum locallysmooth functions see Appendix use network hidden neurons tune GP covariance parameters f Eq using fold cross validation MNIST training set parameter ranges pixels optimal parameters found grid search find optimal parameters pixels f pixels refit optimal model using entire training set parameters MNIST used KMNIST task without additional tuning Vinspired achieves much lower average classification error compared classical RFN number hidden neurons show learning performance using parameters MNIST task Fig achieve error MNIST task requires neurons versus neurons classical RFN structured RFN achieves error neurons Qualitatively similar results hold KMNIST task Fig B although overall errors larger reflecting harder task achieve error KMNIST requires neurons versus neurons classical RFN structured RFN achieves error neurons Open separate window Fig Random V weights enable learning fewer neurons fewer examples digit classification tasks show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properly network performance suffers GP covariance parameters match task happens size parameter smaller stroke width spatial scale f doesnt match stroke variations character Taking incompatible parameters f Fig B structured weights performs worse classical RFN tasks hidden neurons achieves relatively poor test errors MNIST Fig KMNIST Fig B Structured weights improve generalization limited data Alongside learning fewer hidden neurons V structured RFNs also learn accurately fewer examples test fewshot learning using image classification datasets training examples reduced training examples per class test set GP parameters remain Structured encodings allow learning fewer samples unstructured encodings show fewshot learning results Fig C networks performance saturate past hundred hidden neurons MNIST lowest error achieved V structured RFN versus classical RFN using incompatible weights Fig C structured network acheives error using structured features KMNIST task opposed classical RFN using incompatible weights Fig Networks train faster initialized structured weights study effect structured weights initialization strategy fullytrained neural networks weights network vary hypothesized structured initialization allows networks learn faster ie training loss test error would decrease faster unstructured weights shown performance RFNs improves biologically inspired weight sampling However RFNs Eq readout weights modified training hidden weights W frozen initial value compare biologicallymotivated initialization classical initialization variance inversely proportional number hidden neurons w unstruct N initialization widely known Kaiming normal scheme thought stabilize training dynamics controlling magnitude gradients classical approach ensures Tr fair comparison scale structured weight covariance matrix Tr C studies RFNs trace equal weight scale absorbed readout weights due homogeneity ReLU compare structured unstructured weights MNIST KMNIST tasks common benchmarks fullytrained networks architecture single hidden layer feedforward neural network Fig hidden neurons crossentropy loss training sets minimized using simple gradient descent GD epochs fair comparison learning rate optimized network separately define area training loss curve metric speed learning perform grid search range e e learning rate minimizes metric resulting parameters structured unstructured incompatible networks respectively parameters image classification MNIST KMNIST tasks Vinitialized network minimizes loss function faster classically initialized network MNIST task V network achieves loss value epochs compared network Fig see qualitatively similar results KMNIST task end training Vinspired networks loss classically initialized network reaches Fig B find Vinitialized network performs better classical initialization covariance parameters match task incompatible parameters Vinitialized network achieves loss value MNIST KMNIST Open separate window Fig V weight initialization fullytrained networks enables faster training digit classification tasks show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible task minimize training loss faster Vinitialized network also generalizes well achieves lower test error end training MNIST achieves test error compared error classically initialized network using incompatible weights Fig C KMNIST see error compared error classical initalization using incompatible weights Fig see similar results across diverse hidden layer widths learning rates Fig Q Appendix benefits evident wider networks smaller learning rates Furthermore structured weights show similar results trained epochs rate neurons shown optimizers like minibatch Stochastic Gradient Descent SGD ADAM batch size rate neurons shown Structured initialization facilitates learning across wide range networks However improvement universal significant benefit found initializing early convolutional layers deep network AlexNet applying ImageNet dataset shown Appendix Fig U Appendix large amounts training data fact small fraction network initialized structured weights could explain null result Also many scenarios incompatible structured weights get performance par compatible ones end training poor inductive bias overcome Improving representation structured random weights shown structured receptive field weights improve performance RFNs fullytrained networks number supervised learning tasks long receptive fields compatible task performance gains unstructured features possible incompatible networks performs better even worse using classical unstructured weights results understood theoretical framework Structured weights effectively cause input x undergo linear transformation new representation x following Theorem examples new representation bandlimited due design covariance function V weights eigenvalues nonzero spectrum decays exponentially acts lowpass filter moving bandlimited representation filter noisehighfrequency componentsand reduce dimensionalitycoordinates x outside passband zero general noise dimensionality make learning harder easiest understand effects frequency detection task simplicity assume using stationary features warmup frequency detection task signal power contained f Hz frequency everything else due noise weights compatible task means w sum sines cosines frequencies k passband includes f narrower make bandwidth still retaining signal higher SNR x becomes since noise filtered see Appendix Advantages structured random weights artificial learning tasksOur hypothesis neuronal inductive bias structured receptive fields allows networks learn fewer neurons training examples steps gradient descent classification tasks naturalistic inputs examine hypothesis compare performance structured receptive fields classical ones several classification tasks find artificial learning tasks structured random networks learn accurately smaller network sizes fewer training examples gradient stepsFrequency detection CS naturally encode timehistory strain forces acting insect body sensors inspired temporal filtering properties shown accurately classify spatiotemporal data Inspired result test sensillainspired mechanosensory receptive fields timeseries classification task Fig top example presented network ms timeseries sampled kHz goal detect whether example contains sinusoidal signal positive examples sinusoidal signals f Hz corrupted noise SNR dB negative examples Gaussian white noise matched amplitude positive examples Note frequency detection task linearly separable random phases positive negative examples See Appendix additional details including definition SNR crossvalidation used find optimal parameters f lo Hz f hi Hz ms Open separate window Fig Random mechanosensory weights enable learning fewer neurons timeseries classification tasks show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible task number hidden neurons structured RFN significantly outperforms classical RFN show test performance using tuned parameters Fig Even noisy task achieves test error using hidden neurons Meanwhile classical network takes neurons achieve similar error Predictably performance suffers weights incompatible task show results f lo Hz f hi Hz Fig incompatible RFN performs better chance error much worse classical RFN takes neurons achieve test error test error decrease level even additional hidden neuronsFrequency detection CS naturally encode timehistory strain forces acting insect body sensors inspired temporal filtering properties shown accurately classify spatiotemporal data Inspired result test sensillainspired mechanosensory receptive fields timeseries classification task Fig top example presented network ms timeseries sampled kHz goal detect whether example contains sinusoidal signal positive examples sinusoidal signals f Hz corrupted noise SNR dB negative examples Gaussian white noise matched amplitude positive examples Note frequency detection task linearly separable random phases positive negative examples See Appendix additional details including definition SNR crossvalidation used find optimal parameters f lo Hz f hi Hz msFig AFig AdfS AppendixflofhiOpen separate window Fig Random mechanosensory weights enable learning fewer neurons timeseries classification tasks show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible taskOpen separate windowOpen separate windowOpen separate windowFig Random mechanosensory weights enable learning fewer neurons timeseries classification tasks show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible taskFig Fig Random mechanosensory weights enable learning fewer neurons timeseries classification tasks show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible taskRandom mechanosensory weights enable learning fewer neurons timeseries classification tasksWe show test error random feature networks mechanosensory classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error error curves solid lines show average test error shaded regions represent standard error across five generations random network top row shows timeseries tasks networks tested top frequency detection task f Hz frequency signal purple separated white noise black B top frequency XOR task f Hz purple f Hz light purple signals separated white noise black mixtures Hz Hz gray covariance parameters tuned properly mechanosensorinspired networks achieve lower error using fewer hidden neurons frequency detection bottom frequency XOR B bottom tasks However performance bioinspired networks suffer weights incompatible taskfffFor number hidden neurons structured RFN significantly outperforms classical RFN show test performance using tuned parameters Fig Even noisy task achieves test error using hidden neurons Meanwhile classical network takes neurons achieve similar errorFig AFig APredictably performance suffers weights incompatible task show results f lo Hz f hi Hz Fig incompatible RFN performs better chance error much worse classical RFN takes neurons achieve test error test error decrease level even additional hidden neuronsincompatibleflofhiFig AFig AFrequency XOR task challenge mechanosensorinspired networks difficult task build frequency ExclusiveOR XOR problem Fig B top XOR binary function returns true inputs different otherwise returns false XOR classical example function linearly separable thus harder learn inputs ms timeseries sampled kHz inputs either contain pure frequency f Hz f Hz mixed frequency signals f f white noise pure mixed frequency cases add noise SNR See Appendix details goal task output true input contains either pure tone false input contains mixed frequencies white noise tune GP covariance parameters f lo f hi Eq using crossvalidation cross validation procedure algorithmic details identical frequency detection task Using cross validation find optimal parameters f lo Hz f hi Hz ms incompatible weights take f lo Hz f hi Hz structured RFN significantly outperform classical RFN number hidden neurons show network performance using parameters Fig B Classification error achieved hidden neurons sharp contrast classical RFN requires hidden neurons achieve error incompatible weights network needs neurons achieve test error improve larger network sizes four input subclasses consistently fails classify pure Hz sinusoidal signals outside passbandFrequency XOR task challenge mechanosensorinspired networks difficult task build frequency ExclusiveOR XOR problem Fig B top XOR binary function returns true inputs different otherwise returns false XOR classical example function linearly separable thus harder learn inputs ms timeseries sampled kHz inputs either contain pure frequency f Hz f Hz mixed frequency signals f f white noise pure mixed frequency cases add noise SNR See Appendix details goal task output true input contains either pure tone false input contains mixed frequencies white noiseFig BFig BffffS AppendixWe tune GP covariance parameters f lo f hi Eq using crossvalidation cross validation procedure algorithmic details identical frequency detection task Using cross validation find optimal parameters f lo Hz f hi Hz ms incompatible weights take f lo Hz f hi Hz flofhiEq flofhiflofhiThe structured RFN significantly outperform classical RFN number hidden neurons show network performance using parameters Fig B Classification error achieved hidden neurons sharp contrast classical RFN requires hidden neurons achieve error incompatible weights network needs neurons achieve test error improve larger network sizes four input subclasses consistently fails classify pure Hz sinusoidal signals outside passbandFig BFig BImage classification next test Vinspired receptive fields two standard digit classification tasks MNIST KMNIST MNIST KMNIST datasets contain images handwritten digits MNIST Arabic numerals whereas KMNIST Japanese hiragana phonetic characters datasets come split training test examples classes training examples per class Every example grayscale image centered characters hidden weight center c chosen uniformly random pixels ensures networks weights uniformly cover image space fact means network represent sum locallysmooth functions see Appendix use network hidden neurons tune GP covariance parameters f Eq using fold cross validation MNIST training set parameter ranges pixels optimal parameters found grid search find optimal parameters pixels f pixels refit optimal model using entire training set parameters MNIST used KMNIST task without additional tuning Vinspired achieves much lower average classification error compared classical RFN number hidden neurons show learning performance using parameters MNIST task Fig achieve error MNIST task requires neurons versus neurons classical RFN structured RFN achieves error neurons Qualitatively similar results hold KMNIST task Fig B although overall errors larger reflecting harder task achieve error KMNIST requires neurons versus neurons classical RFN structured RFN achieves error neurons Open separate window Fig Random V weights enable learning fewer neurons fewer examples digit classification tasks show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properly network performance suffers GP covariance parameters match task happens size parameter smaller stroke width spatial scale f doesnt match stroke variations character Taking incompatible parameters f Fig B structured weights performs worse classical RFN tasks hidden neurons achieves relatively poor test errors MNIST Fig KMNIST Fig B Image classification next test Vinspired receptive fields two standard digit classification tasks MNIST KMNIST MNIST KMNIST datasets contain images handwritten digits MNIST Arabic numerals whereas KMNIST Japanese hiragana phonetic characters datasets come split training test examples classes training examples per class Every example grayscale image centered charactershiraganaEach hidden weight center c chosen uniformly random pixels ensures networks weights uniformly cover image space fact means network represent sum locallysmooth functions see Appendix use network hidden neurons tune GP covariance parameters f Eq using fold cross validation MNIST training set parameter ranges pixels optimal parameters found grid search find optimal parameters pixels f pixels refit optimal model using entire training set parameters MNIST used KMNIST task without additional tuningccS AppendixsfEq sfThe Vinspired achieves much lower average classification error compared classical RFN number hidden neurons show learning performance using parameters MNIST task Fig achieve error MNIST task requires neurons versus neurons classical RFN structured RFN achieves error neurons Qualitatively similar results hold KMNIST task Fig B although overall errors larger reflecting harder task achieve error KMNIST requires neurons versus neurons classical RFN structured RFN achieves error neuronsFig AFig AFig BFig BOpen separate window Fig Random V weights enable learning fewer neurons fewer examples digit classification tasks show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properlyOpen separate windowOpen separate windowOpen separate windowFig Random V weights enable learning fewer neurons fewer examples digit classification tasks show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properlyFig Fig Random V weights enable learning fewer neurons fewer examples digit classification tasks show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properlyRandom V weights enable learning fewer neurons fewer examples digit classification tasksWe show average test error random feature networks V classical whitenoise weights number neurons hidden layer every hidden layer width generate five random networks average test error solid lines show average test error shaded regions represent standard error across five generations random network top row shows networks test error MNIST B KMNIST tasks covariance parameters tuned properly Vinspired networks achieve lower error using fewer hidden neurons tasks network performance deteriorates weights incompatible task C MNIST KMNIST samples per class V network still achieves lower error fewshot tasks parameters tuned properlyAgain network performance suffers GP covariance parameters match task happens size parameter smaller stroke width spatial scale f doesnt match stroke variations character Taking incompatible parameters f Fig B structured weights performs worse classical RFN tasks hidden neurons achieves relatively poor test errors MNIST Fig KMNIST Fig B sfsfFig BFig BFig AFig AFig BFig BStructured weights improve generalization limited data Alongside learning fewer hidden neurons V structured RFNs also learn accurately fewer examples test fewshot learning using image classification datasets training examples reduced training examples per class test set GP parameters remain Structured encodings allow learning fewer samples unstructured encodings show fewshot learning results Fig C networks performance saturate past hundred hidden neurons MNIST lowest error achieved V structured RFN versus classical RFN using incompatible weights Fig C structured network acheives error using structured features KMNIST task opposed classical RFN using incompatible weights Fig Structured weights improve generalization limited data Alongside learning fewer hidden neurons V structured RFNs also learn accurately fewer examples test fewshot learning using image classification datasets training examples reduced training examples per class test set GP parameters remain sameStructured encodings allow learning fewer samples unstructured encodings show fewshot learning results Fig C networks performance saturate past hundred hidden neurons MNIST lowest error achieved V structured RFN versus classical RFN using incompatible weights Fig C structured network acheives error using structured features KMNIST task opposed classical RFN using incompatible weights Fig Fig C DFig C DFig CFig CFig DFig DNetworks train faster initialized structured weights study effect structured weights initialization strategy fullytrained neural networks weights network vary hypothesized structured initialization allows networks learn faster ie training loss test error would decrease faster unstructured weights shown performance RFNs improves biologically inspired weight sampling However RFNs Eq readout weights modified training hidden weights W frozen initial value compare biologicallymotivated initialization classical initialization variance inversely proportional number hidden neurons w unstruct N initialization widely known Kaiming normal scheme thought stabilize training dynamics controlling magnitude gradients classical approach ensures Tr fair comparison scale structured weight covariance matrix Tr C studies RFNs trace equal weight scale absorbed readout weights due homogeneity ReLU compare structured unstructured weights MNIST KMNIST tasks common benchmarks fullytrained networks architecture single hidden layer feedforward neural network Fig hidden neurons crossentropy loss training sets minimized using simple gradient descent GD epochs fair comparison learning rate optimized network separately define area training loss curve metric speed learning perform grid search range e e learning rate minimizes metric resulting parameters structured unstructured incompatible networks respectively parameters image classification MNIST KMNIST tasks Vinitialized network minimizes loss function faster classically initialized network MNIST task V network achieves loss value epochs compared network Fig see qualitatively similar results KMNIST task end training Vinspired networks loss classically initialized network reaches Fig B find Vinitialized network performs better classical initialization covariance parameters match task incompatible parameters Vinitialized network achieves loss value MNIST KMNIST Open separate window Fig V weight initialization fullytrained networks enables faster training digit classification tasks show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible task minimize training loss faster Vinitialized network also generalizes well achieves lower test error end training MNIST achieves test error compared error classically initialized network using incompatible weights Fig C KMNIST see error compared error classical initalization using incompatible weights Fig see similar results across diverse hidden layer widths learning rates Fig Q Appendix benefits evident wider networks smaller learning rates Furthermore structured weights show similar results trained epochs rate neurons shown optimizers like minibatch Stochastic Gradient Descent SGD ADAM batch size rate neurons shown Structured initialization facilitates learning across wide range networks However improvement universal significant benefit found initializing early convolutional layers deep network AlexNet applying ImageNet dataset shown Appendix Fig U Appendix large amounts training data fact small fraction network initialized structured weights could explain null result Also many scenarios incompatible structured weights get performance par compatible ones end training poor inductive bias overcomeNetworks train faster initialized structured weights study effect structured weights initialization strategy fullytrained neural networks weights network vary hypothesized structured initialization allows networks learn faster ie training loss test error would decrease faster unstructured weights shown performance RFNs improves biologically inspired weight sampling However RFNs Eq readout weights modified training hidden weights W frozen initial valueEq WWWe compare biologicallymotivated initialization classical initialization variance inversely proportional number hidden neurons w unstruct N initialization widely known Kaiming normal scheme thought stabilize training dynamics controlling magnitude gradients classical approach ensures Tr fair comparison scale structured weight covariance matrix Tr C studies RFNs trace equal weight scale absorbed readout weights due homogeneity ReLUw unstruct N w unstruct N w unstruct N w unstructwunstructN ddITr Tr Tr Tr ddICCdWe compare structured unstructured weights MNIST KMNIST tasks common benchmarks fullytrained networks architecture single hidden layer feedforward neural network Fig hidden neurons crossentropy loss training sets minimized using simple gradient descent GD epochs fair comparison learning rate optimized network separately define area training loss curve metric speed learning perform grid search range e e learning rate minimizes metric resulting parameters structured unstructured incompatible networks respectively parameters image classificationFig Fig eeIn MNIST KMNIST tasks Vinitialized network minimizes loss function faster classically initialized network MNIST task V network achieves loss value epochs compared network Fig see qualitatively similar results KMNIST task end training Vinspired networks loss classically initialized network reaches Fig B find Vinitialized network performs better classical initialization covariance parameters match task incompatible parameters Vinitialized network achieves loss value MNIST KMNISTFig AFig AFig BFig BOpen separate window Fig V weight initialization fullytrained networks enables faster training digit classification tasks show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible taskOpen separate windowOpen separate windowOpen separate windowFig V weight initialization fullytrained networks enables faster training digit classification tasks show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible taskFig Fig V weight initialization fullytrained networks enables faster training digit classification tasks show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible taskV weight initialization fullytrained networks enables faster training digit classification tasksWe show average test error train loss fullytrained neural networks number training epochs hidden layer network contains neurons generate five random networks average performance solid lines show average performance metric across five random networks shaded regions represent standard error top row shows networks training loss MNIST B KMNIST tasks bottom row shows corresponding test error C MNIST KMNIST tasks covariance parameters tuned properly Vinitialized networks achieve lower training loss test error fewer epochs MNIST KMNIST tasks network performance better unstructured initialization weights incompatible taskNot minimize training loss faster Vinitialized network also generalizes well achieves lower test error end training MNIST achieves test error compared error classically initialized network using incompatible weights Fig C KMNIST see error compared error classical initalization using incompatible weights Fig Fig CFig CFig DFig DWe see similar results across diverse hidden layer widths learning rates Fig Q Appendix benefits evident wider networks smaller learning rates Furthermore structured weights show similar results trained epochs rate neurons shown optimizers like minibatch Stochastic Gradient Descent SGD ADAM batch size rate neurons shown Structured initialization facilitates learning across wide range networksS AppendixHowever improvement universal significant benefit found initializing early convolutional layers deep network AlexNet applying ImageNet dataset shown Appendix Fig U Appendix large amounts training data fact small fraction network initialized structured weights could explain null result Also many scenarios incompatible structured weights get performance par compatible ones end training poor inductive bias overcomeS AppendixS AppendixImproving representation structured random weights shown structured receptive field weights improve performance RFNs fullytrained networks number supervised learning tasks long receptive fields compatible task performance gains unstructured features possible incompatible networks performs better even worse using classical unstructured weights results understood theoretical framework Structured weights effectively cause input x undergo linear transformation new representation x following Theorem examples new representation bandlimited due design covariance function V weights eigenvalues nonzero spectrum decays exponentially acts lowpass filter moving bandlimited representation filter noisehighfrequency componentsand reduce dimensionalitycoordinates x outside passband zero general noise dimensionality make learning harder easiest understand effects frequency detection task simplicity assume using stationary features warmup frequency detection task signal power contained f Hz frequency everything else due noise weights compatible task means w sum sines cosines frequencies k passband includes f narrower make bandwidth still retaining signal higher SNR x becomes since noise filtered see Appendix Improving representation structured random weights shown structured receptive field weights improve performance RFNs fullytrained networks number supervised learning tasks long receptive fields compatible task performance gains unstructured features possible incompatible networks performs better even worse using classical unstructured weightsThese results understood theoretical framework Structured weights effectively cause input x undergo linear transformation new representation x following Theorem examples new representation bandlimited due design covariance function V weights eigenvalues nonzero spectrum decays exponentially acts lowpass filter moving bandlimited representation filter noisehighfrequency componentsand reduce dimensionalitycoordinates x outside passband zero general noise dimensionality make learning harderxxx x x xx x x xIt easiest understand effects frequency detection task simplicity assume using stationary features warmup frequency detection task signal power contained f Hz frequency everything else due noise weights compatible task means w sum sines cosines frequencies k passband includes f narrower make bandwidth still retaining signal higher SNR x becomes since noise filtered see Appendix fwwkkfx x x xS AppendixDiscussion paper describe random generative model receptive fields sensory neurons Specifically model receptive field random filter sampled Gaussian process GP covariance structure matched statistics experimental neural data show two kinds sensory neuronsinsect mechanosensory simple cells mammalian Vhave receptive fields welldescribed GPs particular generated receptive fields secondorder statistics principal components match receptive field data Theoretically show individual neurons perform randomized transformation filtering inputs connection provides framework sensory neurons compute input transformations like Fourier wavelet transforms biologically plausible way numerical results using structured random receptive fields show offer better learning performance unstructured receptive fields several benchmarks structured networks achieve higher test performance fewer neurons fewer training examples unless frequency content receptive fields incompatible task networks fully trained initializing structured weights leads better network performance measured training loss generalization fewer iterations gradient descent Structured random features may understood theoretically transforming inputs informative basis retains important information stimulus filtering away irrelevant signals Modeling sensory neurons modalities random feature formulation natural extension traditional linearnonlinear LN neuron model approach may applied brain regions LN models successful instance sensory areas primarily feedforward connectivity like somatosensory auditory regions neurons auditory somatosensory systems selective spatial temporal structures stimuli spatial structure emerges networks trained artificial tactile tasks receptive fields could modeled GPs spatiotemporal covariance functions could useful artificial tasks spatiotemporal stimuli movies multivariate timeseries Neurons localized random temporal responses found compatible manifold coding decisionmaking task GPs complementary approach traditional sparse coding efficient coding hypotheses connections theories interesting future research Receptive fields development generative model offers new directions explore biological basis computational principles behind receptive fields Development lays basic architecture conserved animal animal yet details every neural connection specified leading amount inevitable randomness least initially receptive fields random constrained covariance natural ask biology implements Unsupervised Hebbian dynamics local inhibition allow networks learn principal components input interesting future direction similar learning rules may give rise overcomplete nonorthogonal structure similar studied may prove biologically plausible weights result taskdriven optimization assumes receptive field properties actually lie within synaptic weights spatial receptive fields assumption plausible temporal properties receptive fields likely result neurons intrinsic dynamics LN framework model Heterogeneous physiological eg resonator dynamics mechanical position shape mechanosensor relative body structure properties combine give diverse temporal receptive field structures Development thus leverages different mechanisms build structure receptive field properties sensory neurons Connections compressive sensing Random projections seen extensive use field compressive sensing highdimensional signal found measurements long sparse representation Random compression matrices known optimal properties however many cases structured randomness realistic Recent work shown structured random projections local wiring constraints one dimension compatible dictionary learning supporting previous empirical results work shows structured random receptive fields equivalent employing wavelet dictionary dense Gaussian projection Machine learning inductive bias important open question neuroscience machine learning certain networks characterized features architecture weights nonlinearities better others certain problems One perspective network good problem biased towards approximating functions close target known inductive bias depends alignment features encoded neurons task hand approach shows structured receptive fields equivalent linear transformation input build biases Furthermore describe nonlinear properties network using kernel varies depending receptive field structure target function small norm RKHS inductive bias easier learn small norm RKHS means target function varies smoothly inputs Smooth functions easier learn compared fast varying ones way receptive field structure influences ease learning target function conjecture receptive fields neuralinspired distributions affect RKHS geometry target functions norm small RKHS compared RKHS random whitenoise receptive fields leave future work verify conjecture detail Networks endowed principles neural computation like batch normalization pooling inputs residual connections found contain inductive biases certain learning problems Learning datadependent kernels another way add inductive bias also saw initializing fullytrained networks generative models improved speed convergence generalization compared unstructured initialization result consistent known results initialization effect generalization initialization literature mostly focused avoiding explodingvanishing gradients conjecture inductive bias structured connectivity places network closer good solution loss landscape random Vinspired receptive fields model seen similar happens convolutional neural network CNN similarities differences compared brains recent study showed CNNs fixed Vlike convolutional layer robust adversarial perturbations inputs similar vein work using randomly sampled Gabor receptive fields first layer deep network also shown improve performance wavelet scattering transform multilayer network wavelet coefficients passed nonlinearities model similar deep CNNs framework differs randomized model yields wavelets single scale similar studies robustness learning deep networks weights possible Adding layers model sampling weights variety spatial frequencies field sizes would yield random networks behave similar scattering transform offering another connection brain CNNs Directly learning filters Hermite wavelet basis led good perfomance ANNs little data idea extended multiple scales structured random features seen RFN version ideas supporting evidence principles used biology Limitations future directions several limitations random feature approach model neuron responses scalar firing rates instead discrete spikes ignore complex neuronal dynamics neuromodulatory context many details Like LN models random feature model assumes zero plasticity hidden layer neurons However associative learning drive changes receptive fields individual neurons sensory areas like V auditory cortex RFN purely feedforward account feedback connections Recent work suggests feedforward architecture lacks sufficient computational power serve detailed inputoutput model network cortical neurons might need additional layers convolutional filters difficult interpret parameters found fitting receptive field data connect experimental conditions Also GP model weights captures covariance second moments neglects higherorder statistics remains shown theory yield concrete predictions tested vivo experimental conditions random feature receptive field model randomized extension LN neuron model LN model fits parameterized function receptive field contrast random feature framework fits distribution entire population receptive fields generates realistic receptive fields distribution natural question compare goal capture individual differences neuronal receptive fields one resort LN model neurons receptive field fit data random feature model flexible provides direct connection random feature theory mathematically tractable generative connection kernel learning opens door using techniques mainstay machine learning theory literature instance estimate generalization error sample complexity context learning biologically realistic networks see several future directions structured random features connecting computational neuroscience machine learning already stated auditory somatosensory tactile regions good candidates study well developmental principles could give rise random yet structured receptive field properties account plasticity hidden layer one could also analyze neural tangent kernel NTK associated structured features kernels often used analyze ANNs trained gradient descent number hidden neurons large step size small incorporate lateral feedback connections weights could sampled GPs recurrent covariance functions theory may also help explain CNNs fixed Vlike convolutional layer robust adversarial input perturbations filtering high frequency corruptions seems likely structured random features also robust would interesting study intermediate layer weights fullytrained networks approximate samples GP studying covariance structure Finally one could try develop covariance functions optimize RFNs sophisticated learning tasks see near high performancelower error faster training etcon difficult tasks possibleDiscussionIn paper describe random generative model receptive fields sensory neurons Specifically model receptive field random filter sampled Gaussian process GP covariance structure matched statistics experimental neural data show two kinds sensory neuronsinsect mechanosensory simple cells mammalian Vhave receptive fields welldescribed GPs particular generated receptive fields secondorder statistics principal components match receptive field data Theoretically show individual neurons perform randomized transformation filtering inputs connection provides framework sensory neurons compute input transformations like Fourier wavelet transforms biologically plausible wayOur numerical results using structured random receptive fields show offer better learning performance unstructured receptive fields several benchmarks structured networks achieve higher test performance fewer neurons fewer training examples unless frequency content receptive fields incompatible task networks fully trained initializing structured weights leads better network performance measured training loss generalization fewer iterations gradient descent Structured random features may understood theoretically transforming inputs informative basis retains important information stimulus filtering away irrelevant signalsModeling sensory neurons modalities random feature formulation natural extension traditional linearnonlinear LN neuron model approach may applied brain regions LN models successful instance sensory areas primarily feedforward connectivity like somatosensory auditory regions neurons auditory somatosensory systems selective spatial temporal structures stimuli spatial structure emerges networks trained artificial tactile tasks receptive fields could modeled GPs spatiotemporal covariance functions could useful artificial tasks spatiotemporal stimuli movies multivariate timeseries Neurons localized random temporal responses found compatible manifold coding decisionmaking task GPs complementary approach traditional sparse coding efficient coding hypotheses connections theories interesting future researchModeling sensory neurons modalitiesThe random feature formulation natural extension traditional linearnonlinear LN neuron model approach may applied brain regions LN models successful instance sensory areas primarily feedforward connectivity like somatosensory auditory regions neurons auditory somatosensory systems selective spatial temporal structures stimuli spatial structure emerges networks trained artificial tactile tasks receptive fields could modeled GPs spatiotemporal covariance functions could useful artificial tasks spatiotemporal stimuli movies multivariate timeseries Neurons localized random temporal responses found compatible manifold coding decisionmaking task GPs complementary approach traditional sparse coding efficient coding hypotheses connections theories interesting future researchReceptive fields development generative model offers new directions explore biological basis computational principles behind receptive fields Development lays basic architecture conserved animal animal yet details every neural connection specified leading amount inevitable randomness least initially receptive fields random constrained covariance natural ask biology implements Unsupervised Hebbian dynamics local inhibition allow networks learn principal components input interesting future direction similar learning rules may give rise overcomplete nonorthogonal structure similar studied may prove biologically plausible weights result taskdriven optimization assumes receptive field properties actually lie within synaptic weights spatial receptive fields assumption plausible temporal properties receptive fields likely result neurons intrinsic dynamics LN framework model Heterogeneous physiological eg resonator dynamics mechanical position shape mechanosensor relative body structure properties combine give diverse temporal receptive field structures Development thus leverages different mechanisms build structure receptive field properties sensory neuronsReceptive fields developmentOur generative model offers new directions explore biological basis computational principles behind receptive fields Development lays basic architecture conserved animal animal yet details every neural connection specified leading amount inevitable randomness least initially receptive fields random constrained covariance natural ask biology implements Unsupervised Hebbian dynamics local inhibition allow networks learn principal components input interesting future direction similar learning rules may give rise overcomplete nonorthogonal structure similar studied may prove biologically plausible weights result taskdriven optimizationThe assumes receptive field properties actually lie within synaptic weights spatial receptive fields assumption plausible temporal properties receptive fields likely result neurons intrinsic dynamics LN framework model Heterogeneous physiological eg resonator dynamics mechanical position shape mechanosensor relative body structure properties combine give diverse temporal receptive field structures Development thus leverages different mechanisms build structure receptive field properties sensory neuronsConnections compressive sensing Random projections seen extensive use field compressive sensing highdimensional signal found measurements long sparse representation Random compression matrices known optimal properties however many cases structured randomness realistic Recent work shown structured random projections local wiring constraints one dimension compatible dictionary learning supporting previous empirical results work shows structured random receptive fields equivalent employing wavelet dictionary dense Gaussian projectionConnections compressive sensingRandom projections seen extensive use field compressive sensing highdimensional signal found measurements long sparse representation Random compression matrices known optimal properties however many cases structured randomness realistic Recent work shown structured random projections local wiring constraints one dimension compatible dictionary learning supporting previous empirical results work shows structured random receptive fields equivalent employing wavelet dictionary dense Gaussian projectionMachine learning inductive bias important open question neuroscience machine learning certain networks characterized features architecture weights nonlinearities better others certain problems One perspective network good problem biased towards approximating functions close target known inductive bias depends alignment features encoded neurons task hand approach shows structured receptive fields equivalent linear transformation input build biases Furthermore describe nonlinear properties network using kernel varies depending receptive field structure target function small norm RKHS inductive bias easier learn small norm RKHS means target function varies smoothly inputs Smooth functions easier learn compared fast varying ones way receptive field structure influences ease learning target function conjecture receptive fields neuralinspired distributions affect RKHS geometry target functions norm small RKHS compared RKHS random whitenoise receptive fields leave future work verify conjecture detail Networks endowed principles neural computation like batch normalization pooling inputs residual connections found contain inductive biases certain learning problems Learning datadependent kernels another way add inductive bias also saw initializing fullytrained networks generative models improved speed convergence generalization compared unstructured initialization result consistent known results initialization effect generalization initialization literature mostly focused avoiding explodingvanishing gradients conjecture inductive bias structured connectivity places network closer good solution loss landscape random Vinspired receptive fields model seen similar happens convolutional neural network CNN similarities differences compared brains recent study showed CNNs fixed Vlike convolutional layer robust adversarial perturbations inputs similar vein work using randomly sampled Gabor receptive fields first layer deep network also shown improve performance wavelet scattering transform multilayer network wavelet coefficients passed nonlinearities model similar deep CNNs framework differs randomized model yields wavelets single scale similar studies robustness learning deep networks weights possible Adding layers model sampling weights variety spatial frequencies field sizes would yield random networks behave similar scattering transform offering another connection brain CNNs Directly learning filters Hermite wavelet basis led good perfomance ANNs little data idea extended multiple scales structured random features seen RFN version ideas supporting evidence principles used biologyMachine learning inductive biasAn important open question neuroscience machine learning certain networks characterized features architecture weights nonlinearities better others certain problems One perspective network good problem biased towards approximating functions close target known inductive bias depends alignment features encoded neurons task hand approach shows structured receptive fields equivalent linear transformation input build biases Furthermore describe nonlinear properties network using kernel varies depending receptive field structure target function small norm RKHS inductive bias easier learn small norm RKHS means target function varies smoothly inputs Smooth functions easier learn compared fast varying ones way receptive field structure influences ease learning target function conjecture receptive fields neuralinspired distributions affect RKHS geometry target functions norm small RKHS compared RKHS random whitenoise receptive fields leave future work verify conjecture detailinductive biasNetworks endowed principles neural computation like batch normalization pooling inputs residual connections found contain inductive biases certain learning problems Learning datadependent kernels another way add inductive bias also saw initializing fullytrained networks generative models improved speed convergence generalization compared unstructured initialization result consistent known results initialization effect generalization initialization literature mostly focused avoiding explodingvanishing gradients conjecture inductive bias structured connectivity places network closer good solution loss landscape random Vinspired receptive fields model seen similar happens convolutional neural network CNN similarities differences compared brains recent study showed CNNs fixed Vlike convolutional layer robust adversarial perturbations inputs similar vein work using randomly sampled Gabor receptive fields first layer deep network also shown improve performance wavelet scattering transform multilayer network wavelet coefficients passed nonlinearities model similar deep CNNs framework differs randomized model yields wavelets single scale similar studies robustness learning deep networks weights possible Adding layers model sampling weights variety spatial frequencies field sizes would yield random networks behave similar scattering transform offering another connection brain CNNs Directly learning filters Hermite wavelet basis led good perfomance ANNs little data idea extended multiple scales structured random features seen RFN version ideas supporting evidence principles used biologyLimitations future directions several limitations random feature approach model neuron responses scalar firing rates instead discrete spikes ignore complex neuronal dynamics neuromodulatory context many details Like LN models random feature model assumes zero plasticity hidden layer neurons However associative learning drive changes receptive fields individual neurons sensory areas like V auditory cortex RFN purely feedforward account feedback connections Recent work suggests feedforward architecture lacks sufficient computational power serve detailed inputoutput model network cortical neurons might need additional layers convolutional filters difficult interpret parameters found fitting receptive field data connect experimental conditions Also GP model weights captures covariance second moments neglects higherorder statistics remains shown theory yield concrete predictions tested vivo experimental conditions random feature receptive field model randomized extension LN neuron model LN model fits parameterized function receptive field contrast random feature framework fits distribution entire population receptive fields generates realistic receptive fields distribution natural question compare goal capture individual differences neuronal receptive fields one resort LN model neurons receptive field fit data random feature model flexible provides direct connection random feature theory mathematically tractable generative connection kernel learning opens door using techniques mainstay machine learning theory literature instance estimate generalization error sample complexity context learning biologically realistic networks see several future directions structured random features connecting computational neuroscience machine learning already stated auditory somatosensory tactile regions good candidates study well developmental principles could give rise random yet structured receptive field properties account plasticity hidden layer one could also analyze neural tangent kernel NTK associated structured features kernels often used analyze ANNs trained gradient descent number hidden neurons large step size small incorporate lateral feedback connections weights could sampled GPs recurrent covariance functions theory may also help explain CNNs fixed Vlike convolutional layer robust adversarial input perturbations filtering high frequency corruptions seems likely structured random features also robust would interesting study intermediate layer weights fullytrained networks approximate samples GP studying covariance structure Finally one could try develop covariance functions optimize RFNs sophisticated learning tasks see near high performancelower error faster training etcon difficult tasks possibleLimitations future directionsThere several limitations random feature approach model neuron responses scalar firing rates instead discrete spikes ignore complex neuronal dynamics neuromodulatory context many details Like LN models random feature model assumes zero plasticity hidden layer neurons However associative learning drive changes receptive fields individual neurons sensory areas like V auditory cortex RFN purely feedforward account feedback connections Recent work suggests feedforward architecture lacks sufficient computational power serve detailed inputoutput model network cortical neurons might need additional layers convolutional filters difficult interpret parameters found fitting receptive field data connect experimental conditions Also GP model weights captures covariance second moments neglects higherorder statistics remains shown theory yield concrete predictions tested vivo experimental conditionsin vivoThe random feature receptive field model randomized extension LN neuron model LN model fits parameterized function receptive field contrast random feature framework fits distribution entire population receptive fields generates realistic receptive fields distribution natural question compare goal capture individual differences neuronal receptive fields one resort LN model neurons receptive field fit data random feature model flexible provides direct connection random feature theory mathematically tractable generative connection kernel learning opens door using techniques mainstay machine learning theory literature instance estimate generalization error sample complexity context learning biologically realistic networksWe see several future directions structured random features connecting computational neuroscience machine learning already stated auditory somatosensory tactile regions good candidates study well developmental principles could give rise random yet structured receptive field properties account plasticity hidden layer one could also analyze neural tangent kernel NTK associated structured features kernels often used analyze ANNs trained gradient descent number hidden neurons large step size small incorporate lateral feedback connections weights could sampled GPs recurrent covariance functions theory may also help explain CNNs fixed Vlike convolutional layer robust adversarial input perturbations filtering high frequency corruptions seems likely structured random features also robust would interesting study intermediate layer weights fullytrained networks approximate samples GP studying covariance structure Finally one could try develop covariance functions optimize RFNs sophisticated learning tasks see near high performancelower error faster training etcon difficult tasks possibleMethods methods described throughout Results section details additional results Appendix MethodsThe methods described throughout Results section details additional results Appendix AppendixSupporting information Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF Click additional data file pdfSupporting informationS Appendix Additional theory supporting data Fig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phase PDF Click additional data file pdfS AppendixAdditional theory supporting dataFig Simulation results simplified frequency detection task left test error versus dataset size kernel ridge regression using structured kernels purple lines bandwidth unstructured kernel blue right test accuracy versus dataset size SVM classifier readout trained structured random features purple lines bandwidth unstructured random features blue cases task structural information improves performance leading less error Fig B Receptive fields mechanosensory neurons show biological receptive fields B random samples fitted covariance model Fig C Covariance matrix mechanosensory receptive fields unstructured model compare covariance matrices generated receptive fields mechanosensory neurons B unstructured GP model C random samples model Fig Covariance matrix mechanosensory receptive fields Fourier model compare covariance matrices generated receptive fields mechanosensory neurons B Fourier GP model C random samples model Fig E Receptive fields mechanosensory neurons unstructured model Fourier model show receptive fields mechanosensory neurons B unstructured GP model C Fourier GP model Fig F Covariance matrix V receptive fields model white noise stimuli show full structure covariance matrices pixel region around centers pixel matrices matrices generated receptive fields mouse V neurons B GP model Eq C random samples model Fig G Receptive fields V neurons white noise stimuli show biological receptive fields B random samples fitted covariance model Fig H Covariance matrix V receptive fields unstructured model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B unstructured GP model C random samples model Fig Covariance matrix V receptive fields translation invariant V model white noise stimuli compare covariance matrices generated receptive fields mice V neurons B translation invariant version V GP model C random samples model Fig J Receptive fields V neurons unstructured model translation invariant V model white noise stimuli show receptive fields V neurons B unstructured GP model C translation invariant V GP model Fig K Spectral properties V receptive fields model Ringach dataset compare covariance matrices generated receptive fields macaque V neurons B GP model Eq C random samples model data leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavlet eigenfunctions last row E eigenspectrum model matches well data Fig L Receptive fields V neurons Ringach dataset show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model natural image stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices show similar structure explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model compared data Fig N Receptive fields V neurons natural images stimuli show biological receptive fields B random samples fitted covariance model Fig Spectral properties V receptive fields model DHT stimuli compare covariance matrices generated receptive fields mice V neurons B GP model Eq C random samples model leading eigenvectors data model covariance matrices explain variance data Analytical Hermite wavelet eigenfunctions last row E eigenspectrum model matches well data Fig P Receptive fields V neurons DHT stimuli show biological receptive fields B random samples fitted covariance model Fig Q Training loss MNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded region represents standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig R Test error MNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Training loss KMNIST fullytrained neural networks initialized V weights show average training loss fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average training loss shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower training loss fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig Test error KMNIST fullytrained neural networks initialized V weights show average test error fullytrained networks number training epochs across diverse hidden layer widths learning rates every hidden layer width generate five random networks average performance solid lines show average test error shaded regions represent standard error covariance parameters tuned properly Vinitialized networks achieve lower test error fewer epochs benefits significant larger network widths lower learning rates incompatible weights V initialization leads similar performance unstructured initialization Fig U Initializing AlexNet using structured random features shows little benefit ImageNet Training testing loss shown classical structured random initializations convolutional layers AlexNet losses initially lower structured features epochs classical initialization catches eventually reaches slightly lower loss structured initialization Note training losses higher testing due dropout applied training phaseFig ASimulation results simplified frequency detection taskFig BReceptive fields mechanosensory neuronsFig CCovariance matrix mechanosensory receptive fields unstructured modelFig DCovariance matrix mechanosensory receptive fields Fourier modelFig EReceptive fields mechanosensory neurons unstructured model Fourier modelFig FCovariance matrix V receptive fields model white noise stimuliEq Fig GReceptive fields V neurons white noise stimuliFig HCovariance matrix V receptive fields unstructured model white noise stimuliFig ICovariance matrix V receptive fields translation invariant V model white noise stimuliFig JReceptive fields V neurons unstructured model translation invariant V model white noise stimuliFig KSpectral properties V receptive fields model Ringach datasetEq Fig LReceptive fields V neurons Ringach datasetFig MSpectral properties V receptive fields model natural image stimuliEq Fig NReceptive fields V neurons natural images stimuliFig OSpectral properties V receptive fields model DHT stimuliEq Fig PReceptive fields V neurons DHT stimuliFig QTraining loss MNIST fullytrained neural networks initialized V weightsFig RTest error MNIST fullytrained neural networks initialized V weightsFig STraining loss KMNIST fullytrained neural networks initialized V weightsFig TTest error KMNIST fullytrained neural networks initialized V weightsFig UInitializing AlexNet using structured random features shows little benefit ImageNetPDFClick additional data file pdfClick additional data fileM pdfAcknowledgments thank Dario Ringach providing macaque V data Brandon Pratt hawkmoth mechanosensor data grateful Ali Weber Steven Peterson Owen Levin Alice C Schwarze useful discussions thank Sarah Lindo Michalis Michaelos Carsen Stringer help mouse surgeries calcium imaging data processing respectivelyAcknowledgmentsWe thank Dario Ringach providing macaque V data Brandon Pratt hawkmoth mechanosensor data grateful Ali Weber Steven Peterson Owen Levin Alice C Schwarze useful discussions thank Sarah Lindo Michalis Michaelos Carsen Stringer help mouse surgeries calcium imaging data processing respectivelyWe thank Dario Ringach providing macaque V data Brandon Pratt hawkmoth mechanosensor data grateful Ali Weber Steven Peterson Owen Levin Alice C Schwarze useful discussions thank Sarah Lindo Michalis Michaelos Carsen Stringer help mouse surgeries calcium imaging data processing respectivelyFunding Statement BP supported UW Applied Math Frederic Wan Endowed Fellowship Terry Keegan Memorial ARCS Endowment Fellowship Natural Science Foundation Graduate Research Fellowship Program Grant DGE MP supported Janelia Research Campus Howard Hughes Medical Institute BWB supported grants FA FA Air Force Office Scientific Research KDH supported Washington Research Foundation postdoctoral fellowship Western Washington University funders role study design data collection analysis decision publish preparation manuscript httpsamathwashingtonedusupportus httpswwwarcsfoundationorgnationalhomepage httpswwwnsfgrfporg httpswwwjaneliaorglabpachitariulab httpswwwafrlafmilAFOSR httpswwwwrfseattleorggrantswrfpostdoctoralfellowships httpscswwueduharri Funding StatementBP supported UW Applied Math Frederic Wan Endowed Fellowship Terry Keegan Memorial ARCS Endowment Fellowship Natural Science Foundation Graduate Research Fellowship Program Grant DGE MP supported Janelia Research Campus Howard Hughes Medical Institute BWB supported grants FA FA Air Force Office Scientific Research KDH supported Washington Research Foundation postdoctoral fellowship Western Washington University funders role study design data collection analysis decision publish preparation manuscript httpsamathwashingtonedusupportus httpswwwarcsfoundationorgnationalhomepage httpswwwnsfgrfporg httpswwwjaneliaorglabpachitariulab httpswwwafrlafmilAFOSR httpswwwwrfseattleorggrantswrfpostdoctoralfellowships httpscswwueduharri httpsamathwashingtonedusupportushttpswwwarcsfoundationorgnationalhomepagehttpswwwnsfgrfporghttpswwwjaneliaorglabpachitariulabhttpswwwafrlafmilAFOSRhttpswwwwrfseattleorggrantswrfpostdoctoralfellowshipshttpscswwueduharriData Availability relevant data within manuscript Supporting information filesData AvailabilityAll relevant data within manuscript Supporting information filesSupporting informationReferences Yuste R neuron doctrine neural networks Nature Reviews Neuroscience doi nrn PubMed CrossRef Google Scholar Fusi Miller EK Rigotti neurons mix high dimensionality higher cognition Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Saxena Cunningham JP Towards neural population doctrine Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Stringer C Pachitariu Steinmetz N Carandini Harris KD Highdimensional geometry population responses visual cortex Nature doi PMC free article PubMed CrossRef Google Scholar Sherrington C Integrative Action Nervous System Cambridge University Press Google Scholar Chichilnisky EJ simple white noise analysis neuronal light responses Network Computation Neural Systems PubMed Google Scholar Sakai HM Naka K Signal transmission catfish retina V Sensitivity circuit Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Clay Reid R Alonso JM Specificity monosynaptic connections thalamus visual cortex Nature doi PubMed CrossRef Google Scholar Jones JP Palmer LA evaluation twodimensional Gabor filter model simple receptive fields cat striate cortex Journal Neurophysiology PubMed Google Scholar Knudsen EI Konishi Centersurround organization auditory receptive fields owl Science doi science PubMed CrossRef Google Scholar H Central mechanisms tactile shape perception Current opinion neurobiology PubMed Google Scholar Rusanen J Frolov R Weckstrm Kinoshita Arikawa K Nonlinear amplification graded voltage signals firstorder visual interneurons butterfly Papilio xuthus Journal Experimental Biology PubMed Google Scholar Fox JL Fairhall AL Daniel TL Encoding properties haltere neurons enable motion feature detection biological gyroscope Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Pratt B Deora Mohren Daniel Neural evidence supports dual sensorymotor role insect wings Proceedings Royal Society B Biological Sciences doi rspb PMC free article PubMed CrossRef Google Scholar Clemens J Ronacher B Feature Extraction Integration Underlying Perceptual Decision Making Courtship Behavior Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Park Pillow JW Receptive field inference localized priors PLoS computational biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Bonin V Histed MH Yurgenson Reid RC Local Diversity FineScale Organization Receptive Fields Mouse Visual Cortex Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Rosenblatt F perceptron probabilistic model information storage organization brain Psychological Review doi h PubMed CrossRef Google Scholar Caron SJC Ruta V Abbott LF Axel R Random convergence olfactory inputs Drosophila mushroom body Nature doi nature PMC free article PubMed CrossRef Google Scholar LitwinKumar Harris KD Axel R Sompolinsky H Abbott LF Optimal Degrees Synaptic Connectivity Neuron e doi jneuron PMC free article PubMed CrossRef Google Scholar Broomhead DS Lowe Radial basis functions multivariable functional interpolation adaptive networks Royal Signals Radar Establishment Malvern United Kingdom Google Scholar Igelnik B Pao YH Stochastic choice basis functions adaptive function approximation functionallink net IEEE transactions neural networks doi PubMed CrossRef Google Scholar Rahimi Recht B Random Features LargeScale Kernel Machines Platt JC Koller Singer Roweis ST editors Advances Neural Information Processing Systems Curran Associates Inc p Google Scholar Liu F Huang X Chen Suykens JAK Random Features Kernel Approximation Survey Algorithms Theory Beyond arXiv cs stat PubMed Arora Du SS Hu W Li Z Wang R FineGrained Analysis Optimization Generalization Overparameterized TwoLayer Neural Networks arXiv cs stat Arora Du SS Hu W Li Z Salakhutdinov R Wang R Exact Computation Infinitely Wide Neural Net arXiv cs stat Chen L Xu Deep Neural Tangent Kernel Laplace Kernel RKHS arXiv cs math stat Neal RM Priors Infinite Networks New York NY Springer New York p Available CrossRef Google Scholar Williams CKI Computation Infinite Neural Networks Neural Computation doi CrossRef Google Scholar Rahimi Recht B Uniform approximation functions random bases th Annual Allerton Conference Communication Control Computing IEEE p Available httpieeexploreieeeorgdocument Bordelon B Canatar Pehlevan C Spectrum Dependent Learning Curves Kernel Regression Wide Neural Networks arXiv cs stat Bordelon B Pehlevan C Population Codes Enable Learning Examples Shaping Inductive Bias bioRxiv Google Scholar Canatar Bordelon B Pehlevan C Spectral Bias TaskModel Alignment Explain Generalization Kernel Regression Infinitely Wide Neural Networks arXiv condmat stat PMC free article PubMed Harris KD Additive function approximation brain arXiv cs qbio stat Hashemi Schaeffer H Shi R Topcu U Tran G Ward R Generalization Bounds Sparse Random Feature Expansions arXiv cs math stat Xie Muscinelli Decker Harris K LitwinKumar Taskdependent optimal representations cerebellar learning bioRxiv Google Scholar Jacobsen JH van Gemert J Lou Z Smeulders AWM Structured Receptive Fields CNNs arXiv cs Paninski L Convergence properties spiketriggered analysis techniques Network Computation Neural Systems p PubMed Google Scholar Rasmussen CE Williams CKI Gaussian Processes Machine Learning Adaptive Computation Machine Learning MIT Press Google Scholar Kosambi DD Statistics function space Journal Indian Mathematical Society New Series Google Scholar Wahba G Spline Models Observational Data SIAM Dickerson BH Fox JL Sponberg Functional diversity generic encoding insect campaniform sensilla Current Opinion Physiology doi jcophys CrossRef Google Scholar Yarger Fox JL Dipteran Halteres Perspectives Function Integration Unique Sensory Organ Integrative Comparative Biology doi icbicw PubMed CrossRef Google Scholar Fox JL Daniel TL neural basis gyroscopic force measurement halteres Holorusia Journal Comparative Physiology doi sz PubMed CrossRef Google Scholar Genton MG Classes Kernels Machine Learning Statistics Perspective Journal Machine Learning Research Dec Google Scholar Hubel DH Wiesel TN Receptive fields single neurones cats striate cortex Journal Physiology doi jphysiolsp PMC free article PubMed CrossRef Google Scholar Marr Hildreth E Brenner Theory edge detection Proceedings Royal Society London Series B Biological Sciences PubMed Google Scholar Martens JB Hermite transformtheory IEEE Transactions Acoustics Speech Signal Processing doi CrossRef Google Scholar Kleiner Brainard Pelli Whats new Psychtoolbox PerceptionECVP Abstract Supplement European Conference Visual Perception ECVP August Arezzo Italy Sofroniew NJ Flickinger King J Svoboda K large field view twophoton mesoscope subcellular resolution vivo imaging Elife e doi eLife PMC free article PubMed CrossRef Google Scholar Pachitariu Stringer C Dipoppa Schrder Rossi LF Dalgleish H et al Suitep beyond neurons standard twophoton microscopy BioRxiv Google Scholar Mohren TL Daniel TL Brunton SL Brunton BW Neuralinspired sensors enable sparse efficient classification spatiotemporal data Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar LeCun Cortes C Burges C MNIST handwritten digit database ATT Labs Online Available httpyannlecuncomexdbmnist Google Scholar Clanuwat BoberIrizar Kitamoto Lamb Yamamoto K Ha Deep Learning Classical Japanese Literature arXiv cs stat K Zhang X Ren Sun J Delving Deep Rectifiers Surpassing HumanLevel Performance ImageNet Classification IEEE International Conference Computer Vision ICCV p Krizhevsky One weird trick parallelizing convolutional neural networks arXiv cs Russakovsky Deng J Su H Krause J Satheesh et al ImageNet Large Scale Visual Recognition Challenge International Journal Computer Vision doi sy CrossRef Google Scholar Pruszynski JA Johansson RS Edgeorientation processing firstorder tactile neurons Nature Neuroscience doi nn PubMed CrossRef Google Scholar Zhao CW Daley MJ Pruszynski JA Neural network models tactile system develop firstorder units spatially complex receptive fields PLOS ONE e doi journalpone PMC free article PubMed CrossRef Google Scholar Xu Ruan C Korpeoglu E Kumar Achan K Temporal Kernel Approach Deep Learning Continuoustime Information arXiv cs Koay SA Charles Thiberge SY Brody CD Tank DW Sequential efficient neuralpopulation coding complex task information bioRxiv PubMed Google Scholar Olshausen BA Field DJ Sparse Coding Overcomplete Basis Set Strategy Employed V Vision Research doi PubMed CrossRef Google Scholar Barlow HB et al Possible principles underlying transformation sensory messages Sensory communication Google Scholar Chalk Marre Tkaik G Toward Unified Theory Efficient Predictive Sparse Coding Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Swanson LW Brain architecture Understanding basic plan vol xv New York NY US Oxford University Press Google Scholar Strausfeld NJ Arthropod Brains Evolution Functional Elegance Historical Significance Harvard University Press Available httpswwwjstororgstablejctvdpvh Zador critique pure learning artificial neural networks learn animal brains Nature Communications doi PMC free article PubMed CrossRef Google Scholar Oja E Principal components minor components linear neural networks Neural Networks doi CrossRef Google Scholar Pehlevan C Sengupta Chklovskii DB Similarity Matching Objectives Lead HebbianAntiHebbian Networks Neural Computation doi necoa PubMed CrossRef Google Scholar Ringach DL Haphazard Wiring Simple Receptive Fields Orientation Columns Visual Cortex Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Ostojic Brunel N Spiking Neuron Models LinearNonlinear Models PLOS Computational Biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Weber AI Pillow JW Capturing Dynamical Repertoire Single Neurons Generalized Linear Models Neural Computation doi necoa PubMed CrossRef Google Scholar Fairhall receptive field dead Long live receptive field Current Opinion Neurobiology ixxii doi jconb PMC free article PubMed CrossRef Google Scholar Barth FG Mechanics preprocess information fine tuning mechanoreceptors Journal Comparative Physiology doi sz PMC free article PubMed CrossRef Google Scholar Eldar YC Kutyniok G editors Compressed Sensing Theory Applications Cambridge University Press Google Scholar Foucart Rauhut H Mathematical Introduction Compressive Sensing Birkhuser Basel Ganguli Sompolinsky H Compressed Sensing Sparsity Dimensionality Neuronal Information Processing Data Analysis Annual Review Neuroscience doi annurevneuro PubMed CrossRef Google Scholar Fallah K Willats AA Liu N Rozell CJ Learning sparse codes compressed representations biologically plausible local wiring constraints bioRxiv Google Scholar Barranca VJ Kovai G Zhou Cai Improved Compressive Sensing Natural Scenes Using Localized Random Sampling Scientific Reports doi srep PMC free article PubMed CrossRef Google Scholar ShaweTaylor J Cristianini N Kernel Methods Pattern Analysis Cambridge University Press Google Scholar ShalevShwartz BenDavid Understanding Machine Learning Theory Algorithms Cambridge University Press Google Scholar Yamins DLK Hong H Cadieu CF Solomon EA Seibert DiCarlo JJ Performanceoptimized hierarchical models predict neural responses higher visual cortex Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar K Zhang X Ren Sun J Deep Residual Learning Image Recognition arXiv cs Sinha Duchi JC Learning Kernels Random Features Lee Sugiyama Luxburg U Guyon Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileedcbbafbacbePaperpdf Google Scholar Arpit Campos V Bengio Initialize Network Robust Initialization WeightNorm ResNets Wallach H Larochelle H Beygelzimer dAlchBuc F Fox E Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileefaccdaPaperpdf Google Scholar Glorot X Bengio Understanding difficulty training deep feedforward neural networks Proceedings Thirteenth International Conference Artificial Intelligence Statistics JMLR Workshop Conference Proceedings p Available httpproceedingsmlrpressvglorotahtml Olah C Mordvintsev Schubert L Feature Visualization Distill Lindsay GW Convolutional Neural Networks Model Visual System Past Present Future Journal Cognitive Neuroscience p PubMed Google Scholar Dapello J Marques Schrimpf Geiger F Cox DiCarlo JJ Simulating Primary Visual Cortex Front CNNs Improves Robustness Image Perturbations Advances Neural Information Processing Systems Google Scholar Illing B Gerstner W Brea J Biologically plausible deep learningBut far go shallow networks Neural Networks doi jneunet PubMed CrossRef Google Scholar Mallat Group Invariant Scattering Communications Pure Applied Mathematics doi cpa CrossRef Google Scholar Bruna J Mallat Invariant Scattering Convolution Networks IEEE Transactions Pattern Analysis Machine Intelligence doi TPAMI PubMed CrossRef Google Scholar Andn J Mallat Deep Scattering Spectrum IEEE Transactions Signal Processing doi TSP CrossRef Google Scholar Pintea SL Tomen N Goes SF Loog van Gemert JC Resolution learning deep convolutional networks using scalespace theory arXiv cs PubMed Goltstein PM Meijer GT Pennartz CM Conditioning sharpens spatial representation rewarded stimuli mouse primary visual cortex eLife e doi eLife PMC free article PubMed CrossRef Google Scholar Fritz J Shamma Elhilali Klein Rapid taskrelated plasticity spectrotemporal receptive fields primary auditory cortex Nature Neuroscience doi nn PubMed CrossRef Google Scholar Beniaguev Segev London Single cortical neurons deep artificial neural networks Neuron e doi jneuron PubMed CrossRef Google Scholar Jacot Gabriel F Hongler C Neural Tangent Kernel Convergence Generalization Neural Networks Advances Neural Information Processing Systems vol Curran Associates Inc Available httpspapersnipsccpaperhashabefaebbaecbdfaAbstracthtml Google Scholar Mattos CLC Dai Z Damianou Forth J Barreto GA Lawrence ND Recurrent Gaussian Processes arXiv cs stat Ringach DL Spatial structure symmetry simplecell receptive fields macaque primary visual cortex Journal Neurophysiology Jul doi jn PubMed CrossRef Google Scholar References Yuste R neuron doctrine neural networks Nature Reviews Neuroscience doi nrn PubMed CrossRef Google Scholar Fusi Miller EK Rigotti neurons mix high dimensionality higher cognition Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Saxena Cunningham JP Towards neural population doctrine Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Stringer C Pachitariu Steinmetz N Carandini Harris KD Highdimensional geometry population responses visual cortex Nature doi PMC free article PubMed CrossRef Google Scholar Sherrington C Integrative Action Nervous System Cambridge University Press Google Scholar Chichilnisky EJ simple white noise analysis neuronal light responses Network Computation Neural Systems PubMed Google Scholar Sakai HM Naka K Signal transmission catfish retina V Sensitivity circuit Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Clay Reid R Alonso JM Specificity monosynaptic connections thalamus visual cortex Nature doi PubMed CrossRef Google Scholar Jones JP Palmer LA evaluation twodimensional Gabor filter model simple receptive fields cat striate cortex Journal Neurophysiology PubMed Google Scholar Knudsen EI Konishi Centersurround organization auditory receptive fields owl Science doi science PubMed CrossRef Google Scholar H Central mechanisms tactile shape perception Current opinion neurobiology PubMed Google Scholar Rusanen J Frolov R Weckstrm Kinoshita Arikawa K Nonlinear amplification graded voltage signals firstorder visual interneurons butterfly Papilio xuthus Journal Experimental Biology PubMed Google Scholar Fox JL Fairhall AL Daniel TL Encoding properties haltere neurons enable motion feature detection biological gyroscope Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Pratt B Deora Mohren Daniel Neural evidence supports dual sensorymotor role insect wings Proceedings Royal Society B Biological Sciences doi rspb PMC free article PubMed CrossRef Google Scholar Clemens J Ronacher B Feature Extraction Integration Underlying Perceptual Decision Making Courtship Behavior Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Park Pillow JW Receptive field inference localized priors PLoS computational biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Bonin V Histed MH Yurgenson Reid RC Local Diversity FineScale Organization Receptive Fields Mouse Visual Cortex Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Rosenblatt F perceptron probabilistic model information storage organization brain Psychological Review doi h PubMed CrossRef Google Scholar Caron SJC Ruta V Abbott LF Axel R Random convergence olfactory inputs Drosophila mushroom body Nature doi nature PMC free article PubMed CrossRef Google Scholar LitwinKumar Harris KD Axel R Sompolinsky H Abbott LF Optimal Degrees Synaptic Connectivity Neuron e doi jneuron PMC free article PubMed CrossRef Google Scholar Broomhead DS Lowe Radial basis functions multivariable functional interpolation adaptive networks Royal Signals Radar Establishment Malvern United Kingdom Google Scholar Igelnik B Pao YH Stochastic choice basis functions adaptive function approximation functionallink net IEEE transactions neural networks doi PubMed CrossRef Google Scholar Rahimi Recht B Random Features LargeScale Kernel Machines Platt JC Koller Singer Roweis ST editors Advances Neural Information Processing Systems Curran Associates Inc p Google Scholar Liu F Huang X Chen Suykens JAK Random Features Kernel Approximation Survey Algorithms Theory Beyond arXiv cs stat PubMed Arora Du SS Hu W Li Z Wang R FineGrained Analysis Optimization Generalization Overparameterized TwoLayer Neural Networks arXiv cs stat Arora Du SS Hu W Li Z Salakhutdinov R Wang R Exact Computation Infinitely Wide Neural Net arXiv cs stat Chen L Xu Deep Neural Tangent Kernel Laplace Kernel RKHS arXiv cs math stat Neal RM Priors Infinite Networks New York NY Springer New York p Available CrossRef Google Scholar Williams CKI Computation Infinite Neural Networks Neural Computation doi CrossRef Google Scholar Rahimi Recht B Uniform approximation functions random bases th Annual Allerton Conference Communication Control Computing IEEE p Available httpieeexploreieeeorgdocument Bordelon B Canatar Pehlevan C Spectrum Dependent Learning Curves Kernel Regression Wide Neural Networks arXiv cs stat Bordelon B Pehlevan C Population Codes Enable Learning Examples Shaping Inductive Bias bioRxiv Google Scholar Canatar Bordelon B Pehlevan C Spectral Bias TaskModel Alignment Explain Generalization Kernel Regression Infinitely Wide Neural Networks arXiv condmat stat PMC free article PubMed Harris KD Additive function approximation brain arXiv cs qbio stat Hashemi Schaeffer H Shi R Topcu U Tran G Ward R Generalization Bounds Sparse Random Feature Expansions arXiv cs math stat Xie Muscinelli Decker Harris K LitwinKumar Taskdependent optimal representations cerebellar learning bioRxiv Google Scholar Jacobsen JH van Gemert J Lou Z Smeulders AWM Structured Receptive Fields CNNs arXiv cs Paninski L Convergence properties spiketriggered analysis techniques Network Computation Neural Systems p PubMed Google Scholar Rasmussen CE Williams CKI Gaussian Processes Machine Learning Adaptive Computation Machine Learning MIT Press Google Scholar Kosambi DD Statistics function space Journal Indian Mathematical Society New Series Google Scholar Wahba G Spline Models Observational Data SIAM Dickerson BH Fox JL Sponberg Functional diversity generic encoding insect campaniform sensilla Current Opinion Physiology doi jcophys CrossRef Google Scholar Yarger Fox JL Dipteran Halteres Perspectives Function Integration Unique Sensory Organ Integrative Comparative Biology doi icbicw PubMed CrossRef Google Scholar Fox JL Daniel TL neural basis gyroscopic force measurement halteres Holorusia Journal Comparative Physiology doi sz PubMed CrossRef Google Scholar Genton MG Classes Kernels Machine Learning Statistics Perspective Journal Machine Learning Research Dec Google Scholar Hubel DH Wiesel TN Receptive fields single neurones cats striate cortex Journal Physiology doi jphysiolsp PMC free article PubMed CrossRef Google Scholar Marr Hildreth E Brenner Theory edge detection Proceedings Royal Society London Series B Biological Sciences PubMed Google Scholar Martens JB Hermite transformtheory IEEE Transactions Acoustics Speech Signal Processing doi CrossRef Google Scholar Kleiner Brainard Pelli Whats new Psychtoolbox PerceptionECVP Abstract Supplement European Conference Visual Perception ECVP August Arezzo Italy Sofroniew NJ Flickinger King J Svoboda K large field view twophoton mesoscope subcellular resolution vivo imaging Elife e doi eLife PMC free article PubMed CrossRef Google Scholar Pachitariu Stringer C Dipoppa Schrder Rossi LF Dalgleish H et al Suitep beyond neurons standard twophoton microscopy BioRxiv Google Scholar Mohren TL Daniel TL Brunton SL Brunton BW Neuralinspired sensors enable sparse efficient classification spatiotemporal data Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar LeCun Cortes C Burges C MNIST handwritten digit database ATT Labs Online Available httpyannlecuncomexdbmnist Google Scholar Clanuwat BoberIrizar Kitamoto Lamb Yamamoto K Ha Deep Learning Classical Japanese Literature arXiv cs stat K Zhang X Ren Sun J Delving Deep Rectifiers Surpassing HumanLevel Performance ImageNet Classification IEEE International Conference Computer Vision ICCV p Krizhevsky One weird trick parallelizing convolutional neural networks arXiv cs Russakovsky Deng J Su H Krause J Satheesh et al ImageNet Large Scale Visual Recognition Challenge International Journal Computer Vision doi sy CrossRef Google Scholar Pruszynski JA Johansson RS Edgeorientation processing firstorder tactile neurons Nature Neuroscience doi nn PubMed CrossRef Google Scholar Zhao CW Daley MJ Pruszynski JA Neural network models tactile system develop firstorder units spatially complex receptive fields PLOS ONE e doi journalpone PMC free article PubMed CrossRef Google Scholar Xu Ruan C Korpeoglu E Kumar Achan K Temporal Kernel Approach Deep Learning Continuoustime Information arXiv cs Koay SA Charles Thiberge SY Brody CD Tank DW Sequential efficient neuralpopulation coding complex task information bioRxiv PubMed Google Scholar Olshausen BA Field DJ Sparse Coding Overcomplete Basis Set Strategy Employed V Vision Research doi PubMed CrossRef Google Scholar Barlow HB et al Possible principles underlying transformation sensory messages Sensory communication Google Scholar Chalk Marre Tkaik G Toward Unified Theory Efficient Predictive Sparse Coding Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Swanson LW Brain architecture Understanding basic plan vol xv New York NY US Oxford University Press Google Scholar Strausfeld NJ Arthropod Brains Evolution Functional Elegance Historical Significance Harvard University Press Available httpswwwjstororgstablejctvdpvh Zador critique pure learning artificial neural networks learn animal brains Nature Communications doi PMC free article PubMed CrossRef Google Scholar Oja E Principal components minor components linear neural networks Neural Networks doi CrossRef Google Scholar Pehlevan C Sengupta Chklovskii DB Similarity Matching Objectives Lead HebbianAntiHebbian Networks Neural Computation doi necoa PubMed CrossRef Google Scholar Ringach DL Haphazard Wiring Simple Receptive Fields Orientation Columns Visual Cortex Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Ostojic Brunel N Spiking Neuron Models LinearNonlinear Models PLOS Computational Biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Weber AI Pillow JW Capturing Dynamical Repertoire Single Neurons Generalized Linear Models Neural Computation doi necoa PubMed CrossRef Google Scholar Fairhall receptive field dead Long live receptive field Current Opinion Neurobiology ixxii doi jconb PMC free article PubMed CrossRef Google Scholar Barth FG Mechanics preprocess information fine tuning mechanoreceptors Journal Comparative Physiology doi sz PMC free article PubMed CrossRef Google Scholar Eldar YC Kutyniok G editors Compressed Sensing Theory Applications Cambridge University Press Google Scholar Foucart Rauhut H Mathematical Introduction Compressive Sensing Birkhuser Basel Ganguli Sompolinsky H Compressed Sensing Sparsity Dimensionality Neuronal Information Processing Data Analysis Annual Review Neuroscience doi annurevneuro PubMed CrossRef Google Scholar Fallah K Willats AA Liu N Rozell CJ Learning sparse codes compressed representations biologically plausible local wiring constraints bioRxiv Google Scholar Barranca VJ Kovai G Zhou Cai Improved Compressive Sensing Natural Scenes Using Localized Random Sampling Scientific Reports doi srep PMC free article PubMed CrossRef Google Scholar ShaweTaylor J Cristianini N Kernel Methods Pattern Analysis Cambridge University Press Google Scholar ShalevShwartz BenDavid Understanding Machine Learning Theory Algorithms Cambridge University Press Google Scholar Yamins DLK Hong H Cadieu CF Solomon EA Seibert DiCarlo JJ Performanceoptimized hierarchical models predict neural responses higher visual cortex Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar K Zhang X Ren Sun J Deep Residual Learning Image Recognition arXiv cs Sinha Duchi JC Learning Kernels Random Features Lee Sugiyama Luxburg U Guyon Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileedcbbafbacbePaperpdf Google Scholar Arpit Campos V Bengio Initialize Network Robust Initialization WeightNorm ResNets Wallach H Larochelle H Beygelzimer dAlchBuc F Fox E Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileefaccdaPaperpdf Google Scholar Glorot X Bengio Understanding difficulty training deep feedforward neural networks Proceedings Thirteenth International Conference Artificial Intelligence Statistics JMLR Workshop Conference Proceedings p Available httpproceedingsmlrpressvglorotahtml Olah C Mordvintsev Schubert L Feature Visualization Distill Lindsay GW Convolutional Neural Networks Model Visual System Past Present Future Journal Cognitive Neuroscience p PubMed Google Scholar Dapello J Marques Schrimpf Geiger F Cox DiCarlo JJ Simulating Primary Visual Cortex Front CNNs Improves Robustness Image Perturbations Advances Neural Information Processing Systems Google Scholar Illing B Gerstner W Brea J Biologically plausible deep learningBut far go shallow networks Neural Networks doi jneunet PubMed CrossRef Google Scholar Mallat Group Invariant Scattering Communications Pure Applied Mathematics doi cpa CrossRef Google Scholar Bruna J Mallat Invariant Scattering Convolution Networks IEEE Transactions Pattern Analysis Machine Intelligence doi TPAMI PubMed CrossRef Google Scholar Andn J Mallat Deep Scattering Spectrum IEEE Transactions Signal Processing doi TSP CrossRef Google Scholar Pintea SL Tomen N Goes SF Loog van Gemert JC Resolution learning deep convolutional networks using scalespace theory arXiv cs PubMed Goltstein PM Meijer GT Pennartz CM Conditioning sharpens spatial representation rewarded stimuli mouse primary visual cortex eLife e doi eLife PMC free article PubMed CrossRef Google Scholar Fritz J Shamma Elhilali Klein Rapid taskrelated plasticity spectrotemporal receptive fields primary auditory cortex Nature Neuroscience doi nn PubMed CrossRef Google Scholar Beniaguev Segev London Single cortical neurons deep artificial neural networks Neuron e doi jneuron PubMed CrossRef Google Scholar Jacot Gabriel F Hongler C Neural Tangent Kernel Convergence Generalization Neural Networks Advances Neural Information Processing Systems vol Curran Associates Inc Available httpspapersnipsccpaperhashabefaebbaecbdfaAbstracthtml Google Scholar Mattos CLC Dai Z Damianou Forth J Barreto GA Lawrence ND Recurrent Gaussian Processes arXiv cs stat Ringach DL Spatial structure symmetry simplecell receptive fields macaque primary visual cortex Journal Neurophysiology Jul doi jn PubMed CrossRef Google Scholar Yuste R neuron doctrine neural networks Nature Reviews Neuroscience doi nrn PubMed CrossRef Google Scholar Yuste R neuron doctrine neural networks Nature Reviews Neuroscience doi nrn PubMed CrossRef Google Scholar neuron doctrine neural networksNature Reviews NeurosciencePubMedCrossRef Google Scholar Google Scholar Fusi Miller EK Rigotti neurons mix high dimensionality higher cognition Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Fusi Miller EK Rigotti neurons mix high dimensionality higher cognition Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar neurons mix high dimensionality higher cognitionCurrent Opinion NeurobiologyPubMedCrossRef Google Scholar Google Scholar Saxena Cunningham JP Towards neural population doctrine Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Saxena Cunningham JP Towards neural population doctrine Current Opinion Neurobiology doi jconb PubMed CrossRef Google Scholar Towards neural population doctrineCurrent Opinion NeurobiologyPubMedCrossRef Google Scholar Google Scholar Stringer C Pachitariu Steinmetz N Carandini Harris KD Highdimensional geometry population responses visual cortex Nature doi PMC free article PubMed CrossRef Google Scholar Stringer C Pachitariu Steinmetz N Carandini Harris KD Highdimensional geometry population responses visual cortex Nature doi PMC free article PubMed CrossRef Google Scholar Highdimensional geometry population responses visual cortexNature PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Sherrington C Integrative Action Nervous System Cambridge University Press Google Scholar Sherrington C Integrative Action Nervous System Cambridge University Press Google Scholar Integrative Action Nervous System Google Scholar Google Scholar Chichilnisky EJ simple white noise analysis neuronal light responses Network Computation Neural Systems PubMed Google Scholar Chichilnisky EJ simple white noise analysis neuronal light responses Network Computation Neural Systems PubMed Google Scholar simple white noise analysis neuronal light responsesNetwork Computation Neural SystemsPubMed Google Scholar Google Scholar Sakai HM Naka K Signal transmission catfish retina V Sensitivity circuit Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Sakai HM Naka K Signal transmission catfish retina V Sensitivity circuit Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Signal transmission catfish retina V Sensitivity circuitJournal NeurophysiologyPubMedCrossRef Google Scholar Google Scholar Clay Reid R Alonso JM Specificity monosynaptic connections thalamus visual cortex Nature doi PubMed CrossRef Google Scholar Clay Reid R Alonso JM Specificity monosynaptic connections thalamus visual cortex Nature doi PubMed CrossRef Google Scholar Specificity monosynaptic connections thalamus visual cortexNaturePubMedCrossRef Google Scholar Google Scholar Jones JP Palmer LA evaluation twodimensional Gabor filter model simple receptive fields cat striate cortex Journal Neurophysiology PubMed Google Scholar Jones JP Palmer LA evaluation twodimensional Gabor filter model simple receptive fields cat striate cortex Journal Neurophysiology PubMed Google Scholar evaluation twodimensional Gabor filter model simple receptive fields cat striate cortexJournal NeurophysiologyPubMed Google Scholar Google Scholar Knudsen EI Konishi Centersurround organization auditory receptive fields owl Science doi science PubMed CrossRef Google Scholar Knudsen EI Konishi Centersurround organization auditory receptive fields owl Science doi science PubMed CrossRef Google Scholar Centersurround organization auditory receptive fields owlSciencePubMedCrossRef Google Scholar Google Scholar H Central mechanisms tactile shape perception Current opinion neurobiology PubMed Google Scholar H Central mechanisms tactile shape perception Current opinion neurobiology PubMed Google Scholar Central mechanisms tactile shape perceptionCurrent opinion neurobiologyPubMed Google Scholar Google Scholar Rusanen J Frolov R Weckstrm Kinoshita Arikawa K Nonlinear amplification graded voltage signals firstorder visual interneurons butterfly Papilio xuthus Journal Experimental Biology PubMed Google Scholar Rusanen J Frolov R Weckstrm Kinoshita Arikawa K Nonlinear amplification graded voltage signals firstorder visual interneurons butterfly Papilio xuthus Journal Experimental Biology PubMed Google Scholar Nonlinear amplification graded voltage signals firstorder visual interneurons butterfly Papilio xuthusJournal Experimental BiologyPubMed Google Scholar Google Scholar Fox JL Fairhall AL Daniel TL Encoding properties haltere neurons enable motion feature detection biological gyroscope Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Fox JL Fairhall AL Daniel TL Encoding properties haltere neurons enable motion feature detection biological gyroscope Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Encoding properties haltere neurons enable motion feature detection biological gyroscopeProceedings National Academy Sciences PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Pratt B Deora Mohren Daniel Neural evidence supports dual sensorymotor role insect wings Proceedings Royal Society B Biological Sciences doi rspb PMC free article PubMed CrossRef Google Scholar Pratt B Deora Mohren Daniel Neural evidence supports dual sensorymotor role insect wings Proceedings Royal Society B Biological Sciences doi rspb PMC free article PubMed CrossRef Google Scholar Neural evidence supports dual sensorymotor role insect wingsProceedings Royal Society B Biological Sciences PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Clemens J Ronacher B Feature Extraction Integration Underlying Perceptual Decision Making Courtship Behavior Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Clemens J Ronacher B Feature Extraction Integration Underlying Perceptual Decision Making Courtship Behavior Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Feature Extraction Integration Underlying Perceptual Decision Making Courtship BehaviorJournal Neuroscience PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Park Pillow JW Receptive field inference localized priors PLoS computational biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Park Pillow JW Receptive field inference localized priors PLoS computational biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Receptive field inference localized priorsPLoS computational biology PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Bonin V Histed MH Yurgenson Reid RC Local Diversity FineScale Organization Receptive Fields Mouse Visual Cortex Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Bonin V Histed MH Yurgenson Reid RC Local Diversity FineScale Organization Receptive Fields Mouse Visual Cortex Journal Neuroscience doi JNEUROSCI PMC free article PubMed CrossRef Google Scholar Local Diversity FineScale Organization Receptive Fields Mouse Visual CortexJournal Neuroscience PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Rosenblatt F perceptron probabilistic model information storage organization brain Psychological Review doi h PubMed CrossRef Google Scholar Rosenblatt F perceptron probabilistic model information storage organization brain Psychological Review doi h PubMed CrossRef Google Scholar perceptron probabilistic model information storage organization brainPsychological ReviewPubMedCrossRef Google Scholar Google Scholar Caron SJC Ruta V Abbott LF Axel R Random convergence olfactory inputs Drosophila mushroom body Nature doi nature PMC free article PubMed CrossRef Google Scholar Caron SJC Ruta V Abbott LF Axel R Random convergence olfactory inputs Drosophila mushroom body Nature doi nature PMC free article PubMed CrossRef Google Scholar Random convergence olfactory inputs Drosophila mushroom bodyNature PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar LitwinKumar Harris KD Axel R Sompolinsky H Abbott LF Optimal Degrees Synaptic Connectivity Neuron e doi jneuron PMC free article PubMed CrossRef Google Scholar LitwinKumar Harris KD Axel R Sompolinsky H Abbott LF Optimal Degrees Synaptic Connectivity Neuron e doi jneuron PMC free article PubMed CrossRef Google Scholar Optimal Degrees Synaptic ConnectivityNeuron PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Broomhead DS Lowe Radial basis functions multivariable functional interpolation adaptive networks Royal Signals Radar Establishment Malvern United Kingdom Google Scholar Broomhead DS Lowe Radial basis functions multivariable functional interpolation adaptive networks Royal Signals Radar Establishment Malvern United Kingdom Google Scholar Radial basis functions multivariable functional interpolation adaptive networksRoyal Signals Radar Establishment Malvern United Kingdom Google Scholar Google Scholar Igelnik B Pao YH Stochastic choice basis functions adaptive function approximation functionallink net IEEE transactions neural networks doi PubMed CrossRef Google Scholar Igelnik B Pao YH Stochastic choice basis functions adaptive function approximation functionallink net IEEE transactions neural networks doi PubMed CrossRef Google Scholar Stochastic choice basis functions adaptive function approximation functionallink netIEEE transactions neural networksPubMedCrossRef Google Scholar Google Scholar Rahimi Recht B Random Features LargeScale Kernel Machines Platt JC Koller Singer Roweis ST editors Advances Neural Information Processing Systems Curran Associates Inc p Google Scholar Rahimi Recht B Random Features LargeScale Kernel Machines Platt JC Koller Singer Roweis ST editors Advances Neural Information Processing Systems Curran Associates Inc p Google Scholar Advances Neural Information Processing Systems Google Scholar Google Scholar Liu F Huang X Chen Suykens JAK Random Features Kernel Approximation Survey Algorithms Theory Beyond arXiv cs stat PubMed Liu F Huang X Chen Suykens JAK Random Features Kernel Approximation Survey Algorithms Theory Beyond arXiv cs stat PubMed PubMed Arora Du SS Hu W Li Z Wang R FineGrained Analysis Optimization Generalization Overparameterized TwoLayer Neural Networks arXiv cs stat Arora Du SS Hu W Li Z Wang R FineGrained Analysis Optimization Generalization Overparameterized TwoLayer Neural Networks arXiv cs stat Arora Du SS Hu W Li Z Salakhutdinov R Wang R Exact Computation Infinitely Wide Neural Net arXiv cs stat Arora Du SS Hu W Li Z Salakhutdinov R Wang R Exact Computation Infinitely Wide Neural Net arXiv cs stat Chen L Xu Deep Neural Tangent Kernel Laplace Kernel RKHS arXiv cs math stat Chen L Xu Deep Neural Tangent Kernel Laplace Kernel RKHS arXiv cs math stat Neal RM Priors Infinite Networks New York NY Springer New York p Available CrossRef Google Scholar Neal RM Priors Infinite Networks New York NY Springer New York p Available CrossRef Google Scholar Priors Infinite NetworksCrossRef Google Scholar Google Scholar Williams CKI Computation Infinite Neural Networks Neural Computation doi CrossRef Google Scholar Williams CKI Computation Infinite Neural Networks Neural Computation doi CrossRef Google Scholar Computation Infinite Neural NetworksNeural ComputationCrossRef Google Scholar Google Scholar Rahimi Recht B Uniform approximation functions random bases th Annual Allerton Conference Communication Control Computing IEEE p Available httpieeexploreieeeorgdocument Rahimi Recht B Uniform approximation functions random bases th Annual Allerton Conference Communication Control Computing IEEE p Available httpieeexploreieeeorgdocument httpieeexploreieeeorgdocument Bordelon B Canatar Pehlevan C Spectrum Dependent Learning Curves Kernel Regression Wide Neural Networks arXiv cs stat Bordelon B Canatar Pehlevan C Spectrum Dependent Learning Curves Kernel Regression Wide Neural Networks arXiv cs stat Bordelon B Pehlevan C Population Codes Enable Learning Examples Shaping Inductive Bias bioRxiv Google Scholar Bordelon B Pehlevan C Population Codes Enable Learning Examples Shaping Inductive Bias bioRxiv Google Scholar Population Codes Enable Learning Examples Shaping Inductive BiasbioRxiv Google Scholar Google Scholar Canatar Bordelon B Pehlevan C Spectral Bias TaskModel Alignment Explain Generalization Kernel Regression Infinitely Wide Neural Networks arXiv condmat stat PMC free article PubMed Canatar Bordelon B Pehlevan C Spectral Bias TaskModel Alignment Explain Generalization Kernel Regression Infinitely Wide Neural Networks arXiv condmat stat PMC free article PubMed PMC free article PMC free articlePubMed Harris KD Additive function approximation brain arXiv cs qbio stat Harris KD Additive function approximation brain arXiv cs qbio stat Hashemi Schaeffer H Shi R Topcu U Tran G Ward R Generalization Bounds Sparse Random Feature Expansions arXiv cs math stat Hashemi Schaeffer H Shi R Topcu U Tran G Ward R Generalization Bounds Sparse Random Feature Expansions arXiv cs math stat Xie Muscinelli Decker Harris K LitwinKumar Taskdependent optimal representations cerebellar learning bioRxiv Google Scholar Xie Muscinelli Decker Harris K LitwinKumar Taskdependent optimal representations cerebellar learning bioRxiv Google Scholar Taskdependent optimal representations cerebellar learningbioRxiv Google Scholar Google Scholar Jacobsen JH van Gemert J Lou Z Smeulders AWM Structured Receptive Fields CNNs arXiv cs Jacobsen JH van Gemert J Lou Z Smeulders AWM Structured Receptive Fields CNNs arXiv cs Paninski L Convergence properties spiketriggered analysis techniques Network Computation Neural Systems p PubMed Google Scholar Paninski L Convergence properties spiketriggered analysis techniques Network Computation Neural Systems p PubMed Google Scholar Convergence properties spiketriggered analysis techniquesNetwork Computation Neural SystemsPubMed Google Scholar Google Scholar Rasmussen CE Williams CKI Gaussian Processes Machine Learning Adaptive Computation Machine Learning MIT Press Google Scholar Rasmussen CE Williams CKI Gaussian Processes Machine Learning Adaptive Computation Machine Learning MIT Press Google Scholar Gaussian Processes Machine Learning Adaptive Computation Machine Learning Google Scholar Google Scholar Kosambi DD Statistics function space Journal Indian Mathematical Society New Series Google Scholar Kosambi DD Statistics function space Journal Indian Mathematical Society New Series Google Scholar Statistics function spaceThe Journal Indian Mathematical Society New Series Google Scholar Google Scholar Wahba G Spline Models Observational Data SIAM Wahba G Spline Models Observational Data SIAM Dickerson BH Fox JL Sponberg Functional diversity generic encoding insect campaniform sensilla Current Opinion Physiology doi jcophys CrossRef Google Scholar Dickerson BH Fox JL Sponberg Functional diversity generic encoding insect campaniform sensilla Current Opinion Physiology doi jcophys CrossRef Google Scholar Functional diversity generic encoding insect campaniform sensillaCurrent Opinion PhysiologyCrossRef Google Scholar Google Scholar Yarger Fox JL Dipteran Halteres Perspectives Function Integration Unique Sensory Organ Integrative Comparative Biology doi icbicw PubMed CrossRef Google Scholar Yarger Fox JL Dipteran Halteres Perspectives Function Integration Unique Sensory Organ Integrative Comparative Biology doi icbicw PubMed CrossRef Google Scholar Dipteran Halteres Perspectives Function Integration Unique Sensory OrganIntegrative Comparative BiologyPubMedCrossRef Google Scholar Google Scholar Fox JL Daniel TL neural basis gyroscopic force measurement halteres Holorusia Journal Comparative Physiology doi sz PubMed CrossRef Google Scholar Fox JL Daniel TL neural basis gyroscopic force measurement halteres Holorusia Journal Comparative Physiology doi sz PubMed CrossRef Google Scholar neural basis gyroscopic force measurement halteres HolorusiaJournal Comparative Physiology APubMedCrossRef Google Scholar Google Scholar Genton MG Classes Kernels Machine Learning Statistics Perspective Journal Machine Learning Research Dec Google Scholar Genton MG Classes Kernels Machine Learning Statistics Perspective Journal Machine Learning Research Dec Google Scholar Classes Kernels Machine Learning Statistics PerspectiveJournal Machine Learning ResearchDec Google Scholar Google Scholar Hubel DH Wiesel TN Receptive fields single neurones cats striate cortex Journal Physiology doi jphysiolsp PMC free article PubMed CrossRef Google Scholar Hubel DH Wiesel TN Receptive fields single neurones cats striate cortex Journal Physiology doi jphysiolsp PMC free article PubMed CrossRef Google Scholar Receptive fields single neurones cats striate cortexThe Journal Physiology PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Marr Hildreth E Brenner Theory edge detection Proceedings Royal Society London Series B Biological Sciences PubMed Google Scholar Marr Hildreth E Brenner Theory edge detection Proceedings Royal Society London Series B Biological Sciences PubMed Google Scholar Theory edge detectionProceedings Royal Society London Series B Biological SciencesPubMed Google Scholar Google Scholar Martens JB Hermite transformtheory IEEE Transactions Acoustics Speech Signal Processing doi CrossRef Google Scholar Martens JB Hermite transformtheory IEEE Transactions Acoustics Speech Signal Processing doi CrossRef Google Scholar Hermite transformtheoryIEEE Transactions Acoustics Speech Signal ProcessingCrossRef Google Scholar Google Scholar Kleiner Brainard Pelli Whats new Psychtoolbox PerceptionECVP Abstract Supplement European Conference Visual Perception ECVP August Arezzo Italy Kleiner Brainard Pelli Whats new Psychtoolbox PerceptionECVP Abstract Supplement European Conference Visual Perception ECVP August Arezzo Italy Sofroniew NJ Flickinger King J Svoboda K large field view twophoton mesoscope subcellular resolution vivo imaging Elife e doi eLife PMC free article PubMed CrossRef Google Scholar Sofroniew NJ Flickinger King J Svoboda K large field view twophoton mesoscope subcellular resolution vivo imaging Elife e doi eLife PMC free article PubMed CrossRef Google Scholar large field view twophoton mesoscope subcellular resolution vivo imagingElife PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Pachitariu Stringer C Dipoppa Schrder Rossi LF Dalgleish H et al Suitep beyond neurons standard twophoton microscopy BioRxiv Google Scholar Pachitariu Stringer C Dipoppa Schrder Rossi LF Dalgleish H et al Suitep beyond neurons standard twophoton microscopy BioRxiv Google Scholar Suitep beyond neurons standard twophoton microscopyBioRxiv Google Scholar Google Scholar Mohren TL Daniel TL Brunton SL Brunton BW Neuralinspired sensors enable sparse efficient classification spatiotemporal data Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Mohren TL Daniel TL Brunton SL Brunton BW Neuralinspired sensors enable sparse efficient classification spatiotemporal data Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Neuralinspired sensors enable sparse efficient classification spatiotemporal dataProceedings National Academy Sciences PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar LeCun Cortes C Burges C MNIST handwritten digit database ATT Labs Online Available httpyannlecuncomexdbmnist Google Scholar LeCun Cortes C Burges C MNIST handwritten digit database ATT Labs Online Available httpyannlecuncomexdbmnist Google Scholar MNIST handwritten digit databaseATT Labshttpyannlecuncomexdbmnist Google Scholar Google Scholar Clanuwat BoberIrizar Kitamoto Lamb Yamamoto K Ha Deep Learning Classical Japanese Literature arXiv cs stat Clanuwat BoberIrizar Kitamoto Lamb Yamamoto K Ha Deep Learning Classical Japanese Literature arXiv cs stat K Zhang X Ren Sun J Delving Deep Rectifiers Surpassing HumanLevel Performance ImageNet Classification IEEE International Conference Computer Vision ICCV p K Zhang X Ren Sun J Delving Deep Rectifiers Surpassing HumanLevel Performance ImageNet Classification IEEE International Conference Computer Vision ICCV p Krizhevsky One weird trick parallelizing convolutional neural networks arXiv cs Krizhevsky One weird trick parallelizing convolutional neural networks arXiv cs Russakovsky Deng J Su H Krause J Satheesh et al ImageNet Large Scale Visual Recognition Challenge International Journal Computer Vision doi sy CrossRef Google Scholar Russakovsky Deng J Su H Krause J Satheesh et al ImageNet Large Scale Visual Recognition Challenge International Journal Computer Vision doi sy CrossRef Google Scholar ImageNet Large Scale Visual Recognition ChallengeInternational Journal Computer VisionCrossRef Google Scholar Google Scholar Pruszynski JA Johansson RS Edgeorientation processing firstorder tactile neurons Nature Neuroscience doi nn PubMed CrossRef Google Scholar Pruszynski JA Johansson RS Edgeorientation processing firstorder tactile neurons Nature Neuroscience doi nn PubMed CrossRef Google Scholar Edgeorientation processing firstorder tactile neuronsNature NeurosciencePubMedCrossRef Google Scholar Google Scholar Zhao CW Daley MJ Pruszynski JA Neural network models tactile system develop firstorder units spatially complex receptive fields PLOS ONE e doi journalpone PMC free article PubMed CrossRef Google Scholar Zhao CW Daley MJ Pruszynski JA Neural network models tactile system develop firstorder units spatially complex receptive fields PLOS ONE e doi journalpone PMC free article PubMed CrossRef Google Scholar Neural network models tactile system develop firstorder units spatially complex receptive fieldsPLOS ONE PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Xu Ruan C Korpeoglu E Kumar Achan K Temporal Kernel Approach Deep Learning Continuoustime Information arXiv cs Xu Ruan C Korpeoglu E Kumar Achan K Temporal Kernel Approach Deep Learning Continuoustime Information arXiv cs Koay SA Charles Thiberge SY Brody CD Tank DW Sequential efficient neuralpopulation coding complex task information bioRxiv PubMed Google Scholar Koay SA Charles Thiberge SY Brody CD Tank DW Sequential efficient neuralpopulation coding complex task information bioRxiv PubMed Google Scholar Sequential efficient neuralpopulation coding complex task informationbioRxivPubMed Google Scholar Google Scholar Olshausen BA Field DJ Sparse Coding Overcomplete Basis Set Strategy Employed V Vision Research doi PubMed CrossRef Google Scholar Olshausen BA Field DJ Sparse Coding Overcomplete Basis Set Strategy Employed V Vision Research doi PubMed CrossRef Google Scholar Sparse Coding Overcomplete Basis Set Strategy Employed VVision ResearchPubMedCrossRef Google Scholar Google Scholar Barlow HB et al Possible principles underlying transformation sensory messages Sensory communication Google Scholar Barlow HB et al Possible principles underlying transformation sensory messages Sensory communication Google Scholar Possible principles underlying transformation sensory messagesSensory communication Google Scholar Google Scholar Chalk Marre Tkaik G Toward Unified Theory Efficient Predictive Sparse Coding Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Chalk Marre Tkaik G Toward Unified Theory Efficient Predictive Sparse Coding Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Toward Unified Theory Efficient Predictive Sparse CodingProceedings National Academy Sciences PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Swanson LW Brain architecture Understanding basic plan vol xv New York NY US Oxford University Press Google Scholar Swanson LW Brain architecture Understanding basic plan vol xv New York NY US Oxford University Press Google Scholar Brain architecture Understanding basic planvol xv Google Scholar Google Scholar Strausfeld NJ Arthropod Brains Evolution Functional Elegance Historical Significance Harvard University Press Available httpswwwjstororgstablejctvdpvh Strausfeld NJ Arthropod Brains Evolution Functional Elegance Historical Significance Harvard University Press Available httpswwwjstororgstablejctvdpvh httpswwwjstororgstablejctvdpvh Zador critique pure learning artificial neural networks learn animal brains Nature Communications doi PMC free article PubMed CrossRef Google Scholar Zador critique pure learning artificial neural networks learn animal brains Nature Communications doi PMC free article PubMed CrossRef Google Scholar critique pure learning artificial neural networks learn animal brainsNature Communications PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Oja E Principal components minor components linear neural networks Neural Networks doi CrossRef Google Scholar Oja E Principal components minor components linear neural networks Neural Networks doi CrossRef Google Scholar Principal components minor components linear neural networksNeural NetworksCrossRef Google Scholar Google Scholar Pehlevan C Sengupta Chklovskii DB Similarity Matching Objectives Lead HebbianAntiHebbian Networks Neural Computation doi necoa PubMed CrossRef Google Scholar Pehlevan C Sengupta Chklovskii DB Similarity Matching Objectives Lead HebbianAntiHebbian Networks Neural Computation doi necoa PubMed CrossRef Google Scholar Similarity Matching Objectives Lead HebbianAntiHebbian NetworksNeural ComputationPubMedCrossRef Google Scholar Google Scholar Ringach DL Haphazard Wiring Simple Receptive Fields Orientation Columns Visual Cortex Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Ringach DL Haphazard Wiring Simple Receptive Fields Orientation Columns Visual Cortex Journal Neurophysiology doi jn PubMed CrossRef Google Scholar Haphazard Wiring Simple Receptive Fields Orientation Columns Visual CortexJournal NeurophysiologyPubMedCrossRef Google Scholar Google Scholar Ostojic Brunel N Spiking Neuron Models LinearNonlinear Models PLOS Computational Biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Ostojic Brunel N Spiking Neuron Models LinearNonlinear Models PLOS Computational Biology e doi journalpcbi PMC free article PubMed CrossRef Google Scholar Spiking Neuron Models LinearNonlinear ModelsPLOS Computational Biology PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Weber AI Pillow JW Capturing Dynamical Repertoire Single Neurons Generalized Linear Models Neural Computation doi necoa PubMed CrossRef Google Scholar Weber AI Pillow JW Capturing Dynamical Repertoire Single Neurons Generalized Linear Models Neural Computation doi necoa PubMed CrossRef Google Scholar Capturing Dynamical Repertoire Single Neurons Generalized Linear ModelsNeural ComputationPubMedCrossRef Google Scholar Google Scholar Fairhall receptive field dead Long live receptive field Current Opinion Neurobiology ixxii doi jconb PMC free article PubMed CrossRef Google Scholar Fairhall receptive field dead Long live receptive field Current Opinion Neurobiology ixxii doi jconb PMC free article PubMed CrossRef Google Scholar receptive field dead Long live receptive fieldCurrent Opinion Neurobiology PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Barth FG Mechanics preprocess information fine tuning mechanoreceptors Journal Comparative Physiology doi sz PMC free article PubMed CrossRef Google Scholar Barth FG Mechanics preprocess information fine tuning mechanoreceptors Journal Comparative Physiology doi sz PMC free article PubMed CrossRef Google Scholar Mechanics preprocess information fine tuning mechanoreceptorsJournal Comparative Physiology PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Eldar YC Kutyniok G editors Compressed Sensing Theory Applications Cambridge University Press Google Scholar Eldar YC Kutyniok G editors Compressed Sensing Theory Applications Cambridge University Press Google Scholar Compressed Sensing Theory Applications Google Scholar Google Scholar Foucart Rauhut H Mathematical Introduction Compressive Sensing Birkhuser Basel Foucart Rauhut H Mathematical Introduction Compressive Sensing Birkhuser Basel Ganguli Sompolinsky H Compressed Sensing Sparsity Dimensionality Neuronal Information Processing Data Analysis Annual Review Neuroscience doi annurevneuro PubMed CrossRef Google Scholar Ganguli Sompolinsky H Compressed Sensing Sparsity Dimensionality Neuronal Information Processing Data Analysis Annual Review Neuroscience doi annurevneuro PubMed CrossRef Google Scholar Compressed Sensing Sparsity Dimensionality Neuronal Information Processing Data AnalysisAnnual Review NeurosciencePubMedCrossRef Google Scholar Google Scholar Fallah K Willats AA Liu N Rozell CJ Learning sparse codes compressed representations biologically plausible local wiring constraints bioRxiv Google Scholar Fallah K Willats AA Liu N Rozell CJ Learning sparse codes compressed representations biologically plausible local wiring constraints bioRxiv Google Scholar Learning sparse codes compressed representations biologically plausible local wiring constraintsbioRxiv Google Scholar Google Scholar Barranca VJ Kovai G Zhou Cai Improved Compressive Sensing Natural Scenes Using Localized Random Sampling Scientific Reports doi srep PMC free article PubMed CrossRef Google Scholar Barranca VJ Kovai G Zhou Cai Improved Compressive Sensing Natural Scenes Using Localized Random Sampling Scientific Reports doi srep PMC free article PubMed CrossRef Google Scholar Improved Compressive Sensing Natural Scenes Using Localized Random SamplingScientific Reports PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar ShaweTaylor J Cristianini N Kernel Methods Pattern Analysis Cambridge University Press Google Scholar ShaweTaylor J Cristianini N Kernel Methods Pattern Analysis Cambridge University Press Google Scholar Kernel Methods Pattern Analysis Google Scholar Google Scholar ShalevShwartz BenDavid Understanding Machine Learning Theory Algorithms Cambridge University Press Google Scholar ShalevShwartz BenDavid Understanding Machine Learning Theory Algorithms Cambridge University Press Google Scholar Understanding Machine Learning Theory Algorithms Google Scholar Google Scholar Yamins DLK Hong H Cadieu CF Solomon EA Seibert DiCarlo JJ Performanceoptimized hierarchical models predict neural responses higher visual cortex Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Yamins DLK Hong H Cadieu CF Solomon EA Seibert DiCarlo JJ Performanceoptimized hierarchical models predict neural responses higher visual cortex Proceedings National Academy Sciences doi pnas PMC free article PubMed CrossRef Google Scholar Performanceoptimized hierarchical models predict neural responses higher visual cortexProceedings National Academy Sciences PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar K Zhang X Ren Sun J Deep Residual Learning Image Recognition arXiv cs K Zhang X Ren Sun J Deep Residual Learning Image Recognition arXiv cs Sinha Duchi JC Learning Kernels Random Features Lee Sugiyama Luxburg U Guyon Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileedcbbafbacbePaperpdf Google Scholar Sinha Duchi JC Learning Kernels Random Features Lee Sugiyama Luxburg U Guyon Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileedcbbafbacbePaperpdf Google Scholar Advances Neural Information Processing Systemsvol httpsproceedingsneuripsccpaperfileedcbbafbacbePaperpdf Google Scholar Google Scholar Arpit Campos V Bengio Initialize Network Robust Initialization WeightNorm ResNets Wallach H Larochelle H Beygelzimer dAlchBuc F Fox E Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileefaccdaPaperpdf Google Scholar Arpit Campos V Bengio Initialize Network Robust Initialization WeightNorm ResNets Wallach H Larochelle H Beygelzimer dAlchBuc F Fox E Garnett R editors Advances Neural Information Processing Systems vol Curran Associates Inc Available httpsproceedingsneuripsccpaperfileefaccdaPaperpdf Google Scholar Advances Neural Information Processing Systemsvol httpsproceedingsneuripsccpaperfileefaccdaPaperpdf Google Scholar Google Scholar Glorot X Bengio Understanding difficulty training deep feedforward neural networks Proceedings Thirteenth International Conference Artificial Intelligence Statistics JMLR Workshop Conference Proceedings p Available httpproceedingsmlrpressvglorotahtml Glorot X Bengio Understanding difficulty training deep feedforward neural networks Proceedings Thirteenth International Conference Artificial Intelligence Statistics JMLR Workshop Conference Proceedings p Available httpproceedingsmlrpressvglorotahtml httpproceedingsmlrpressvglorotahtml Olah C Mordvintsev Schubert L Feature Visualization Distill Olah C Mordvintsev Schubert L Feature Visualization Distill Lindsay GW Convolutional Neural Networks Model Visual System Past Present Future Journal Cognitive Neuroscience p PubMed Google Scholar Lindsay GW Convolutional Neural Networks Model Visual System Past Present Future Journal Cognitive Neuroscience p PubMed Google Scholar Convolutional Neural Networks Model Visual System Past Present FutureJournal Cognitive NeurosciencePubMed Google Scholar Google Scholar Dapello J Marques Schrimpf Geiger F Cox DiCarlo JJ Simulating Primary Visual Cortex Front CNNs Improves Robustness Image Perturbations Advances Neural Information Processing Systems Google Scholar Dapello J Marques Schrimpf Geiger F Cox DiCarlo JJ Simulating Primary Visual Cortex Front CNNs Improves Robustness Image Perturbations Advances Neural Information Processing Systems Google Scholar Simulating Primary Visual Cortex Front CNNs Improves Robustness Image PerturbationsAdvances Neural Information Processing Systems Google Scholar Google Scholar Illing B Gerstner W Brea J Biologically plausible deep learningBut far go shallow networks Neural Networks doi jneunet PubMed CrossRef Google Scholar Illing B Gerstner W Brea J Biologically plausible deep learningBut far go shallow networks Neural Networks doi jneunet PubMed CrossRef Google Scholar Biologically plausible deep learningBut far go shallow networksNeural NetworksPubMedCrossRef Google Scholar Google Scholar Mallat Group Invariant Scattering Communications Pure Applied Mathematics doi cpa CrossRef Google Scholar Mallat Group Invariant Scattering Communications Pure Applied Mathematics doi cpa CrossRef Google Scholar Group Invariant ScatteringCommunications Pure Applied MathematicsCrossRef Google Scholar Google Scholar Bruna J Mallat Invariant Scattering Convolution Networks IEEE Transactions Pattern Analysis Machine Intelligence doi TPAMI PubMed CrossRef Google Scholar Bruna J Mallat Invariant Scattering Convolution Networks IEEE Transactions Pattern Analysis Machine Intelligence doi TPAMI PubMed CrossRef Google Scholar Invariant Scattering Convolution NetworksIEEE Transactions Pattern Analysis Machine IntelligencePubMedCrossRef Google Scholar Google Scholar Andn J Mallat Deep Scattering Spectrum IEEE Transactions Signal Processing doi TSP CrossRef Google Scholar Andn J Mallat Deep Scattering Spectrum IEEE Transactions Signal Processing doi TSP CrossRef Google Scholar Deep Scattering SpectrumIEEE Transactions Signal ProcessingCrossRef Google Scholar Google Scholar Pintea SL Tomen N Goes SF Loog van Gemert JC Resolution learning deep convolutional networks using scalespace theory arXiv cs PubMed Pintea SL Tomen N Goes SF Loog van Gemert JC Resolution learning deep convolutional networks using scalespace theory arXiv cs PubMed PubMed Goltstein PM Meijer GT Pennartz CM Conditioning sharpens spatial representation rewarded stimuli mouse primary visual cortex eLife e doi eLife PMC free article PubMed CrossRef Google Scholar Goltstein PM Meijer GT Pennartz CM Conditioning sharpens spatial representation rewarded stimuli mouse primary visual cortex eLife e doi eLife PMC free article PubMed CrossRef Google Scholar Conditioning sharpens spatial representation rewarded stimuli mouse primary visual cortexeLife PMC free article PMC free articlePubMedCrossRef Google Scholar Google Scholar Fritz J Shamma Elhilali Klein Rapid taskrelated plasticity spectrotemporal receptive fields primary auditory cortex Nature Neuroscience doi nn PubMed CrossRef Google Scholar Fritz J Shamma Elhilali Klein Rapid taskrelated plasticity spectrotemporal receptive fields primary auditory cortex Nature Neuroscience doi nn PubMed CrossRef Google Scholar Rapid taskrelated plasticity spectrotemporal receptive fields primary auditory cortexNature NeurosciencePubMedCrossRef Google Scholar Google Scholar Beniaguev Segev London Single cortical neurons deep artificial neural networks Neuron e doi jneuron PubMed CrossRef Google Scholar Beniaguev Segev London Single cortical neurons deep artificial neural networks Neuron e doi jneuron PubMed CrossRef Google Scholar Single cortical neurons deep artificial neural networksNeuronPubMedCrossRef Google Scholar Google Scholar Jacot Gabriel F Hongler C Neural Tangent Kernel Convergence Generalization Neural Networks Advances Neural Information Processing Systems vol Curran Associates Inc Available httpspapersnipsccpaperhashabefaebbaecbdfaAbstracthtml Google Scholar Jacot Gabriel F Hongler C Neural Tangent Kernel Convergence Generalization Neural Networks Advances Neural Information Processing Systems vol Curran Associates Inc Available httpspapersnipsccpaperhashabefaebbaecbdfaAbstracthtml Google Scholar Advances Neural Information Processing Systemsvol httpspapersnipsccpaperhashabefaebbaecbdfaAbstracthtml Google Scholar Google Scholar Mattos CLC Dai Z Damianou Forth J Barreto GA Lawrence ND Recurrent Gaussian Processes arXiv cs stat Mattos CLC Dai Z Damianou Forth J Barreto GA Lawrence ND Recurrent Gaussian Processes arXiv cs stat Ringach DL Spatial structure symmetry simplecell receptive fields macaque primary visual cortex Journal Neurophysiology Jul doi jn PubMed CrossRef Google Scholar Ringach DL Spatial structure symmetry simplecell receptive fields macaque primary visual cortex Journal Neurophysiology Jul doi jn PubMed CrossRef Google Scholar Spatial structure symmetry simplecell receptive fields macaque primary visual cortexJournal NeurophysiologyJulPubMedCrossRef Google Scholar Google ScholarPLoS Comput Biol Oct e Decision Letter PLoS Comput Biol Oct e Decision Letter PLoS Comput Biol Oct e PLoS Comput Biol Oct e Decision Letter Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Mar Dear Mr Pandey Thank much submitting manuscript Structured random receptive fields enable informative sensory encodings consideration PLOS Computational Biology papers reviewed journal manuscript reviewed members editorial board several independent reviewers light reviews email would like invite resubmission significantlyrevised version takes account reviewers comments make decision publication seen revised manuscript response reviewers comments revised manuscript also likely sent reviewers evaluation ready resubmit please upload following letter containing detailed list responses review comments description changes made manuscript Please note forming response article accepted may opportunity make peer review history publicly available record include editor decision letters reviews responses reviewer comments eligible contact opt Two versions revised manuscript one either highlights tracked changes denoting text changed clean version uploaded manuscript file Important additional instructions given reviewer comments Please prepare submit revised manuscript within days anticipate delay please let us know expected resubmission date replying email Please note revised manuscripts received day due date may require evaluation peer review similar newly submitted manuscripts Thank submission hope editorial process constructive far welcome feedback time Please dont hesitate contact us questions comments Sincerely Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer authors make two main contributions First modeling receptive fields independent draws Gaussian process Second arguing structured receptive fields beneficial efficient learning downstream authors prove theorem provides interpretation random receptive fields performing filtering eigenbasis Gaussian process inductive bias argued reason downstream predictors performance may enhanced receptive fields compatible learning problem contributions potentially important neither two ideas sufficiently motivated developed Therefore paper feels light content suggestions improvement need modeling receptive fields random draws motivated found strange introduction paper reviews wide range literature including even technical references reproducing kernel Hilbert spaces whose relevance questionable main motivation work short sentence referring data ref went paper see motivates authors approach advantage modeling receptive fields random draws authors provide evidence models capture second order statistics experimental receptive fields clear advantage modeling approach compared say fitting LN model every neuron authors state performance metrics make comparisons relevant techniques Authors mention RKHSs times even theoretical discussion Appendix However clear connection important relevant Reading paragraph equation sorry line numbers provided discussion section feels like authors want claim learning efficiency results related learning RKHSs would nice see point fully developed Instead MNIST authors could use dataset naturalistic complex stimuli theoretical study makes receptive field covariance compatible incompatible downstream learning pursued may related RKHS picture authors hinting authors great list possible future directions One directions could pursued paper Black line Figure visible Reviewer submitted article authors present novel framework modeling receptive fields sensory cortices introducing concept structured receptive field drawn biased probability distribution model sensory neurons interpretable fashion still incorporating randomness crucial biological systems article broadly split two parts First structured receptive fields used fit experimental data biological receptive fields suitability machine learning applications discussed results interesting novel described points respects analysis explication results could improved Major points results matching biological receptive fields based qualitativenot quantitativecomparisons authors compare covariance matrices experimental data fitted model based similar look plot One way results might strengthened including null hypothesis like purely random receptive fields receptive fields based Gabor filters V authors nice job motivating theory theory random feature networks machine learning literature would also liked see authors discuss detail ideas context previous computational neuroscience approaches describing receptive fields including efficient coding sparse coding related approaches establishing structured weights lead faster learning Section convincing way show view would define metric speed learning eg area learning curve ii optimize learning rate separately network respect metric iii compare metric two networks separately optimized learning rates way simulations currently done leaves open possibility slower learning unstructured case might due suboptimal choice learning rate Minor points section authors explain L regularization parameter set possible presented results would change parameter set differently first paragraph Section claim made without specifying particular task adding inductive bias improves learning authors careful statements like elsewhere paper since obviously aware given illustrate point simulations inductive bias expected improve learning well matched particular computational task actually worsen learning case Section authors state GP covariance function reflects statistical regularities within sensory inputs network authors provide citation statement cases considered secondtolast paragraph section bandwidth fhi flo equation section stochastic coefficients cosomegak sinomegak independent end section authors precisely explain statement smoothness also controlled overall magnitude nonzero eigenvalues describing computation Cdata Section explanation meant centered receptive fields could helpful Section unit pixels seems incorrect since squared pixel generally meant pixel explanation mechanism leads prominent dark patches covariance matrices shown Figure would appreciated result Figures E extent Figure authors mean remarkably well might addressed response major point Figure E authors comment faster decay spectrum model Could due noise effect related calcium imaging text describing Figure stated Cdata shown zoomed nothing mentioned caption mention scale given one look supplemental figure get still imprecise idea whats going Given dark patches still clearly visible full covariance plotted supplemental figure would recommend either showing full version instead Figure describing zoomedin version clearly plots machine learning applications authors add green curves fact show higher test errors blue ones tautological since parameters latter optimized precisely minimize test error may value curves point illustrate poorly chosen inductive bias worse inductive bias doesnt seem borne pairs green vs orange curves figures doesnt seem point made text last paragraph appendix authors claim functions small Hnorm easier learn larger norm statement made precise perhaps specific regression algorithm Reviewer paper authors considered receptive fields random samples parametrized distributions gaussian process particular following modalities insect mechanosensors mammalian visual cortex neurons demonstrated random feature neurons RFN remove high frequency noise boost signals Finally also show RFNs enable efficient learning number neurons training time perspective topic general interest visual neuroscience growing subfield within computational neuroscience intersecting artificial neural networks comments questions motivations implications work attached theoretical analysis equivalence structured weight vector transformation filtering Gaussian Processing eigenbasis random projection onto spherical random gaussian interesting Aside connection kernel theory learning broader implications finding either sensory neuroscience deep learning fields would helpful get discussion around topic revised version motivation using Gaussian Process type generative model mathematical andor neuro perspectives possible sampling random filters Gaussian Process mathematically tractable also biologically plausible compared say learning filters taskdriven optimization would helpful get discussion around revised version surprising filter samples GPs covariance real data match various properties real neural populations might helpful control models fail capture biological fidelity example Fig revised version Minor comment name structured random receptive field properly defined paper bit vague initially thought random implies stochastic Dapello et al Overall think paper interesting recommend acceptance revisions authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes Reviewer None PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer Reviewer Figure Files revising submission please upload figure files Preflight Analysis Conversion Engine PACE digital diagnostic tool httpspacevapexcovantagecom PACE helps ensure figures meet PLOS requirements use PACE must first register user login navigate UPLOAD tab find detailed instructions use tool encounter issues questions using PACE please email us grosolpserugif Data Requirements Please note condition publication PLOS data policy requires make available data used draw conclusions outlined manuscript Data must deposited appropriate repository included within body manuscript uploaded supporting information includes numerical values used generate graphs histograms etc example PLOS Biology see httpwwwplosbiologyorgarticleinfoAdoiFFjournalpbios Reproducibility enhance reproducibility results recommend deposit laboratory protocols protocolsio protocol assigned identifier DOI cited independently future Additionally PLOS ONE offers option publish peerreviewed clinical study protocols Read information sharing protocols httpsplosorgprotocolsutmmediumeditorialemailutmsourceauthorlettersutmcampaignprotocols Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Mar Dear Mr Pandey Thank much submitting manuscript Structured random receptive fields enable informative sensory encodings consideration PLOS Computational Biology papers reviewed journal manuscript reviewed members editorial board several independent reviewers light reviews email would like invite resubmission significantlyrevised version takes account reviewers comments make decision publication seen revised manuscript response reviewers comments revised manuscript also likely sent reviewers evaluation ready resubmit please upload following letter containing detailed list responses review comments description changes made manuscript Please note forming response article accepted may opportunity make peer review history publicly available record include editor decision letters reviews responses reviewer comments eligible contact opt Two versions revised manuscript one either highlights tracked changes denoting text changed clean version uploaded manuscript file Important additional instructions given reviewer comments Please prepare submit revised manuscript within days anticipate delay please let us know expected resubmission date replying email Please note revised manuscripts received day due date may require evaluation peer review similar newly submitted manuscripts Thank submission hope editorial process constructive far welcome feedback time Please dont hesitate contact us questions comments Sincerely Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer authors make two main contributions First modeling receptive fields independent draws Gaussian process Second arguing structured receptive fields beneficial efficient learning downstream authors prove theorem provides interpretation random receptive fields performing filtering eigenbasis Gaussian process inductive bias argued reason downstream predictors performance may enhanced receptive fields compatible learning problem contributions potentially important neither two ideas sufficiently motivated developed Therefore paper feels light content suggestions improvement need modeling receptive fields random draws motivated found strange introduction paper reviews wide range literature including even technical references reproducing kernel Hilbert spaces whose relevance questionable main motivation work short sentence referring data ref went paper see motivates authors approach advantage modeling receptive fields random draws authors provide evidence models capture second order statistics experimental receptive fields clear advantage modeling approach compared say fitting LN model every neuron authors state performance metrics make comparisons relevant techniques Authors mention RKHSs times even theoretical discussion Appendix However clear connection important relevant Reading paragraph equation sorry line numbers provided discussion section feels like authors want claim learning efficiency results related learning RKHSs would nice see point fully developed Instead MNIST authors could use dataset naturalistic complex stimuli theoretical study makes receptive field covariance compatible incompatible downstream learning pursued may related RKHS picture authors hinting authors great list possible future directions One directions could pursued paper Black line Figure visible Reviewer submitted article authors present novel framework modeling receptive fields sensory cortices introducing concept structured receptive field drawn biased probability distribution model sensory neurons interpretable fashion still incorporating randomness crucial biological systems article broadly split two parts First structured receptive fields used fit experimental data biological receptive fields suitability machine learning applications discussed results interesting novel described points respects analysis explication results could improved Major points results matching biological receptive fields based qualitativenot quantitativecomparisons authors compare covariance matrices experimental data fitted model based similar look plot One way results might strengthened including null hypothesis like purely random receptive fields receptive fields based Gabor filters V authors nice job motivating theory theory random feature networks machine learning literature would also liked see authors discuss detail ideas context previous computational neuroscience approaches describing receptive fields including efficient coding sparse coding related approaches establishing structured weights lead faster learning Section convincing way show view would define metric speed learning eg area learning curve ii optimize learning rate separately network respect metric iii compare metric two networks separately optimized learning rates way simulations currently done leaves open possibility slower learning unstructured case might due suboptimal choice learning rate Minor points section authors explain L regularization parameter set possible presented results would change parameter set differently first paragraph Section claim made without specifying particular task adding inductive bias improves learning authors careful statements like elsewhere paper since obviously aware given illustrate point simulations inductive bias expected improve learning well matched particular computational task actually worsen learning case Section authors state GP covariance function reflects statistical regularities within sensory inputs network authors provide citation statement cases considered secondtolast paragraph section bandwidth fhi flo equation section stochastic coefficients cosomegak sinomegak independent end section authors precisely explain statement smoothness also controlled overall magnitude nonzero eigenvalues describing computation Cdata Section explanation meant centered receptive fields could helpful Section unit pixels seems incorrect since squared pixel generally meant pixel explanation mechanism leads prominent dark patches covariance matrices shown Figure would appreciated result Figures E extent Figure authors mean remarkably well might addressed response major point Figure E authors comment faster decay spectrum model Could due noise effect related calcium imaging text describing Figure stated Cdata shown zoomed nothing mentioned caption mention scale given one look supplemental figure get still imprecise idea whats going Given dark patches still clearly visible full covariance plotted supplemental figure would recommend either showing full version instead Figure describing zoomedin version clearly plots machine learning applications authors add green curves fact show higher test errors blue ones tautological since parameters latter optimized precisely minimize test error may value curves point illustrate poorly chosen inductive bias worse inductive bias doesnt seem borne pairs green vs orange curves figures doesnt seem point made text last paragraph appendix authors claim functions small Hnorm easier learn larger norm statement made precise perhaps specific regression algorithm Reviewer paper authors considered receptive fields random samples parametrized distributions gaussian process particular following modalities insect mechanosensors mammalian visual cortex neurons demonstrated random feature neurons RFN remove high frequency noise boost signals Finally also show RFNs enable efficient learning number neurons training time perspective topic general interest visual neuroscience growing subfield within computational neuroscience intersecting artificial neural networks comments questions motivations implications work attached theoretical analysis equivalence structured weight vector transformation filtering Gaussian Processing eigenbasis random projection onto spherical random gaussian interesting Aside connection kernel theory learning broader implications finding either sensory neuroscience deep learning fields would helpful get discussion around topic revised version motivation using Gaussian Process type generative model mathematical andor neuro perspectives possible sampling random filters Gaussian Process mathematically tractable also biologically plausible compared say learning filters taskdriven optimization would helpful get discussion around revised version surprising filter samples GPs covariance real data match various properties real neural populations might helpful control models fail capture biological fidelity example Fig revised version Minor comment name structured random receptive field properly defined paper bit vague initially thought random implies stochastic Dapello et al Overall think paper interesting recommend acceptance revisions authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes Reviewer None PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer Reviewer Figure Files revising submission please upload figure files Preflight Analysis Conversion Engine PACE digital diagnostic tool httpspacevapexcovantagecom PACE helps ensure figures meet PLOS requirements use PACE must first register user login navigate UPLOAD tab find detailed instructions use tool encounter issues questions using PACE please email us grosolpserugif Data Requirements Please note condition publication PLOS data policy requires make available data used draw conclusions outlined manuscript Data must deposited appropriate repository included within body manuscript uploaded supporting information includes numerical values used generate graphs histograms etc example PLOS Biology see httpwwwplosbiologyorgarticleinfoAdoiFFjournalpbios Reproducibility enhance reproducibility results recommend deposit laboratory protocols protocolsio protocol assigned identifier DOI cited independently future Additionally PLOS ONE offers option publish peerreviewed clinical study protocols Read information sharing protocols httpsplosorgprotocolsutmmediumeditorialemailutmsourceauthorlettersutmcampaignprotocols Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Oct e Published online Oct doi journalpcbir Oct e Published online Oct doi journalpcbir Oct e Oct e Published online Oct doi journalpcbirPublished online Oct doi journalpcbirdoi journalpcbirDecision Letter Thomas Serre Section Editor Xuexin Wei Academic EditorThomas Serre Section Editor Xuexin Wei Academic EditorThomas SerreSection EditorXuexin WeiAcademic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Copyright License information DisclaimerCopyright License informationDisclaimerCopyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source creditedCopyright Serre WeiCopyrightThis open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source creditedCreative Commons Attribution License Mar Dear Mr Pandey Thank much submitting manuscript Structured random receptive fields enable informative sensory encodings consideration PLOS Computational Biology papers reviewed journal manuscript reviewed members editorial board several independent reviewers light reviews email would like invite resubmission significantlyrevised version takes account reviewers comments make decision publication seen revised manuscript response reviewers comments revised manuscript also likely sent reviewers evaluation ready resubmit please upload following letter containing detailed list responses review comments description changes made manuscript Please note forming response article accepted may opportunity make peer review history publicly available record include editor decision letters reviews responses reviewer comments eligible contact opt Two versions revised manuscript one either highlights tracked changes denoting text changed clean version uploaded manuscript file Important additional instructions given reviewer comments Please prepare submit revised manuscript within days anticipate delay please let us know expected resubmission date replying email Please note revised manuscripts received day due date may require evaluation peer review similar newly submitted manuscripts Thank submission hope editorial process constructive far welcome feedback time Please dont hesitate contact us questions comments Sincerely Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer authors make two main contributions First modeling receptive fields independent draws Gaussian process Second arguing structured receptive fields beneficial efficient learning downstream authors prove theorem provides interpretation random receptive fields performing filtering eigenbasis Gaussian process inductive bias argued reason downstream predictors performance may enhanced receptive fields compatible learning problem contributions potentially important neither two ideas sufficiently motivated developed Therefore paper feels light content suggestions improvement need modeling receptive fields random draws motivated found strange introduction paper reviews wide range literature including even technical references reproducing kernel Hilbert spaces whose relevance questionable main motivation work short sentence referring data ref went paper see motivates authors approach advantage modeling receptive fields random draws authors provide evidence models capture second order statistics experimental receptive fields clear advantage modeling approach compared say fitting LN model every neuron authors state performance metrics make comparisons relevant techniques Authors mention RKHSs times even theoretical discussion Appendix However clear connection important relevant Reading paragraph equation sorry line numbers provided discussion section feels like authors want claim learning efficiency results related learning RKHSs would nice see point fully developed Instead MNIST authors could use dataset naturalistic complex stimuli theoretical study makes receptive field covariance compatible incompatible downstream learning pursued may related RKHS picture authors hinting authors great list possible future directions One directions could pursued paper Black line Figure visible Reviewer submitted article authors present novel framework modeling receptive fields sensory cortices introducing concept structured receptive field drawn biased probability distribution model sensory neurons interpretable fashion still incorporating randomness crucial biological systems article broadly split two parts First structured receptive fields used fit experimental data biological receptive fields suitability machine learning applications discussed results interesting novel described points respects analysis explication results could improved Major points results matching biological receptive fields based qualitativenot quantitativecomparisons authors compare covariance matrices experimental data fitted model based similar look plot One way results might strengthened including null hypothesis like purely random receptive fields receptive fields based Gabor filters V authors nice job motivating theory theory random feature networks machine learning literature would also liked see authors discuss detail ideas context previous computational neuroscience approaches describing receptive fields including efficient coding sparse coding related approaches establishing structured weights lead faster learning Section convincing way show view would define metric speed learning eg area learning curve ii optimize learning rate separately network respect metric iii compare metric two networks separately optimized learning rates way simulations currently done leaves open possibility slower learning unstructured case might due suboptimal choice learning rate Minor points section authors explain L regularization parameter set possible presented results would change parameter set differently first paragraph Section claim made without specifying particular task adding inductive bias improves learning authors careful statements like elsewhere paper since obviously aware given illustrate point simulations inductive bias expected improve learning well matched particular computational task actually worsen learning case Section authors state GP covariance function reflects statistical regularities within sensory inputs network authors provide citation statement cases considered secondtolast paragraph section bandwidth fhi flo equation section stochastic coefficients cosomegak sinomegak independent end section authors precisely explain statement smoothness also controlled overall magnitude nonzero eigenvalues describing computation Cdata Section explanation meant centered receptive fields could helpful Section unit pixels seems incorrect since squared pixel generally meant pixel explanation mechanism leads prominent dark patches covariance matrices shown Figure would appreciated result Figures E extent Figure authors mean remarkably well might addressed response major point Figure E authors comment faster decay spectrum model Could due noise effect related calcium imaging text describing Figure stated Cdata shown zoomed nothing mentioned caption mention scale given one look supplemental figure get still imprecise idea whats going Given dark patches still clearly visible full covariance plotted supplemental figure would recommend either showing full version instead Figure describing zoomedin version clearly plots machine learning applications authors add green curves fact show higher test errors blue ones tautological since parameters latter optimized precisely minimize test error may value curves point illustrate poorly chosen inductive bias worse inductive bias doesnt seem borne pairs green vs orange curves figures doesnt seem point made text last paragraph appendix authors claim functions small Hnorm easier learn larger norm statement made precise perhaps specific regression algorithm Reviewer paper authors considered receptive fields random samples parametrized distributions gaussian process particular following modalities insect mechanosensors mammalian visual cortex neurons demonstrated random feature neurons RFN remove high frequency noise boost signals Finally also show RFNs enable efficient learning number neurons training time perspective topic general interest visual neuroscience growing subfield within computational neuroscience intersecting artificial neural networks comments questions motivations implications work attached theoretical analysis equivalence structured weight vector transformation filtering Gaussian Processing eigenbasis random projection onto spherical random gaussian interesting Aside connection kernel theory learning broader implications finding either sensory neuroscience deep learning fields would helpful get discussion around topic revised version motivation using Gaussian Process type generative model mathematical andor neuro perspectives possible sampling random filters Gaussian Process mathematically tractable also biologically plausible compared say learning filters taskdriven optimization would helpful get discussion around revised version surprising filter samples GPs covariance real data match various properties real neural populations might helpful control models fail capture biological fidelity example Fig revised version Minor comment name structured random receptive field properly defined paper bit vague initially thought random implies stochastic Dapello et al Overall think paper interesting recommend acceptance revisions authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes Reviewer None PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer Reviewer Figure Files revising submission please upload figure files Preflight Analysis Conversion Engine PACE digital diagnostic tool httpspacevapexcovantagecom PACE helps ensure figures meet PLOS requirements use PACE must first register user login navigate UPLOAD tab find detailed instructions use tool encounter issues questions using PACE please email us grosolpserugif Data Requirements Please note condition publication PLOS data policy requires make available data used draw conclusions outlined manuscript Data must deposited appropriate repository included within body manuscript uploaded supporting information includes numerical values used generate graphs histograms etc example PLOS Biology see httpwwwplosbiologyorgarticleinfoAdoiFFjournalpbios Reproducibility enhance reproducibility results recommend deposit laboratory protocols protocolsio protocol assigned identifier DOI cited independently future Additionally PLOS ONE offers option publish peerreviewed clinical study protocols Read information sharing protocols httpsplosorgprotocolsutmmediumeditorialemailutmsourceauthorlettersutmcampaignprotocols Mar Mar Dear Mr PandeyThank much submitting manuscript Structured random receptive fields enable informative sensory encodings consideration PLOS Computational BiologyAs papers reviewed journal manuscript reviewed members editorial board several independent reviewers light reviews email would like invite resubmission significantlyrevised version takes account reviewers commentsWe make decision publication seen revised manuscript response reviewers comments revised manuscript also likely sent reviewers evaluationWhen ready resubmit please upload following letter containing detailed list responses review comments description changes made manuscript Please note forming response article accepted may opportunity make peer review history publicly available record include editor decision letters reviews responses reviewer comments eligible contact opt Two versions revised manuscript one either highlights tracked changes denoting text changed clean version uploaded manuscript fileImportant additional instructions given reviewer commentsPlease prepare submit revised manuscript within days anticipate delay please let us know expected resubmission date replying email Please note revised manuscripts received day due date may require evaluation peer review similar newly submitted manuscriptsThank submission hope editorial process constructive far welcome feedback time Please dont hesitate contact us questions commentsSincerelyXuexin WeiAssociate EditorPLOS Computational BiologyThomas SerreDeputy EditorPLOS Computational BiologyReviewers Responses Questions Comments Authors Comments Authors Please note review uploaded attachment Please note review uploaded attachmentReviewer authors make two main contributions First modeling receptive fields independent draws Gaussian process Second arguing structured receptive fields beneficial efficient learning downstream authors prove theorem provides interpretation random receptive fields performing filtering eigenbasis Gaussian process inductive bias argued reason downstream predictors performance may enhanced receptive fields compatible learning problemWhile contributions potentially important neither two ideas sufficiently motivated developed Therefore paper feels light content suggestions improvement need modeling receptive fields random draws motivated found strange introduction paper reviews wide range literature including even technical references reproducing kernel Hilbert spaces whose relevance questionable main motivation work short sentence referring data ref went paper see motivates authors approach advantage modeling receptive fields random draws authors provide evidence models capture second order statistics experimental receptive fields clear advantage modeling approach compared say fitting LN model every neuron authors state performance metrics make comparisons relevant techniques Authors mention RKHSs times even theoretical discussion Appendix However clear connection important relevant Reading paragraph equation sorry line numbers provided discussion section feels like authors want claim learning efficiency results related learning RKHSs would nice see point fully developed Instead MNIST authors could use dataset naturalistic complex stimuli theoretical study makes receptive field covariance compatible incompatible downstream learning pursued may related RKHS picture authors hinting authors great list possible future directions One directions could pursued paperOther Black line Figure visibleReviewer submitted article authors present novel framework modeling receptive fields sensory cortices introducing concept structured receptive field drawn biased probability distribution model sensory neurons interpretable fashion still incorporating randomness crucial biological systems article broadly split two parts First structured receptive fields used fit experimental data biological receptive fields suitability machine learning applications discussedThese results interesting novel described points respects analysis explication results could improvedMajor points results matching biological receptive fields based qualitativenot quantitativecomparisons authors compare covariance matrices experimental data fitted model based similar look plot One way results might strengthened including null hypothesis like purely random receptive fields receptive fields based Gabor filters V authors nice job motivating theory theory random feature networks machine learning literature would also liked see authors discuss detail ideas context previous computational neuroscience approaches describing receptive fields including efficient coding sparse coding related approaches establishing structured weights lead faster learning Section convincing way show view would define metric speed learning eg area learning curve ii optimize learning rate separately network respect metric iii compare metric two networks separately optimized learning rates way simulations currently done leaves open possibility slower learning unstructured case might due suboptimal choice learning rateMinor points section authors explain L regularization parameter set possible presented results would change parameter set differently first paragraph Section claim made without specifying particular task adding inductive bias improves learning authors careful statements like elsewhere paper since obviously aware given illustrate point simulations inductive bias expected improve learning well matched particular computational task actually worsen learning case Section authors state GP covariance function reflects statistical regularities within sensory inputs network authors provide citation statement cases considered secondtolast paragraph section bandwidth fhi flo equation section stochastic coefficients cosomegak sinomegak independent end section authors precisely explain statement smoothness also controlled overall magnitude nonzero eigenvalues describing computation Cdata Section explanation meant centered receptive fields could helpful Section unit pixels seems incorrect since squared pixel generally meant pixel explanation mechanism leads prominent dark patches covariance matrices shown Figure would appreciated result Figures E extent Figure authors mean remarkably well might addressed response major point Figure E authors comment faster decay spectrum model Could due noise effect related calcium imaging text describing Figure stated Cdata shown zoomed nothing mentioned caption mention scale given one look supplemental figure get still imprecise idea whats going Given dark patches still clearly visible full covariance plotted supplemental figure would recommend either showing full version instead Figure describing zoomedin version clearly plots machine learning applications authors add green curves fact show higher test errors blue ones tautological since parameters latter optimized precisely minimize test error may value curves point illustrate poorly chosen inductive bias worse inductive bias doesnt seem borne pairs green vs orange curves figures doesnt seem point made text last paragraph appendix authors claim functions small Hnorm easier learn larger norm statement made precise perhaps specific regression algorithmReviewer paper authors considered receptive fields random samples parametrized distributions gaussian process particular following modalities insect mechanosensors mammalian visual cortex neurons demonstrated random feature neurons RFN remove high frequency noise boost signals Finally also show RFNs enable efficient learning number neurons training time perspectiveThis topic general interest visual neuroscience growing subfield within computational neuroscience intersecting artificial neural networks comments questions motivations implications work attached theoretical analysis equivalence structured weight vector transformation filtering Gaussian Processing eigenbasis random projection onto spherical random gaussian interesting Aside connection kernel theory learning broader implications finding either sensory neuroscience deep learning fields would helpful get discussion around topic revised version motivation using Gaussian Process type generative model mathematical andor neuro perspectives possible sampling random filters Gaussian Process mathematically tractable also biologically plausible compared say learning filters taskdriven optimization would helpful get discussion around revised version surprising filter samples GPs covariance real data match various properties real neural populations might helpful control models fail capture biological fidelity example Fig revised versionMinor comment name structured random receptive field properly defined paper bit vague initially thought random implies stochastic Dapello et alOverall think paper interesting recommend acceptance revisions authors made data applicable computational code underlying findings manuscript fully available authors made data applicable computational code underlying findings manuscript fully availableThe PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specifiedPLOS Data policyReviewer NoneReviewer YesReviewer NonePLOS authors option publish peer review history article mean published include full peer review attached fileswhat meanIf choose identity remain anonymous review may still made publicDo want identity public peer review information choice including consent withdrawal please see Privacy Policy want identity public peer reviewPrivacy PolicyReviewer NoReviewer NoReviewer Figure Files Figure FilesWhile revising submission please upload figure files Preflight Analysis Conversion Engine PACE digital diagnostic tool httpspacevapexcovantagecom PACE helps ensure figures meet PLOS requirements use PACE must first register user login navigate UPLOAD tab find detailed instructions use tool encounter issues questions using PACE please email us grosolpserugif httpspacevapexcovantagecomhttpspacevapexcovantagecomgrosolpserugifgrosolpserugif Data Requirements Data RequirementsPlease note condition publication PLOS data policy requires make available data used draw conclusions outlined manuscript Data must deposited appropriate repository included within body manuscript uploaded supporting information includes numerical values used generate graphs histograms etc example PLOS Biology see httpwwwplosbiologyorgarticleinfoAdoiFFjournalpbios httpwwwplosbiologyorgarticleinfoAdoiFFjournalpbios Reproducibility ReproducibilityTo enhance reproducibility results recommend deposit laboratory protocols protocolsio protocol assigned identifier DOI cited independently future Additionally PLOS ONE offers option publish peerreviewed clinical study protocols Read information sharing protocols httpsplosorgprotocolsutmmediumeditorialemailutmsourceauthorlettersutmcampaignprotocolshttpsplosorgprotocolsutmmediumeditorialemailutmsourceauthorlettersutmcampaignprotocolsPLoS Comput Biol Oct e Author response Decision Letter PLoS Comput Biol Oct e Author response Decision Letter PLoS Comput Biol Oct e PLoS Comput Biol Oct e Author response Decision Letter Oct e Published online Oct doi journalpcbir Author response Decision Letter Copyright License information Disclaimer Copyright notice Jun Attachment Submitted filename coverletterresubmissionpdf Click additional data file pdf Oct e Published online Oct doi journalpcbir Author response Decision Letter Copyright License information Disclaimer Copyright notice Jun Attachment Submitted filename coverletterresubmissionpdf Click additional data file pdf Oct e Published online Oct doi journalpcbir Author response Decision Letter Copyright License information Disclaimer Copyright notice Oct e Published online Oct doi journalpcbir Oct e Published online Oct doi journalpcbir Oct e Oct e Published online Oct doi journalpcbirPublished online Oct doi journalpcbirdoi journalpcbirAuthor response Decision Letter Copyright License information Disclaimer Copyright notice Copyright License information DisclaimerCopyright License informationDisclaimerCopyright notice Copyright notice Copyright notice Jun Attachment Submitted filename coverletterresubmissionpdf Click additional data file pdf Jun Jun Attachment Submitted filename coverletterresubmissionpdf Click additional data file pdfAttachmentSubmitted filename coverletterresubmissionpdfcoverletterresubmissionpdfClick additional data file pdfClick additional data fileM pdfPLoS Comput Biol Oct e Decision Letter PLoS Comput Biol Oct e Decision Letter PLoS Comput Biol Oct e PLoS Comput Biol Oct e Decision Letter Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Aug Dear Dr Harris pleased inform manuscript Structured random receptive fields enable informative sensory encodings provisionally accepted publication PLOS Computational Biology manuscript formally accepted need complete formatting changes receive follow email member team touch set requests Please note manuscript scheduled publication made required changes swift response appreciated IMPORTANT editorial review process complete PLOS permit corrections spelling formatting significant scientific errors point onwards Requests major changes affect scientific understanding work cause delays publication date manuscript institutions press office journal office choose press release paper automatically opted early publication ask notify us institution planning press release article press must coordinated PLOS Thank supporting Open Access publishing looking forward publishing work PLOS Computational Biology Best regards Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer thank authors response appreciate effort put revision Although argued many asked beyond scope paper disagree agree necessary address concerns Clarification new additions paper including Appendix adequate dont comments suggestions recommend acceptance reading paper rebuttal think negative recommending rejection first review apologize think fair assessment would major revision Reviewer authors done excellent job responding earlier comments pleased recommend manuscript accepted publication especially appreciated new appendix section greatly strengthens mathematical foundations work Two minor comments follow things noticed reading manuscript might help improve paper final publication bar plot part Fig illustrating results three models described line perhaps also including sketch receptive fields two null models look like would helpful way visually summarize result similar visualization could helpful Fig Perhaps missed Figures exactly percentages reported panel calculated Comparing orange curves green curves doesnt appear numbers correspond curves appear next straightforward way could discern authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Aug Dear Dr Harris pleased inform manuscript Structured random receptive fields enable informative sensory encodings provisionally accepted publication PLOS Computational Biology manuscript formally accepted need complete formatting changes receive follow email member team touch set requests Please note manuscript scheduled publication made required changes swift response appreciated IMPORTANT editorial review process complete PLOS permit corrections spelling formatting significant scientific errors point onwards Requests major changes affect scientific understanding work cause delays publication date manuscript institutions press office journal office choose press release paper automatically opted early publication ask notify us institution planning press release article press must coordinated PLOS Thank supporting Open Access publishing looking forward publishing work PLOS Computational Biology Best regards Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer thank authors response appreciate effort put revision Although argued many asked beyond scope paper disagree agree necessary address concerns Clarification new additions paper including Appendix adequate dont comments suggestions recommend acceptance reading paper rebuttal think negative recommending rejection first review apologize think fair assessment would major revision Reviewer authors done excellent job responding earlier comments pleased recommend manuscript accepted publication especially appreciated new appendix section greatly strengthens mathematical foundations work Two minor comments follow things noticed reading manuscript might help improve paper final publication bar plot part Fig illustrating results three models described line perhaps also including sketch receptive fields two null models look like would helpful way visually summarize result similar visualization could helpful Fig Perhaps missed Figures exactly percentages reported panel calculated Comparing orange curves green curves doesnt appear numbers correspond curves appear next straightforward way could discern authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer Oct e Published online Oct doi journalpcbir Decision Letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Oct e Published online Oct doi journalpcbir Oct e Published online Oct doi journalpcbir Oct e Oct e Published online Oct doi journalpcbirPublished online Oct doi journalpcbirdoi journalpcbirDecision Letter Thomas Serre Section Editor Xuexin Wei Academic EditorThomas Serre Section Editor Xuexin Wei Academic EditorThomas SerreSection EditorXuexin WeiAcademic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Copyright License information DisclaimerCopyright License informationDisclaimerCopyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source creditedCopyright Serre WeiCopyrightThis open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source creditedCreative Commons Attribution License Aug Dear Dr Harris pleased inform manuscript Structured random receptive fields enable informative sensory encodings provisionally accepted publication PLOS Computational Biology manuscript formally accepted need complete formatting changes receive follow email member team touch set requests Please note manuscript scheduled publication made required changes swift response appreciated IMPORTANT editorial review process complete PLOS permit corrections spelling formatting significant scientific errors point onwards Requests major changes affect scientific understanding work cause delays publication date manuscript institutions press office journal office choose press release paper automatically opted early publication ask notify us institution planning press release article press must coordinated PLOS Thank supporting Open Access publishing looking forward publishing work PLOS Computational Biology Best regards Xuexin Wei Associate Editor PLOS Computational Biology Thomas Serre Deputy Editor PLOS Computational Biology Reviewers Responses Questions Comments Authors Please note review uploaded attachment Reviewer thank authors response appreciate effort put revision Although argued many asked beyond scope paper disagree agree necessary address concerns Clarification new additions paper including Appendix adequate dont comments suggestions recommend acceptance reading paper rebuttal think negative recommending rejection first review apologize think fair assessment would major revision Reviewer authors done excellent job responding earlier comments pleased recommend manuscript accepted publication especially appreciated new appendix section greatly strengthens mathematical foundations work Two minor comments follow things noticed reading manuscript might help improve paper final publication bar plot part Fig illustrating results three models described line perhaps also including sketch receptive fields two null models look like would helpful way visually summarize result similar visualization could helpful Fig Perhaps missed Figures exactly percentages reported panel calculated Comparing orange curves green curves doesnt appear numbers correspond curves appear next straightforward way could discern authors made data applicable computational code underlying findings manuscript fully available PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specified Reviewer None Reviewer Yes PLOS authors option publish peer review history article mean published include full peer review attached files choose identity remain anonymous review may still made public want identity public peer review information choice including consent withdrawal please see Privacy Policy Reviewer Reviewer Aug Aug Dear Dr HarrisWe pleased inform manuscript Structured random receptive fields enable informative sensory encodings provisionally accepted publication PLOS Computational BiologyBefore manuscript formally accepted need complete formatting changes receive follow email member team touch set requestsPlease note manuscript scheduled publication made required changes swift response appreciatedIMPORTANT editorial review process complete PLOS permit corrections spelling formatting significant scientific errors point onwards Requests major changes affect scientific understanding work cause delays publication date manuscriptShould institutions press office journal office choose press release paper automatically opted early publication ask notify us institution planning press release article press must coordinated PLOSThank supporting Open Access publishing looking forward publishing work PLOS Computational Biology Best regardsXuexin WeiAssociate EditorPLOS Computational BiologyThomas SerreDeputy EditorPLOS Computational BiologyReviewers Responses Questions Comments Authors Comments Authors Please note review uploaded attachment Please note review uploaded attachmentReviewer thank authors response appreciate effort put revision Although argued many asked beyond scope paper disagree agree necessary address concerns Clarification new additions paper including Appendix adequate dont comments suggestions recommend acceptanceAfter reading paper rebuttal think negative recommending rejection first review apologize think fair assessment would major revisionReviewer authors done excellent job responding earlier comments pleased recommend manuscript accepted publication especially appreciated new appendix section greatly strengthens mathematical foundations workTwo minor comments follow things noticed reading manuscript might help improve paper final publication bar plot part Fig illustrating results three models described line perhaps also including sketch receptive fields two null models look like would helpful way visually summarize result similar visualization could helpful Fig Perhaps missed Figures exactly percentages reported panel calculated Comparing orange curves green curves doesnt appear numbers correspond curves appear next straightforward way could discern authors made data applicable computational code underlying findings manuscript fully available authors made data applicable computational code underlying findings manuscript fully availableThe PLOS Data policy requires authors make data code underlying findings described manuscript fully available without restriction rare exception please refer Data Availability Statement manuscript PDF file data code provided part manuscript supporting information deposited public repository example addition summary statistics data points behind means medians variance measures available restrictions publicly sharing data code eg participant privacy use data third partythose must specifiedPLOS Data policyReviewer NoneReviewer YesPLOS authors option publish peer review history article mean published include full peer review attached fileswhat meanIf choose identity remain anonymous review may still made publicDo want identity public peer review information choice including consent withdrawal please see Privacy Policy want identity public peer reviewPrivacy PolicyReviewer NoReviewer NoPLoS Comput Biol Oct e Acceptance letterPLoS Comput Biol Oct e Acceptance letterPLoS Comput Biol Oct e PLoS Comput Biol Oct e Acceptance letter Oct e Published online Oct doi journalpcbir Acceptance letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Oct PCOMPBIOLDR Structured random receptive fields enable informative sensory encodings Dear Dr Harris pleased inform manuscript formally accepted publication PLOS Computational Biology manuscript production department notified publication date due course corresponding author soon receiving typeset proof review ensure errors introduced production Please review PDF proof manuscript carefully last chance correct errors Please note major changes affect scientific understanding work likely cause delays publication date manuscript Soon final files uploaded unless opted early version manuscript published online date early version articles publication date final article published URL versions paper accessible readers Thank supporting PLOS Computational Biology openaccess publishing looking forward publishing work kind regards Anita Estes PLOS Computational Biology Carlyle House Carlyle Road Cambridge CB DN United Kingdom grosolploibpmocsolp Phone ploscompbiolorg PLOSCompBiol Oct e Published online Oct doi journalpcbir Acceptance letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Oct PCOMPBIOLDR Structured random receptive fields enable informative sensory encodings Dear Dr Harris pleased inform manuscript formally accepted publication PLOS Computational Biology manuscript production department notified publication date due course corresponding author soon receiving typeset proof review ensure errors introduced production Please review PDF proof manuscript carefully last chance correct errors Please note major changes affect scientific understanding work likely cause delays publication date manuscript Soon final files uploaded unless opted early version manuscript published online date early version articles publication date final article published URL versions paper accessible readers Thank supporting PLOS Computational Biology openaccess publishing looking forward publishing work kind regards Anita Estes PLOS Computational Biology Carlyle House Carlyle Road Cambridge CB DN United Kingdom grosolploibpmocsolp Phone ploscompbiolorg PLOSCompBiol Oct e Published online Oct doi journalpcbir Acceptance letter Thomas Serre Section Editor Xuexin Wei Academic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Oct e Published online Oct doi journalpcbir Oct e Published online Oct doi journalpcbir Oct e Oct e Published online Oct doi journalpcbirPublished online Oct doi journalpcbirdoi journalpcbirAcceptance letterThomas Serre Section Editor Xuexin Wei Academic EditorThomas Serre Section Editor Xuexin Wei Academic EditorThomas SerreSection EditorXuexin WeiAcademic Editor Copyright License information Disclaimer Copyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source credited Copyright License information DisclaimerCopyright License informationDisclaimerCopyright Serre Wei open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source creditedCopyright Serre WeiCopyrightThis open access article distributed terms Creative Commons Attribution License permits unrestricted use distribution reproduction medium provided original author source creditedCreative Commons Attribution License Oct PCOMPBIOLDR Structured random receptive fields enable informative sensory encodings Dear Dr Harris pleased inform manuscript formally accepted publication PLOS Computational Biology manuscript production department notified publication date due course corresponding author soon receiving typeset proof review ensure errors introduced production Please review PDF proof manuscript carefully last chance correct errors Please note major changes affect scientific understanding work likely cause delays publication date manuscript Soon final files uploaded unless opted early version manuscript published online date early version articles publication date final article published URL versions paper accessible readers Thank supporting PLOS Computational Biology openaccess publishing looking forward publishing work kind regards Anita Estes PLOS Computational Biology Carlyle House Carlyle Road Cambridge CB DN United Kingdom grosolploibpmocsolp Phone ploscompbiolorg PLOSCompBiol Oct Oct PCOMPBIOLDR Structured random receptive fields enable informative sensory encodingsDear Dr HarrisI pleased inform manuscript formally accepted publication PLOS Computational Biology manuscript production department notified publication date due courseThe corresponding author soon receiving typeset proof review ensure errors introduced production Please review PDF proof manuscript carefully last chance correct errors Please note major changes affect scientific understanding work likely cause delays publication date manuscript Soon final files uploaded unless opted early version manuscript published online date early version articles publication date final article published URL versions paper accessible readersThank supporting PLOS Computational Biology openaccess publishing looking forward publishing work kind regardsAnita EstesPLOS Computational Biology Carlyle House Carlyle Road Cambridge CB DN United Kingdom grosolploibpmocsolp Phone ploscompbiolorg PLOSCompBiolgrosolploibpmocsolpploscompbiolorgArticles PLOS Computational Biology provided courtesy PLOSArticles PLOS Computational Biology provided courtesy PLOSPLOS Computational BiologyPLOS Formats PubReader PDF Actions Cite Collections Add Collections Create new collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Add Cancel Share Permalink Copy RESOURCES Similar articles Cited articles Links NCBI Databases Formats PubReader PDF Actions Cite Collections Add Collections Create new collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Add Cancel Share Permalink Copy RESOURCES Similar articles Cited articles Links NCBI Databases Formats PubReader PDF Formats PubReader PDF PubReaderPubReaderPDF MPDF Actions Cite Collections Add Collections Create new collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Add Cancel Actions Cite Collections Add Collections Create new collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Add Cancel Cite Cite Cite Collections Add Collections Create new collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Add Cancel Collections Add Collections Create new collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Add Cancel Collections Collections Add Collections Create new collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Add Cancel Add Collections Create new collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Add Cancel Add Collections Create new collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Add Cancel Create new collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Add Cancel Create new collection Add existing collection Create new collection Add existing collection Create new collection Create new collection Add existing collection Add existing collection Name collection Name must less characters Choose collection Unable load collection due error Please try Name collection Name must less characters Name collection Name must less characters Choose collection Unable load collection due error Please try Choose collection Unable load collection due error Please try Please try Add Cancel Add Cancel Share Permalink Copy Share Permalink Copy Permalink Copy Permalink Copy Permalink Copy Permalink Copy Copy Copy RESOURCES Similar articles Cited articles Links NCBI Databases RESOURCES Similar articles Cited articles Links NCBI Databases Similar articles Similar articles Similar articles Cited articles Cited articles Cited articles Links NCBI Databases Links NCBI Databases Links NCBI Databases