Massively Parallel Genetics
Jay Shendure
* Department of Genome Sciences and
‡ Department of Medicine , University of Washington, Seattle, Washington 98115
Stanley Fields
* Department of Genome Sciences and
‡ Department of Medicine , University of Washington, Seattle, Washington 98115
Abstract
Human genetics has historically depended on the identification of individuals whose natural genetic variation underlies an observable trait or disease risk. Here we argue that new technologies now augment this historical approach by allowing the use of massively parallel assays in model systems to measure the functional effects of genetic variation in many human genes. These studies will help establish the disease risk of both observed and potential genetic variants and to overcome the problem of “variants of uncertain significance.”

Since genetics began as a field, its rate-limiting step has been the cost and resolution of ascertaining genotypes. However, the recent emergence of massively parallel DNA sequencing has made genotyping both comprehensive and cheap: we can now conduct genetic analyses at a scale scarcely imaginable a decade ago (1000 Genomes Project  et al.  2015). But the ease of genotyping has exposed the limits of genetic analyses, particularly as they have been applied to human phenotypes. Here, we highlight some of these limitations and argue that massively parallel approaches to experimentally measure the functional consequences of individual variants will advance our ability to interpret human genomes.
As a first example, consider genetic analyses of common diseases. Whereas linkage studies, with some important exceptions, largely proved to be a disappointment in this context, genome-wide association studies have been very successful in identifying thousands of reproducible associations between common variants and common diseases . However, the resolution of these associations is inherently limited by linkage disequilibrium in human populations, such that a variant causally underlying an association as well as the gene through which its effects are mediated are rarely known. Although strategies have been developed for fine-mapping, these do not scale or generalize well, and our inability to nail down variants and genes for these associations fundamentally limits this approach. Furthermore, most of the variants discovered by these studies have only a modest effect on phenotype; for example, these variants sum to on the order of 10–20% of narrow-sense heritability for common diseases such as type 2 diabetes, multiple sclerosis, and Crohn’s disease .
As a second example, consider genetic analyses of rare diseases. We are well along the path to the comprehensive elucidation of genes that underlie monogenic syndromes as well as monogenic forms of autism and intellectual disability . However, we remain poorly equipped to predict the consequences of individual variants within implicated genes, such that “variants of uncertain significance” persist as the disappointing outcome in many cases, even for well-studied cancer-predisposing genes such as  BRCA1 . It is unlikely that simply more sequencing will resolve this problem. Although every possible single nucleotide variant (SNV) compatible with life is quite possibly present in a living human somewhere on earth , nearly all such individual variants are exceedingly rare. As such, even with millions of genomes and medical records in hand, and even where there is a specific hypothesis ( e.g. , that a missense variant resulting in loss of function in  BRCA1  will lead to cancer), we will remain poorly powered to quantify the risk that an individual SNV confers by genotype–phenotype analyses alone.
The shared obstacle of these examples is the fact that we cannot control which variants and haplotypes are present in the individuals that we study: human genetics is necessarily “observational.” If we want to confidently interpret variants observed even in only one or a few humans, we propose that “observational” genetics in humans must be supplemented with “perturbational” genetics in model systems:  in vivo  in model organisms,  in vitro  in tissue culture lines, or in cell-free assays of protein function ( ;  ). This “massively parallel” paradigm requires that we know enough about the function of a gene to establish a DNA sequencing-based assay for its activity. The approach takes advantage of recent methodological developments and involves the following:
Functionally testing every possible variant in the human genome in all relevant assays and contexts is of course unrealistic. Furthermore, each of these variants exists within a context of other genetic variation the precise epistatic relationships of which may take decades to decipher. However, by focusing on genes of special value—and assuming that epistasis may not be a significant contributor to the overall burden of disease—we may be able to advance the field of human genetics beyond the bottlenecks that it currently faces. Our proposed approach also benefits from the “multiplexing” of large numbers of independent functional assays within single experiments, as well as on continuing technological improvements, such that the cost in personnel and sequencing reagents for each mutational scan is anticipated to steadily decrease.
Establishing disease risk for all potential variants in cancer-predisposing genes would help to overcome the long-standing problem of variants of uncertain significance. These experiments are currently feasible for many genes the functions of which in signaling, transcription, cell cycle control, and DNA repair are both known and able to be recapitulated in a model system. Defining causal variants and genes within genome-wide association study-implicated haplotypes would dramatically advance our understanding of the genetic basis of common disease. Finally, contemporary computational approaches to variant effect prediction are not as powerful as they might be because they are largely trained on evolutionary metrics and amino acid chemical similarity and they aggregate the effects of variants in many different proteins, leading to poor resolution . It is likely that by experimentally measuring the functional effects of thousands to millions of variants in individual genes, we will be in a much better position to apply the sorts of “deep learning” approaches that are proving successful in other fields for complex pattern recognition.
Functional assays have long been used toward the interpretation of individual genetic variants, both for understanding biological mechanisms and for informing clinical decision-making, but largely in a one-off fashion. What we suggest here is simply taking advantage of recent methodological developments to take the next logical step of massively parallelizing such experiments. Whether the resulting data sets of measurements will be predictive of organismal phenotypes remains a hypothesis. However, given the ongoing explosion in human genome sequencing, it seems very much worth testing.
Footnotes
Communicating editor: M. Johnston
Communicating editor: M. Johnston
Literature Cited
Other Formats
Actions
Share
RESOURCES